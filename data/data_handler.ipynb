{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset from Logical Fallacy Detection paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_datasets = dict()\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    old_datasets[split] = pd.read_csv(f'edu_{split}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_datasets = {\n",
    "    key: dataset.drop(columns=[column for column in dataset.columns if column not in ['source_article', 'updated_label', 'masked_articles']], axis = 1)\n",
    "    for key, dataset in old_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_datasets = {\n",
    "    key: dataset.drop_duplicates()\n",
    "    for key, dataset in old_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_datasets = {\n",
    "    key: dataset.reset_index(drop = True)\n",
    "    for key, dataset in old_datasets.items()\n",
    "}\n",
    "for key in old_datasets.keys():\n",
    "    old_datasets[key].index.name = 'id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated dataset with the prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_datasets = dict()\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    new_datasets[split] = pd.read_csv(f'fallacy_{split}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_datasets = {\n",
    "    key: dataset.drop_duplicates(subset = ['sentence', 'prompt', 'fine_class'])\n",
    "    for key, dataset in new_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_datasets = {\n",
    "    key: dataset.reset_index(drop = True)\n",
    "    for key, dataset in new_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in new_datasets.keys():\n",
    "    new_datasets[key]['sent_id'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "sentence_to_id_dicts = defaultdict(dict)\n",
    "for key in old_datasets.keys():\n",
    "    sentence_to_id_dicts[key] = dict(zip(old_datasets[key]['source_article'].tolist(), old_datasets[key].index.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_not_found = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in new_datasets.keys():\n",
    "    for i in range(len(new_datasets[key])):\n",
    "        sentence = new_datasets[key]['sentence'][i]\n",
    "        label = new_datasets[key]['fine_class'][i]\n",
    "        try:\n",
    "            new_datasets[key]['sent_id'][i] = sentence_to_id_dicts[key][sentence]\n",
    "        except Exception as e:\n",
    "            sentences_not_found.append((key, sentence, label))\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_not_found_df = pd.DataFrame({\n",
    "    'split': list(map(lambda x: x[0], sentences_not_found)),\n",
    "    'sentence': list(map(lambda x: x[1], sentences_not_found)),\n",
    "    'label': list(map(lambda x: x[2], sentences_not_found))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_not_found_df.to_csv('not_found_sentences.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
