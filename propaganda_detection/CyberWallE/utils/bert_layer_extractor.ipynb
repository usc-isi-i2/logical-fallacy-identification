{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_layer_extractor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znPjzZcHHZuE",
        "colab_type": "text"
      },
      "source": [
        "# Token-level and `[CLS]` BERT Embeddings\n",
        "\n",
        "Extracts token-level BERT embeddings for our span identification system and [CLS] embeddings for our technique classification system.\n",
        "\n",
        "Partially adapted from https://github.com/huggingface/transformers/blob/3763f8944dc3fef8afb0c525a2ced8a04889c14f/examples/extract_features.py\n",
        "(Apache License 2.0)\n",
        "\n",
        "The output is a TSV file:\n",
        "```\n",
        "sentence_id    layer_nr    token    embedding\n",
        "```\n",
        "E.g. (with `LAYERS=[-1, -2], TOKEN_LVL=True`, and the input \"hello world embedding\")\n",
        "```\n",
        "1    -1    hello    [0.123456789, 0.987654321, ...]\n",
        "1    -2    hello    [0.111111111, 0.222222222, ...]\n",
        "1    -1    world    ...\n",
        "1    -2    world    ...\n",
        "1    -1    em       ...\n",
        "1    -2    em       ...\n",
        "1    -1    ##bed    ...\n",
        "1    -2    ##bed    ...\n",
        "1    -1    ##ding    ...\n",
        "1    -2    ##ding    ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBQM4ABPLy5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRZKVwilvnSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch_pretrained_bert "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHJxYR06vgrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import urllib\n",
        "import pandas as pd\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZnTBzZvymH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "ROUNDING_ACC = 9\n",
        "LAYERS = [-1]  # [-1, -2]\n",
        "TOKEN_LVL = True  # token-based or sequence-based\n",
        "\n",
        "if TOKEN_LVL:\n",
        "    # Task 1: Span identification\n",
        "    MAX_LEN = 95\n",
        "    TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-train.tsv?token=AD7GEDMEHQSUS34AOSIHGF26Q4WYK'\n",
        "    DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-dev.tsv?token=AD7GEDI3J6KMIKA6XXTKT6S6Q4WYI'\n",
        "    TEST_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-test.tsv?token=AD7GEDM7A3GFIAEZHHESFO26LP4BQ'\n",
        "    # Make sure you have enough free space in your Drive\n",
        "    # train_bert.tsv is 4.3 GB (dev_bert.tsv is only 730 MB)\n",
        "else:\n",
        "    # Task 2: Technique classification\n",
        "    MAX_LEN = 128\n",
        "    TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/tc-train.tsv?token=AD7GEDOCX5E6S5RBB5T5YPS6NJMGI'\n",
        "    DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/tc-dev.tsv?token=AD7GEDIJLXNFOELYLNANKFK6NJMGK'\n",
        "    TEST_URL = ''\n",
        "    TOKEN_INDICES = []  # if you only want the [CLS] token\n",
        "    # TOKEN_INDICES = list(range(1, 11)) # if you also only want the first 10 tokens\n",
        "\n",
        "OUT_PREFIX = '/content/gdrive/My Drive/colab_projects/data/'\n",
        "MODEL = 'bert-base-uncased'\n",
        "# MODEL = 'bert-base-cased'\n",
        "# MODEL = 'bert-large-uncased'\n",
        "\n",
        "# Toggle this!\n",
        "MODE = 'train'\n",
        "# MODE = 'dev'\n",
        "# MODE = 'test'\n",
        "\n",
        "if MODE == 'train':\n",
        "    IN_URL = TRAIN_URL\n",
        "elif MODE == 'dev':\n",
        "    IN_URL = DEV_URL\n",
        "elif MODE == 'test':\n",
        "    IN_URL = TEST_URL\n",
        "\n",
        "UNCASED = 'uncased' in MODEL\n",
        "\n",
        "EXTRA_FILE_INFO = ''\n",
        "\n",
        "OUT_FILE = OUT_PREFIX + MODE + '_' + MODEL + EXTRA_FILE_INFO + '.tsv'\n",
        "if not TOKEN_LVL:\n",
        "    OUT_FILE = OUT_PREFIX + 'tc_' + MODE + '_' + MODEL + EXTRA_FILE_INFO + '.tsv'\n",
        "print(OUT_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43x0CqLkyprJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not torch.cuda.is_available():\n",
        "    print(\"WARNING: GPU not available!!\")\n",
        "device = torch.device(\"cuda\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0) \n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL, do_lower_case=UNCASED)\n",
        "model = BertModel.from_pretrained(MODEL)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw53eIej-9wM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_comments(filename, url=True):\n",
        "    if url:\n",
        "        comments = []\n",
        "        with urllib.request.urlopen(filename) as f:\n",
        "            for line in f:\n",
        "                if line.startswith(b'#'):\n",
        "                    comments.append(line.decode(\"utf-8\"))\n",
        "                else:\n",
        "                    break\n",
        "        return comments\n",
        "    with open(filename, 'r', encoding='utf8') as f:\n",
        "        commentiter = takewhile(lambda s: s.startswith('#'), f)\n",
        "        comments = list(commentiter)\n",
        "    return comments\n",
        "\n",
        "comments = get_comments(IN_URL)\n",
        "full_df = pd.read_csv(IN_URL, sep='\\t', skiprows=len(comments), quoting=3)\n",
        "if TOKEN_LVL:\n",
        "    sent_df = full_df.groupby('sent_id')['token'].apply(list).to_frame()\n",
        "    sentences = sent_df['token'].tolist()\n",
        "else:\n",
        "    sentences = full_df['text'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr12SCKSauVM",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "\n",
        "    def __init__(self, tokens, sent_idx, input_ids, input_mask, sentence):\n",
        "        self.tokens = tokens\n",
        "        self.sent_idx = sent_idx\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.sentence = sentence\n",
        "\n",
        "\n",
        "def convert_sentences_to_features(sentences, seq_length, tokenizer):\n",
        "    features = []\n",
        "    for (idx, tok_list) in enumerate(sentences, start=1):\n",
        "        if TOKEN_LVL:\n",
        "            if UNCASED:\n",
        "                tok_list = [str(tok).lower() for tok in tok_list]\n",
        "            else:\n",
        "                tok_list = [str(tok) for tok in tok_list]\n",
        "            sentence = ' '.join(tok_list)\n",
        "        else:\n",
        "            if UNCASED:\n",
        "                sentence = tok_list.lower()\n",
        "        tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "        # +2 = [CLS] and [SEP]\n",
        "        if len(tokens) + 2 > seq_length:\n",
        "            print('Sentence ' + str(idx) + ' will be truncated; original length:', len(tokens))\n",
        "            print(sentence)\n",
        "            print(tokens)\n",
        "            tokens = tokens[0:(seq_length - 2)]\n",
        "\n",
        "        tokens.insert(0, '[CLS]')\n",
        "        tokens.append('[SEP]')\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        input_mask = [1] * len(input_ids)\n",
        "        while len(input_ids) < seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                tokens=tokens,\n",
        "                sent_idx=idx,\n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                sentence=sentence))\n",
        "    return idx, features\n",
        "\n",
        "n_sents, features = convert_sentences_to_features(sentences, MAX_LEN + 2,\n",
        "                                                  tokenizer)\n",
        "\n",
        "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "all_indices = torch.arange(n_sents, dtype=torch.long)\n",
        "\n",
        "data = TensorDataset(all_input_ids, all_input_mask, all_indices)\n",
        "sampler = SequentialSampler(data)\n",
        "dataloader = DataLoader(data, sampler=sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvGg0uZrYm-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The loop nesting is inopportune, but it is partially because of the batching\n",
        "# and partially in order to sort the output by token first\n",
        "# (instead of by layer).\n",
        "\n",
        "model.eval()\n",
        "with open(OUT_FILE, 'w', encoding='utf-8') as f:\n",
        "    for input_ids, input_mask, index_batch in dataloader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        input_mask = input_mask.to(device)\n",
        "\n",
        "        all_encoder_layers, _ = model(input_ids, token_type_ids=None, \n",
        "                                      attention_mask=input_mask)\n",
        "\n",
        "        for b, idx in enumerate(index_batch):\n",
        "            feature = features[idx.item()]\n",
        "            sent_idx = feature.sent_idx\n",
        "            if TOKEN_LVL:\n",
        "                for (tok_idx, token) in enumerate(feature.tokens):\n",
        "                    if token in ['[CLS]', '[SEP]']:\n",
        "                        continue\n",
        "                    for layer in LAYERS:\n",
        "                        layer_output = all_encoder_layers[int(layer)].detach().cpu().numpy()\n",
        "                        layer_output = layer_output[b]\n",
        "                        values = [round(x.item(), ROUNDING_ACC) for x in layer_output[tok_idx]]\n",
        "                        out = str(sent_idx) + '\\t' + str(layer) + '\\t' + token + '\\t' + str(values)\n",
        "                        f.write(out + '\\n')\n",
        "                if (sent_idx % 1000 == 0):\n",
        "                    print(out)\n",
        "            else:\n",
        "                CLS_IDX = 0  # [CLS] as representation of the entire sequence\n",
        "                for layer in LAYERS:\n",
        "                    f.write(str(sent_idx) + '\\t' + str(layer) + '\\t' + feature.sentence + '\\t')\n",
        "                    out = '['\n",
        "                    layer_output = all_encoder_layers[int(layer)].detach().cpu().numpy()\n",
        "                    layer_output = layer_output[b]\n",
        "                    for tok_idx in [CLS_IDX] + TOKEN_INDICES:\n",
        "                        values = [round(x.item(), ROUNDING_ACC) for x in layer_output[tok_idx]]\n",
        "                        out += str(values)[1:-1] + ', '\n",
        "                    f.write(out[:-2] + ']\\n')\n",
        "                if (sent_idx % 100 == 0):\n",
        "                    print(sent_idx)\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5v8wlG3vBq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUT_FILE_CMD = \"'\" + OUT_FILE + \"'\"\n",
        "!echo $OUT_FILE_CMD\n",
        "!head $OUT_FILE_CMD\n",
        "!tail $OUT_FILE_CMD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nje30h2vdTuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1/0  # To avoid running the following sections if selecting 'Run all'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuGPf58aYoaU",
        "colab_type": "text"
      },
      "source": [
        "# Appending the extracted embeddings to pre-softmax predictions\n",
        "\n",
        "For the \"BERT-base-uncased embeddings of `[CLS]` & the first 10 tokens\" runs in the feature ablation section for the technique classification system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igct7k5fYo7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine(predictions, tokens, combined):\n",
        "    with open(predictions, encoding='utf8') as fpred:\n",
        "        with open(tokens, encoding='utf8') as fcls:\n",
        "            with open(combined, 'w', encoding='utf8') as out:\n",
        "                for line_pred in fpred:\n",
        "                    cells = line_pred.split('\\t')\n",
        "                    out.write('\\t'.join(cells[:3]) + '\\t')\n",
        "                    pred = cells[3]\n",
        "                    line_cls = next(fcls)\n",
        "                    cls = line_cls.strip().split('\\t')[3]\n",
        "                    out.write(cls[:-1])\n",
        "                    out.write(', ')\n",
        "                    out.write(pred[1:])\n",
        "\n",
        "combine('gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
        "        'gdrive/My Drive/colab_projects/data/tc_train_bert-base-uncased_10.tsv',\n",
        "        '/content/gdrive/My Drive/colab_projects/data/full_bert_train.tsv')\n",
        "combine('gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
        "        'gdrive/My Drive/colab_projects/data/tc_dev_bert-base-uncased_10.tsv',\n",
        "        '/content/gdrive/My Drive/colab_projects/data/full_bert_dev.tsv')\n",
        "combine('gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
        "        'gdrive/My Drive/colab_projects/data/tc_test_bert-base-uncased_10.tsv',\n",
        "        '/content/gdrive/My Drive/colab_projects/data/full_bert_test.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VerIXJbi-o2G",
        "colab_type": "text"
      },
      "source": [
        "# Appending dev_bert-base-uncased to train_bert-base-uncased\n",
        "\n",
        "(Larger training set for the span identification task)\n",
        "\n",
        "Only run if necessary! This is slow and the resulting file is very large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-UXLVkH_egC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It's a lot faster to use the Google Drive GUI to copy & rename the file instead of doing this:\n",
        "# !cp '/content/gdrive/My Drive/colab_projects/data/train_bert-base-uncased.tsv' '/content/gdrive/My Drive/colab_projects/data/train+dev_bert-base-uncased.tsv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afQShThV_JSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LAST_TRAIN_SENT_ID = 21501\n",
        "START_SENT_ID = 22\n",
        "\n",
        "with open('/content/gdrive/My Drive/colab_projects/data/train+dev_bert-base-uncased.tsv', 'a', encoding='utf8') as f_out:\n",
        "    with open('/content/gdrive/My Drive/colab_projects/data/dev_bert-base-uncased.tsv', encoding='utf8') as f_in:\n",
        "        for line in f_in:\n",
        "            cells = line.split('\\t', 1)\n",
        "            sent_id = int(cells[0])\n",
        "            if sent_id < START_SENT_ID:\n",
        "                continue\n",
        "            # Update the sentence ID\n",
        "            f_out.write(str(LAST_TRAIN_SENT_ID + sent_id - (START_SENT_ID - 1)) + '\\t' + cells[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM8EyCVD-niN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head '/content/gdrive/My Drive/colab_projects/data/train+dev_bert-base-uncased.tsv'\n",
        "!tail '/content/gdrive/My Drive/colab_projects/data/train+dev_bert-base-uncased.tsv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abB2V4d8s5J0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In case Google Drive doesn't appear to update the files:\n",
        "# (Takes very long...)\n",
        "drive.flush_and_unmount()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}