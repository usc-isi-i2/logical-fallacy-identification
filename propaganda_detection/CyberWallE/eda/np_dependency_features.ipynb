{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a Noun Phrase Dependency Feature\n",
    "This notebook walks through the process of adding a binary Noun Phrase (NP) dependency feature to the data.\n",
    "Each token in the data is considered in the context of its original sentence in the corpus. \n",
    "SpaCy's dependency parser is used to extract the noun phrases in a sentence. \n",
    "All tokens found to be within an NP are marked with a one; otherwise, they are marked with a 0.\n",
    "\n",
    "Note: Due to no meaningful gains in final performance, this feature is not considered during ablation experiments in the system paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2xeJJ73Z7aO"
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6zyB2i9wOmP"
   },
   "source": [
    "Add better visibiity when checking head of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjbT7foUuQeW"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXQHvVuUZgcO"
   },
   "outputs": [],
   "source": [
    "def get_comments(filename, url=True):\n",
    "    if url:\n",
    "        comments = []\n",
    "        with urllib.request.urlopen(filename) as f:\n",
    "            for line in f:\n",
    "                if line.startswith(b'#'):\n",
    "                    comments.append(line.decode(\"utf-8\"))\n",
    "                else:\n",
    "                    break\n",
    "        return comments\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        commentiter = takewhile(lambda s: s.startswith('#'), f)\n",
    "        comments = list(commentiter)\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0axNlxoyWLKP"
   },
   "outputs": [],
   "source": [
    "TRAIN_DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-train.tsv?token=AFDEFD3RCBKTJKERYHW56QS6M24HK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "feVeu1ioZS7V",
    "outputId": "947d5f7d-b372-4860-965e-e302f6bc1226"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   document_id  sent_id  token_start  token_end       token label  positive  \\\n",
      "0  111111111    1        0            4          Next        O     0.000000   \n",
      "1  111111111    1        5            11         plague      O     0.071429   \n",
      "2  111111111    1        12           20         outbreak    O     0.000000   \n",
      "3  111111111    1        21           23         in          O     0.000000   \n",
      "4  111111111    1        24           34         Madagascar  O     0.000000   \n",
      "\n",
      "   negative  arglex_any  ADJ  ADP  ADV  CCONJ  DET  INTJ  NOUN  NUM  PART  \\\n",
      "0  0.031250  0           1    0    0    0      0    0     0     0    0      \n",
      "1  0.214286  0           0    0    0    0      0    0     1     0    0      \n",
      "2  0.125000  0           0    0    0    0      0    0     1     0    0      \n",
      "3  0.000000  0           0    1    0    0      0    0     0     0    0      \n",
      "4  0.000000  0           0    0    0    0      0    0     0     0    0      \n",
      "\n",
      "   PRON  PROPN  PUNCT  SYM  VERB  X  \n",
      "0  0     0      0      0    0     0  \n",
      "1  0     0      0      0    0     0  \n",
      "2  0     0      0      0    0     0  \n",
      "3  0     0      0      0    0     0  \n",
      "4  0     1      0      0    0     0  \n",
      "                                                                                                                                                                                                                                                  token\n",
      "sent_id                                                                                                                                                                                                                                                \n",
      "1        [Next, plague, outbreak, in, Madagascar, could, be, ', stronger, ', :, WHO]                                                                                                                                                                   \n",
      "2        [Geneva, -, The, World, Health, Organisation, chief, on, Wednesday, said, a, deadly, plague, epidemic, appeared, to, have, been, brought, under, control, in, Madagascar, ,, but, warned, the, next, outbreak, would, likely, be, stronger, .]\n",
      "3        [\", The, next, transmission, could, be, more, pronounced, or, stronger, ,, \", WHO, Director, -, General, Tedros, Adhanom, Ghebreyesus, told, reporters, in, Geneva, ,, insisting, that, \", the, issue, is, serious, ., \"]                     \n",
      "4        [An, outbreak, of, both, bubonic, plague, ,, which, is, spread, by, infected, rats, via, flea, bites, ,, and, pneumonic, plague, ,, spread, person, to, person, ,]                                                                            \n",
      "5        [has, killed, more, than, 200, people, in, the, Indian, Ocean, island, nation, since, August, .]                                                                                                                                              \n",
      "['document_id', 'sent_id', 'token_start', 'token_end', 'token', 'label', 'positive', 'negative', 'arglex_any', 'ADJ', 'ADP', 'ADV', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SYM', 'VERB', 'X']\n",
      "401288\n"
     ]
    }
   ],
   "source": [
    "comments = get_comments(TRAIN_DEV_URL)\n",
    "train_df = pd.read_csv(TRAIN_DEV_URL, sep='\\t', skiprows=len(comments), quoting=3)\n",
    "train_input = train_df.groupby('sent_id')['token'].apply(list).to_frame()\n",
    "print(train_df.head())\n",
    "print(train_input.head())\n",
    "print(list(train_df.columns))\n",
    "print(train_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mAD0jIZ5waKI"
   },
   "source": [
    "Concatenate tokens back into sentences so we can parse their structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "id": "LBc-dwzEnSsQ",
    "outputId": "71473718-8d88-438f-8a14-226b6ea6f054"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sent_id  \\\n",
      "0  1         \n",
      "1  2         \n",
      "2  3         \n",
      "3  4         \n",
      "4  5         \n",
      "\n",
      "                                                                                                                                                                                                                                   tokenized_sent  \\\n",
      "0  [Next, plague, outbreak, in, Madagascar, could, be, ', stronger, ', :, WHO]                                                                                                                                                                      \n",
      "1  [Geneva, -, The, World, Health, Organisation, chief, on, Wednesday, said, a, deadly, plague, epidemic, appeared, to, have, been, brought, under, control, in, Madagascar, ,, but, warned, the, next, outbreak, would, likely, be, stronger, .]   \n",
      "2  [\", The, next, transmission, could, be, more, pronounced, or, stronger, ,, \", WHO, Director, -, General, Tedros, Adhanom, Ghebreyesus, told, reporters, in, Geneva, ,, insisting, that, \", the, issue, is, serious, ., \"]                        \n",
      "3  [An, outbreak, of, both, bubonic, plague, ,, which, is, spread, by, infected, rats, via, flea, bites, ,, and, pneumonic, plague, ,, spread, person, to, person, ,]                                                                               \n",
      "4  [has, killed, more, than, 200, people, in, the, Indian, Ocean, island, nation, since, August, .]                                                                                                                                                 \n",
      "\n",
      "                                                                                                                                                                                                    full_sents  \n",
      "0  Next plague outbreak in Madagascar could be ' stronger ' : WHO                                                                                                                                               \n",
      "1  Geneva - The World Health Organisation chief on Wednesday said a deadly plague epidemic appeared to have been brought under control in Madagascar , but warned the next outbreak would likely be stronger .  \n",
      "2  \" The next transmission could be more pronounced or stronger , \" WHO Director - General Tedros Adhanom Ghebreyesus told reporters in Geneva , insisting that \" the issue is serious . \"                      \n",
      "3  An outbreak of both bubonic plague , which is spread by infected rats via flea bites , and pneumonic plague , spread person to person ,                                                                      \n",
      "4  has killed more than 200 people in the Indian Ocean island nation since August .                                                                                                                             \n"
     ]
    }
   ],
   "source": [
    "sents_df = pd.DataFrame({'sent_id':train_input['token'].index, 'tokenized_sent':train_input['token'].values})\n",
    "sents_df[\"full_sents\"]= sents_df[\"tokenized_sent\"].str.join(\" \")\n",
    "print(sents_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJb6HHWzw9aV"
   },
   "source": [
    "Check to see if we have the same number of tokenized sentences and the now concatenated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "PwamITlQBydG",
    "outputId": "82c12030-ba71-4cbb-b4c0-4ceb720539e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21501 21501\n"
     ]
    }
   ],
   "source": [
    "sents_list = sents_df['tokenized_sent'].tolist()\n",
    "full_sents_list = sents_df['full_sents'].tolist()\n",
    "print(len(sents_list), len(full_sents_list))\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEosw-DPxGrc"
   },
   "source": [
    "Iterate though the full sentences and get all the tokens in each sentence that exist in Noun Phrase chunks. Noun Phrase chunking is already provided by Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNJHGqeoNicm"
   },
   "outputs": [],
   "source": [
    "all_marked_tokens = []\n",
    "for count, sent in enumerate(full_sents_list):\n",
    "  doc = nlp(str(sent))\n",
    "  chunks_list = list(doc.noun_chunks)\n",
    "  tokens_in_sent = []\n",
    "  for chunk in chunks_list:\n",
    "    for token in chunk:\n",
    "      tokens_in_sent.append(token.text)\n",
    "  all_marked_tokens.append(tokens_in_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7msIhfUGxf7l"
   },
   "source": [
    " Match the tokens in NP chunks to the pre-tokenized sentences, marking 1 for a match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "xmaMcGUkDGR5",
    "outputId": "3c6f65f8-b95d-4610-a3a5-61e0f2fa5130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.]), array([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.]), array([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.]), array([1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 1., 0.]), array([0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.])]\n",
      "21501\n"
     ]
    }
   ],
   "source": [
    "all_isin_np_list = []\n",
    "for count, sent in enumerate(sents_list):\n",
    "  isin_np_list = np.zeros(len(sent))\n",
    "  for i, word in enumerate(sent):\n",
    "    if word in all_marked_tokens[count]:\n",
    "      isin_np_list[i] = 1\n",
    "      all_marked_tokens[count].remove(word)\n",
    "  all_isin_np_list.append(isin_np_list)\n",
    "print(all_isin_np_list[0:5])\n",
    "print(len(all_isin_np_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Tl283tFFjfu4",
    "outputId": "b2d35ba7-8e67-4187-ef47-d86207ee46d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0], [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0], [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]]\n",
      "[['Madagascar', 'has', 'suffered', 'bubonic', 'plague', 'outbreaks', 'almost', 'every', 'year', 'since', '1980', ',', 'often', 'caused', 'by', 'rats', 'fleeing', 'forest', 'fires', '.'], ['The', 'disease', 'tends', 'to', 'make', 'a', 'comeback', 'each', 'hot', 'rainy', 'season', ',', 'from', 'September', 'to', 'April', '.'], ['On', 'average', ',', 'between', '300', 'and', '600', 'infections', 'are', 'recorded', 'every', 'year', 'among', 'a', 'population', 'approaching', '25', 'million', 'people', ',', 'according', 'to', 'a', 'UN', 'estimate', '.'], ['But', 'Tedros', 'voiced', 'alarm', 'that', '\"', 'plague', 'in', 'Madagascar', 'behaved', 'in', 'a', 'very', ',', 'very', 'different', 'way', 'this', 'year', '.', '\"'], ['Cases', 'sprang', 'up', 'far', 'earlier', 'than', 'usual', 'and', ',', 'instead', 'of', 'being', 'confined', 'to', 'the', 'countryside', ',', 'the', 'disease', 'infiltrated', 'towns', '.']]\n",
      "['Madagascar has suffered bubonic plague outbreaks almost every year since 1980 , often caused by rats fleeing forest fires .', 'The disease tends to make a comeback each hot rainy season , from September to April .', 'On average , between 300 and 600 infections are recorded every year among a population approaching 25 million people , according to a UN estimate .', 'But Tedros voiced alarm that \" plague in Madagascar behaved in a very , very different way this year . \"', 'Cases sprang up far earlier than usual and , instead of being confined to the countryside , the disease infiltrated towns .']\n"
     ]
    }
   ],
   "source": [
    "np_list = [x.tolist() for x in all_isin_np_list]\n",
    "print(np_list[5:10])\n",
    "print(sents_list[5:10])\n",
    "print(full_sents_list[5:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LNRWOyBjyZCf"
   },
   "source": [
    "Flattening to merge easily as single dataframe column corresponding one-to-one with each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ns2aGb7VrSUn"
   },
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in np_list for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xaCFhPUx1Fa"
   },
   "source": [
    "The number of 0's and 1's equals the number of tokens in our original train+dev dataframe! Every token has been checked to be in an NP chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_0KmbUuErmJ-",
    "outputId": "f81df913-aca5-4c91-e6d3-94897de27d5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401288"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3p1jSNmHsAF2"
   },
   "outputs": [],
   "source": [
    "isin_np_df = pd.DataFrame({'isin_np':flat_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WA2XWHUvs2_Z"
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(isin_np_df, train_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sp9mRxxdttUZ"
   },
   "outputs": [],
   "source": [
    "merged_df_1 = merged_df[['document_id', 'sent_id', 'token_start', 'token_end', 'token', 'label', 'positive', 'negative', 'arglex_any','ADJ', 'ADP', 'ADV', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SYM', 'VERB', 'X', 'isin_np']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "colab_type": "code",
    "id": "70xlQXMFtbgD",
    "outputId": "52691899-7477-4374-fc9b-c5c6ba0322b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document_id', 'sent_id', 'token_start', 'token_end', 'token', 'label', 'positive', 'negative', 'arglex_any', 'ADJ', 'ADP', 'ADV', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SYM', 'VERB', 'X', 'isin_np']\n",
      "   document_id  sent_id  token_start  token_end       token label  positive  \\\n",
      "0  111111111    1        0            4          Next        O     0.000000   \n",
      "1  111111111    1        5            11         plague      O     0.071429   \n",
      "2  111111111    1        12           20         outbreak    O     0.000000   \n",
      "3  111111111    1        21           23         in          O     0.000000   \n",
      "4  111111111    1        24           34         Madagascar  O     0.000000   \n",
      "\n",
      "   negative  arglex_any  ADJ  ADP  ADV  CCONJ  DET  INTJ  NOUN  NUM  PART  \\\n",
      "0  0.031250  0           1    0    0    0      0    0     0     0    0      \n",
      "1  0.214286  0           0    0    0    0      0    0     1     0    0      \n",
      "2  0.125000  0           0    0    0    0      0    0     1     0    0      \n",
      "3  0.000000  0           0    1    0    0      0    0     0     0    0      \n",
      "4  0.000000  0           0    0    0    0      0    0     0     0    0      \n",
      "\n",
      "   PRON  PROPN  PUNCT  SYM  VERB  X  isin_np  \n",
      "0  0     0      0      0    0     0  1.0      \n",
      "1  0     0      0      0    0     0  1.0      \n",
      "2  0     0      0      0    0     0  1.0      \n",
      "3  0     0      0      0    0     0  0.0      \n",
      "4  0     1      0      0    0     0  1.0      \n"
     ]
    }
   ],
   "source": [
    "print(list(merged_df_1.columns))\n",
    "print(merged_df_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOWl5WiebVIv"
   },
   "outputs": [],
   "source": [
    "#merged_df_1 = merged_df_1.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "JtQ8vAgvc_Y6",
    "outputId": "2facb922-6562-405d-b0f3-22fd6617bdb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   document_id  sent_id  token_start  token_end       token label  positive  \\\n",
      "0  111111111    1        0            4          Next        O     0.000000   \n",
      "1  111111111    1        5            11         plague      O     0.071429   \n",
      "2  111111111    1        12           20         outbreak    O     0.000000   \n",
      "3  111111111    1        21           23         in          O     0.000000   \n",
      "4  111111111    1        24           34         Madagascar  O     0.000000   \n",
      "\n",
      "   negative  arglex_any  ADJ  ADP  ADV  CCONJ  DET  INTJ  NOUN  NUM  PART  \\\n",
      "0  0.031250  0           1    0    0    0      0    0     0     0    0      \n",
      "1  0.214286  0           0    0    0    0      0    0     1     0    0      \n",
      "2  0.125000  0           0    0    0    0      0    0     1     0    0      \n",
      "3  0.000000  0           0    1    0    0      0    0     0     0    0      \n",
      "4  0.000000  0           0    0    0    0      0    0     0     0    0      \n",
      "\n",
      "   PRON  PROPN  PUNCT  SYM  VERB  X  isin_np  \n",
      "0  0     0      0      0    0     0  1        \n",
      "1  0     0      0      0    0     0  1        \n",
      "2  0     0      0      0    0     0  1        \n",
      "3  0     0      0      0    0     0  0        \n",
      "4  0     1      0      0    0     0  1        \n"
     ]
    }
   ],
   "source": [
    "cols = ['isin_np', 'X']\n",
    "merged_df_1[cols] = merged_df_1[cols].applymap(np.int64)\n",
    "print(merged_df_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U4N4gvqxug1R"
   },
   "outputs": [],
   "source": [
    "file = merged_df_1.to_csv(\"si-train+dependency.tsv\", sep='\\t',index=False)\n",
    "from google.colab import files\n",
    "files.download(\"si-train+dependency.tsv\") "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "np_dependency_features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
