{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_sequence_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ef2OZhqdX0",
        "colab_type": "text"
      },
      "source": [
        "Creates BertForSequenceClassification predictions for the Technique Classification (TC) task.\n",
        "This is where we extract the pre-softmax embeddings for the *base model* of our TC system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlPBMsP2nX8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdpigBNWorJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DWPQ8clozwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS9qKJinLZ7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### CONFIG ###\n",
        "TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/tc-train-repetition.tsv?token=AD7GEDPHP327J4M4QTVBV3K6NVUAE'\n",
        "DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/tc-dev-repetition.tsv?token=AD7GEDK2P4NPZCM32L5M4BK6NVUEA'\n",
        "TEST_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/tc-test-repetition.tsv?token=AD7GEDIUC2H5665EBOKWRNK6NVUE6'\n",
        "\n",
        "MAX_LEN = 200\n",
        "BATCH_SIZE = 12\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP = .1\n",
        "N_EPOCHS = 5  # 2-4 recommended\n",
        "BERT_MODEL = 'bert-base-uncased'\n",
        "\n",
        "ROUNDING_ACC = 9\n",
        "# Can be 'all', 'last' or a list of epoch numbers:\n",
        "PREDICTION_EPOCHS = [2, 3, 4, 5]\n",
        "SAVE_LAYER_EPOCHS = []  \n",
        "\n",
        "\n",
        "UNCASED = 'uncased' in BERT_MODEL\n",
        "FILE_PREFIX = 'gdrive/My Drive/colab_projects/'\n",
        "NOW = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
        "LOG_FILE = FILE_PREFIX + 'semeval-predictions/log_bert_' + NOW + '.txt'\n",
        "BERT_TRAIN_PFX = FILE_PREFIX + 'data/tc_train_' + NOW + '_'\n",
        "BERT_DEV_PFX = FILE_PREFIX + 'data/tc_dev_' + NOW + '_'\n",
        "BERT_TEST_PFX = FILE_PREFIX + 'data/tc_test_' + NOW + '_'\n",
        "PREDICTIONS_DEV_PFX = FILE_PREFIX + 'semeval-predictions/labels_dev_' + NOW + '_'\n",
        "PREDICTIONS_TEST_PFX = FILE_PREFIX + 'semeval-predictions/labels_test_' + NOW + '_'\n",
        "##############"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS4_qt4RMbBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(url, training=True):\n",
        "    df = pd.read_csv(url, sep='\\t', quoting=3, usecols=[0, 1, 2, 3, 4])\n",
        "    labels = None\n",
        "    label_encoder = None\n",
        "    if training:\n",
        "        label_encoder = LabelEncoder()\n",
        "        labels = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in df.text.values]\n",
        "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL,\n",
        "                                              do_lower_case=not UNCASED)\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                              truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    attention_masks = []\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # Used for extracting the data in the right order:\n",
        "    spans = df.text.tolist()\n",
        "    span_ids = list(range(len(spans)))\n",
        "\n",
        "    if UNCASED:\n",
        "        spans = [span.lower() for span in spans]\n",
        "\n",
        "    if training:\n",
        "        data = TensorDataset(torch.tensor(input_ids),\n",
        "                             torch.tensor(attention_masks),\n",
        "                             torch.tensor(labels),\n",
        "                             torch.tensor(span_ids))\n",
        "        sampler = RandomSampler(data)\n",
        "    else:\n",
        "        data = TensorDataset(torch.tensor(input_ids),\n",
        "                             torch.tensor(attention_masks))\n",
        "        sampler = SequentialSampler(data)\n",
        "\n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "    return df, label_encoder, dataloader, spans\n",
        "\n",
        "_, label_encoder, train_dataloader, train_spans = get_data(TRAIN_URL)\n",
        "dev_df, _, dev_dataloader, dev_spans = get_data(DEV_URL, training=False)\n",
        "del dev_df['text']\n",
        "test_df, _, test_dataloader, test_spans = get_data(TEST_URL, training=False)\n",
        "del test_df['text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7joGviRRxW4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=14)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEXb9Z3fyz26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=LEARNING_RATE,\n",
        "                     warmup=WARMUP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjEETvuiVJHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, dataloader, save_layer_rep, layer_file_pfx, pred_file_pfx,\n",
        "            epoch, spans, df):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)  # Add batch to GPU\n",
        "        b_input_ids, b_input_mask = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "            logits = model(b_input_ids, token_type_ids=None,\n",
        "                           attention_mask=b_input_mask)\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()  # Move logits and labels to CPU\n",
        "        preds.append(logits)\n",
        "\n",
        "    predictions = [item for sublist in preds for item in sublist]\n",
        "    flat_predictions = np.argmax(predictions, axis=1).flatten()\n",
        "\n",
        "    if save_layer_rep:\n",
        "        with open(layer_file_pfx + str(epoch) + '.tsv', 'w',\n",
        "                    encoding='utf-8') as f:\n",
        "            for pred, span in zip(predictions, spans):\n",
        "                f.write('1\\tclass\\t' + span + '\\t')\n",
        "                values = [round(x, ROUNDING_ACC) for x in pred]\n",
        "                f.write(str(values) + '\\n')\n",
        "\n",
        "    predicted_labels = label_encoder.inverse_transform(flat_predictions)\n",
        "    df['label'] = predicted_labels\n",
        "    df.to_csv(pred_file_pfx + str(epoch) + '.txt', sep='\\t',\n",
        "              header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bNObYYXzSRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_steps = []\n",
        "train_loss_epochs = []\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch', epoch, end=' ')\n",
        "    make_prediction = (PREDICTION_EPOCHS == 'all'\n",
        "                       or epoch in PREDICTION_EPOCHS\n",
        "                       or (PREDICTION_EPOCHS == 'last' and epoch == N_EPOCHS))\n",
        "    save_layer_rep = (SAVE_LAYER_EPOCHS == 'all'\n",
        "                      or epoch in SAVE_LAYER_EPOCHS\n",
        "                      or (SAVE_LAYER_EPOCHS == 'last' and epoch == N_EPOCHS))\n",
        "    entries = []\n",
        "\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)  # Add batch to GPU\n",
        "\n",
        "        b_input_ids, b_input_mask, b_labels, b_span_ids = batch\n",
        "\n",
        "        optimizer.zero_grad()  # Clear out the gradients (by default they accumulate)\n",
        "        # Forward pass\n",
        "        loss = model(b_input_ids, token_type_ids=None,\n",
        "                     attention_mask=b_input_mask, labels=b_labels)\n",
        "        train_loss_steps.append(loss.item())    \n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update parameters via gradient\n",
        "        \n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1 \n",
        "\n",
        "        if save_layer_rep:\n",
        "            # Save predictions (pre-softmax)\n",
        "            layers = model(b_input_ids, token_type_ids=None,\n",
        "                           attention_mask=b_input_mask)\n",
        "            b_span_ids = b_span_ids.tolist()\n",
        "            for entry in range(layers.size(0)):\n",
        "                predictions = layers[entry].detach().cpu().numpy()\n",
        "                values = [round(x, ROUNDING_ACC) for x in predictions]\n",
        "                entries.append((b_span_ids[entry],\n",
        "                                train_spans[b_span_ids[entry]],\n",
        "                                str(values)))\n",
        "                \n",
        "        if step % 50 == 0:\n",
        "            print('.', end='')\n",
        "\n",
        "    print('\\tTraining loss: {}'.format(tr_loss / nb_tr_steps))\n",
        "    train_loss_epochs.append(tr_loss / nb_tr_steps)\n",
        "\n",
        "    if save_layer_rep:\n",
        "        entries = sorted(entries, key=lambda entry: entry[0])\n",
        "        with open(BERT_TRAIN_PFX + str(epoch) + '.tsv', 'w',\n",
        "                  encoding='utf-8') as f:\n",
        "            for entry in entries:\n",
        "                f.write('1\\tclass\\t' + entry[1] + '\\t' + entry[2] + '\\n')\n",
        "\n",
        "    if make_prediction:\n",
        "        predict(model, dev_dataloader, save_layer_rep, BERT_DEV_PFX,\n",
        "                PREDICTIONS_DEV_PFX, epoch, dev_spans, dev_df)\n",
        "        predict(model, test_dataloader, save_layer_rep, BERT_TEST_PFX,\n",
        "                PREDICTIONS_TEST_PFX, epoch, test_spans, test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7N4mO-pU5mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(LOG_FILE, 'w', encoding='utf-8') as f:\n",
        "    f.write('BERT_MODEL: ' + str(BERT_MODEL) + '\\n')\n",
        "    f.write('MAX_LEN: ' + str(MAX_LEN) + '\\n')\n",
        "    f.write('BATCH_SIZE: ' + str(BATCH_SIZE) + '\\n')\n",
        "    f.write('LEARNING_RATE: ' + str(LEARNING_RATE) + '\\n')\n",
        "    f.write('WARMUP: ' + str(WARMUP) + '\\n')\n",
        "    f.write('N_EPOCHS: ' + str(N_EPOCHS) + '\\n')\n",
        "    f.write('TRAIN LOSS BY EPOCH: ' + str(train_loss_epochs))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}