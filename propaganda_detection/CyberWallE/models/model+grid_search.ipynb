{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VubK6qZQUmeM"
   },
   "source": [
    "# CyberWallE at SemEval-2020 Task 11\n",
    "(V. Blaschke, M. Korniyenko & S. Tureski, 2020)\n",
    "\n",
    "This file contains the main script for the system for subtask 1 (span identification) and the base model for task 2 (technique classification), as well as the feature ablation configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5HfEvv3STwo"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-tBTO_r1Fv5x"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_1W_TMlZXll"
   },
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_uUPU9_5SZ9h"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import takewhile\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from keras.layers import Bidirectional, CuDNNLSTM, Dense, Dropout, \\\n",
    "    TimeDistributed, Activation\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn import svm, preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding, Conv1D\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "########################\n",
    "# Processing the input #\n",
    "########################\n",
    "\n",
    "\n",
    "# Helper method for prepare_data\n",
    "def get_comments(filename, url=True):\n",
    "    if url:\n",
    "        comments = []\n",
    "        with urllib.request.urlopen(filename) as f:\n",
    "            for line in f:\n",
    "                if line.startswith(b'#'):\n",
    "                    comments.append(line.decode(\"utf-8\"))\n",
    "                else:\n",
    "                    break\n",
    "        return comments\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        commentiter = takewhile(lambda s: s.startswith('#'), f)\n",
    "        comments = list(commentiter)\n",
    "    return comments\n",
    "\n",
    "\n",
    "# Helper method for prepare_data\n",
    "def get_cols(input_df, col):\n",
    "    return input_df.groupby('sent_id')[col].apply(list).to_frame()\n",
    "\n",
    "\n",
    "# Helper method for prepare_data\n",
    "def add_sent_lens(input_df, col='token'):\n",
    "    input_df['n_toks'] = input_df[col].apply(lambda x: len(x))\n",
    "    return input_df\n",
    "\n",
    "\n",
    "# Helper method for prepare_data\n",
    "def get_features(input_df, feature_cols):\n",
    "    x = add_sent_lens(get_cols(input_df, 'token'))\n",
    "    for feature in feature_cols:\n",
    "        x = pd.merge(left=x, right=get_cols(input_df, feature),\n",
    "                     left_on='sent_id', right_on='sent_id')\n",
    "    return x\n",
    "\n",
    "\n",
    "# Helper method for encode_x_bert\n",
    "def bert_embeddings_for_sent(bert_tokens, row, feature_header, embedding_matrix,\n",
    "                             embed_dim, sent_idx, uncased):\n",
    "    if len(bert_tokens) < len(row.token):\n",
    "        # No need to worry if this prints something about '\\ufeff'\n",
    "        print('BERT', [i[0] for i in bert_tokens])\n",
    "        print('X', row.token)\n",
    "    word_idx = 0\n",
    "    for (tok, embed) in bert_tokens:\n",
    "        if word_idx == row.n_toks:\n",
    "            break\n",
    "        word = str(row.token[word_idx])\n",
    "        if word == '\\ufeff':  # Prints a warning, but is dealt with.\n",
    "            word_idx += 1\n",
    "            continue\n",
    "        if uncased:\n",
    "            word = word.lower()\n",
    "        if tok == word or word.startswith(tok):\n",
    "            # startswith: Use embedding of first subtoken\n",
    "            embedding_matrix[sent_idx - 1][word_idx][:embed_dim] = embed\n",
    "            for i, feature in enumerate(feature_header):\n",
    "                embedding_matrix[sent_idx - 1][word_idx][embed_dim + i] = \\\n",
    "                    getattr(row, feature)[word_idx]\n",
    "            word_idx += 1\n",
    "            continue\n",
    "        if tok.startswith('##') and not word.startswith('##'):\n",
    "            # BERT word continutation prefix (e.g. per ##pet ##uate)\n",
    "            continue\n",
    "\n",
    "\n",
    "# Task 1: Token embeddings\n",
    "def encode_x_bert(x, bert_file, feature_header, max_seq_len, embed_dim=768,\n",
    "                  uncased=True):\n",
    "    # TODO this currently assumes that the BERT file only contains information\n",
    "    # about a single layer. extend this to multiple layers?\n",
    "    embedding_matrix = np.zeros([len(x), max_seq_len,\n",
    "                                 embed_dim + len(feature_header)])\n",
    "    prev_sent_idx = 1\n",
    "    bert_tokens = []\n",
    "    sentences = x.itertuples()\n",
    "    with open(bert_file, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            cells = line.split('\\t')\n",
    "            sent_idx = int(cells[0])\n",
    "            layer = int(cells[1])\n",
    "            token = cells[2]\n",
    "            embedding = np.fromstring(cells[3][1:-1], sep=',')\n",
    "\n",
    "            if sent_idx != prev_sent_idx:\n",
    "                if sent_idx % 1000 == 0:\n",
    "                    print(\"BERT embeddings for sentence\", sent_idx)\n",
    "                row = next(sentences)\n",
    "                assert row.Index == prev_sent_idx\n",
    "                bert_embeddings_for_sent(bert_tokens, row, feature_header,\n",
    "                                         embedding_matrix, embed_dim,\n",
    "                                         prev_sent_idx, uncased)\n",
    "                bert_tokens = []\n",
    "\n",
    "            bert_tokens.append((token, embedding))\n",
    "            prev_sent_idx = sent_idx\n",
    "\n",
    "    # Last line:\n",
    "    row = next(sentences)\n",
    "    bert_embeddings_for_sent(bert_tokens, row, feature_header, embedding_matrix,\n",
    "                             embed_dim, prev_sent_idx, uncased)\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "# Task 2: Sequence embeddings\n",
    "def encode_x_seq(x, bert_file, feature_header, embed_dim=768, uncased=True,\n",
    "                 n_bert_layers=1):\n",
    "    embedding_matrix = np.zeros([len(x),\n",
    "                                 embed_dim * n_bert_layers + len(feature_header)])\n",
    "    prev_sent_idx = 1\n",
    "    bert_tokens = []\n",
    "    sequences = x.itertuples()\n",
    "    with open(bert_file, encoding='utf8') as f:\n",
    "        idx = 0\n",
    "        for line in f:\n",
    "            row = next(sequences)\n",
    "            for bert_layer in range(n_bert_layers):\n",
    "                cells = line.split('\\t')\n",
    "                sent_idx = int(cells[0])\n",
    "                layer = cells[1]\n",
    "                seq = cells[2]\n",
    "                embedding = np.fromstring(cells[3][1:-1], sep=',')\n",
    "                text = row.text\n",
    "                if uncased:\n",
    "                    text = text.lower()\n",
    "                # assert text == seq or text + ' ' + text == seq\n",
    "                embedding_matrix[idx][embed_dim * bert_layer:embed_dim * (bert_layer + 1)] = embedding\n",
    "                if n_bert_layers > 1 and bert_layer < n_bert_layers - 1:\n",
    "                    line = next(f)\n",
    "            for i, feature in enumerate(feature_header):\n",
    "                embedding_matrix[idx][embed_dim * n_bert_layers + i] = getattr(row, feature)\n",
    "            idx += 1\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def encode_x(x, word2embedding, feature_header, max_seq_len,\n",
    "             embed_dim, uncased):\n",
    "    \"\"\"Encode the input data.\n",
    "\n",
    "    Arguments:\n",
    "    x -- a Pandas dataframe\n",
    "    word2embedding -- a dict(str -> np.array) from tokens to embeddings\n",
    "    feature_header -- dataframe names of additional feature columns\n",
    "    max_seq_len -- the maximum number of tokens per sentence in x\n",
    "    embed_dim -- the array length of the vectors in word2embedding\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros([len(x), max_seq_len,\n",
    "                                 embed_dim + len(feature_header)])\n",
    "    for row in x.itertuples():\n",
    "        sent_idx = row.Index - 1\n",
    "        for tok_idx in range(row.n_toks):\n",
    "            word = str(row.token[tok_idx])\n",
    "            if uncased:\n",
    "                word = word.lower()\n",
    "            embedding_matrix[sent_idx][tok_idx][:embed_dim] = \\\n",
    "                word2embedding.get(word, np.random.randn(embed_dim))\n",
    "            for i, feature in enumerate(feature_header):\n",
    "                embedding_matrix[sent_idx][tok_idx][embed_dim + i] = \\\n",
    "                    getattr(row, feature)[tok_idx]\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def encode_y(y, label2idx, max_seq_len, n_classes):\n",
    "    if n_classes == 1:\n",
    "        if max_seq_len > 1:\n",
    "            labels = np.zeros([len(y), max_seq_len])\n",
    "        else:\n",
    "            labels = np.zeros(len(y))\n",
    "    else:\n",
    "        labels = np.zeros([len(y), max_seq_len, n_classes])\n",
    "\n",
    "    if max_seq_len > 1:\n",
    "        for row in y.itertuples():\n",
    "            sent_idx = row.Index - 1\n",
    "            for tok_idx, label in enumerate(row.label):\n",
    "                labels[sent_idx][tok_idx] = label2idx[label]\n",
    "    else:\n",
    "        for row in y.iteritems():\n",
    "            labels[row[0]] = label2idx[row[1]]\n",
    "    return labels\n",
    "\n",
    "\n",
    "def prepare_data(config, word2embedding, phase):\n",
    "    # We're getting the comments this way so we can:\n",
    "    # - add them to the output\n",
    "    # - parse lines that actually contain '#' as token\n",
    "    if phase == 'train':\n",
    "        infile = config.TRAIN_URL\n",
    "    elif phase == 'dev':\n",
    "        infile = config.DEV_URL\n",
    "    elif phase == 'test':\n",
    "        infile = config.TEST_URL\n",
    "    comments = get_comments(infile, config.ONLINE_SOURCES)\n",
    "    df = pd.read_csv(infile, sep='\\t', skiprows=len(comments), quoting=3,\n",
    "                     encoding='utf8')\n",
    "    \n",
    "    if config.TOKEN_LVL:\n",
    "        std_cols = ['document_id', 'sent_id', 'token_start',\n",
    "                    'token_end', 'token', 'label']\n",
    "    else:\n",
    "        std_cols = ['document_id', 'span_start', 'span_end', 'text', 'label']\n",
    "    feature_cols = []\n",
    "    for col in df.columns:\n",
    "        if col in config.EXCLUDE_FEATURES:\n",
    "            continue\n",
    "        if config.FEATURES is None:  # Determine features based on file header\n",
    "            if col not in std_cols:\n",
    "                feature_cols.append(col)\n",
    "        else:\n",
    "            if col in config.FEATURES:\n",
    "                feature_cols.append(col)\n",
    "\n",
    "    if config.TOKEN_LVL:\n",
    "        x_raw = get_features(df, feature_cols)\n",
    "    else:\n",
    "        x_raw = df\n",
    "\n",
    "    if config.USE_BERT:\n",
    "        if phase == 'train':\n",
    "            bert_file = config.TRAIN_BERT\n",
    "        elif phase == 'dev':\n",
    "            bert_file = config.DEV_BERT\n",
    "        elif phase == 'test':\n",
    "            bert_file = config.TEST_BERT\n",
    "        if config.TOKEN_LVL:\n",
    "            x_enc = encode_x_bert(x_raw, bert_file, feature_cols,\n",
    "                                  config.MAX_SEQ_LEN, config.EMBED_DIM,\n",
    "                                  config.UNCASED)\n",
    "        else:\n",
    "            x_enc = encode_x_seq(x_raw, bert_file, feature_cols, \n",
    "                                 config.EMBED_DIM, config.UNCASED,\n",
    "                                 config.N_BERT_LAYERS)\n",
    "    else:\n",
    "        x_enc = encode_x(x_raw, word2embedding, feature_cols,\n",
    "                     config.MAX_SEQ_LEN, config.EMBED_DIM, config.UNCASED)\n",
    "        \n",
    "    \n",
    "    print(x_enc.shape)\n",
    "\n",
    "    y = None\n",
    "    sample_weight = None\n",
    "    if phase == 'train':\n",
    "        if config.TOKEN_LVL:\n",
    "            y_raw = get_cols(df, 'label')\n",
    "            if config.N_CLASSES == 3:\n",
    "                label2idx = {\"O\": [1, 0, 0], \"B\": [0, 0, 1], \"I\": [0, 1, 0]}\n",
    "            elif config.N_CLASSES == 2:\n",
    "                label2idx = {\"O\": [1, 0], \"B\": [0, 1], \"I\": [0, 1]}\n",
    "            y = encode_y(y_raw, label2idx, config.MAX_SEQ_LEN, config.N_CLASSES)\n",
    "            sample_weight = encode_y(y_raw, config.CLASS_WEIGHTS,\n",
    "                                     config.MAX_SEQ_LEN, n_classes=1)\n",
    "        else:\n",
    "            y = df.label\n",
    "            if config.CLASS_WEIGHTS:\n",
    "                sample_weight = encode_y(y, config.CLASS_WEIGHTS,\n",
    "                                         config.MAX_SEQ_LEN, n_classes=1)\n",
    "\n",
    "    return df, x_raw, x_enc, y, sample_weight, comments, feature_cols\n",
    "\n",
    "\n",
    "def load_zipped_embeddings(infile):\n",
    "    word2embedding = {}\n",
    "    with zipfile.ZipFile(infile) as f_in_zip:\n",
    "        file_in = f_in_zip.filelist[0].filename\n",
    "        i = 0\n",
    "        with f_in_zip.open(file_in, 'r') as f_in:\n",
    "            for line in f_in:\n",
    "                values = line.decode().rstrip().split()\n",
    "                word2embedding[values[0]] = np.asarray(values[1:],\n",
    "                                                       dtype='float32')\n",
    "                i += 1\n",
    "                if i % 100000 == 0:\n",
    "                    print(\"Read \" + str(i) + \" embeddings\")\n",
    "    return word2embedding\n",
    "\n",
    "\n",
    "def get_data(config, word2embedding=None):\n",
    "    if (not word2embedding) and (not config.USE_BERT):\n",
    "        if config.EMBEDDING_PATH[-4:] == '.zip':\n",
    "            word2embedding = load_zipped_embeddings(config.EMBEDDING_PATH)\n",
    "        else:\n",
    "            word2embedding = {}\n",
    "            f = open(config.EMBEDDING_PATH)\n",
    "            for line in f:\n",
    "                values = line.rstrip().split()\n",
    "                word2embedding[values[0]] = np.asarray(values[1:],\n",
    "                                                       dtype='float32')\n",
    "            f.close()\n",
    "\n",
    "    _, _, train_x, train_y, sample_weight, comments, features = prepare_data(\n",
    "        config, word2embedding, phase='train')\n",
    "    dev_df, dev_raw, dev_x, _, _, _, _ = prepare_data(config, word2embedding,\n",
    "                                                      phase='dev')\n",
    "    if config.TEST_URL:\n",
    "        test_df, test_raw, test_x, _, _, _, _ = prepare_data(config,\n",
    "                                                             word2embedding,\n",
    "                                                             phase='test')\n",
    "    else:\n",
    "        test_df, test_raw, test_x = None, None, None\n",
    "    return Data(train_x, train_y,dev_df, dev_raw, dev_x, test_df, test_raw,\n",
    "                test_x, sample_weight, comments, features)\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self,\n",
    "                 # If initializing on the fly:\n",
    "                 train_x=None, train_y=None,\n",
    "                 dev_df=None, dev_raw=None, dev_x=None,\n",
    "                 test_df=None, test_raw=None, test_x=None,\n",
    "                 sample_weight=None, comments=None, features=None,\n",
    "                 # If initializing from files:\n",
    "                 path=None):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.sample_weight = sample_weight\n",
    "        self.comments = comments\n",
    "        self.features = features\n",
    "        self.dev_df = dev_df\n",
    "        self.dev_raw = dev_raw\n",
    "        self.dev_x = dev_x\n",
    "        self.test_df = test_df\n",
    "        self.test_raw = test_raw\n",
    "        self.test_x = test_x\n",
    "        if path:\n",
    "            self.load(path)\n",
    "\n",
    "\n",
    "    def save(self, path='gdrive/My Drive/colab_projects/data/data/'):\n",
    "        np.save(path + 'train_x', self.train_x)\n",
    "        np.save(path + 'train_y', self.train_y)\n",
    "        np.save(path + 'dev_x', self.dev_x)\n",
    "        np.save(path + 'test_x', self.test_x)\n",
    "        np.save(path + 'sample_weight', self.sample_weight)\n",
    "        self.dev_raw.to_csv(path + 'dev_raw')\n",
    "        self.dev_df.to_csv(path + 'dev_df')\n",
    "        self.test_raw.to_csv(path + 'test_raw')\n",
    "        self.test_df.to_csv(path + 'test_df')\n",
    "        with open(path + 'comments.txt', 'w', encoding='utf8') as f:\n",
    "            for comment in self.comments:\n",
    "                f.write(comment + '\\n')\n",
    "        with open(path + 'features.txt', 'w', encoding='utf8') as f:\n",
    "            for feature in self.features:\n",
    "                f.write(feature + '\\n')\n",
    "\n",
    "\n",
    "    def load(self, path='gdrive/My Drive/colab_projects/data/data/'):\n",
    "        self.train_x = np.load(path + 'train_x.npy')\n",
    "        self.train_y = np.load(path + 'train_y.npy')\n",
    "        self.dev_x = np.load(path + 'dev_x.npy')\n",
    "        self.test_x = np.load(path + 'test_x.npy')\n",
    "        self.sample_weight = np.load(path + 'sample_weight.npy')\n",
    "        self.dev_raw = pd.read_csv(path + 'dev_raw')\n",
    "        self.dev_df = pd.read_csv(path + 'dev_df')\n",
    "        self.test_raw = pd.read_csv(path + 'test_raw')\n",
    "        self.test_df = pd.read_csv(path + 'test_df')\n",
    "        self.comments =[]\n",
    "        with open(path + 'comments.txt', 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    self.comments.append(line)\n",
    "        self.features =[]\n",
    "        with open(path + 'features.txt', 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    self.features.append(line)\n",
    "\n",
    "\n",
    "######################\n",
    "# Creating the model #\n",
    "######################]\n",
    "\n",
    "def get_svm(train_x, train_y):\n",
    "    model = svm.SVC(decision_function_shape='ovo')\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_xgb(train_x, train_y):\n",
    "    model = XGBClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ffnn(config, train_x, train_y, sample_weight, single_layer=False):\n",
    "    y_encoder = preprocessing.OneHotEncoder()\n",
    "    train_y_enc = y_encoder.fit_transform(train_y.to_numpy().reshape(-1, 1))\n",
    "    model = Sequential()\n",
    "    if single_layer:\n",
    "        model.add(Dense(config.N_CLASSES, input_dim=train_x.shape[1]))\n",
    "    else:\n",
    "        model.add(Dense(config.HIDDEN, activation='relu',\n",
    "                        input_dim=train_x.shape[1]))\n",
    "        model.add(Dropout(config.DROPOUT))\n",
    "        model.add(Dense(config.N_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss=config.LOSS, optimizer=config.OPTIMIZER,\n",
    "                  metrics=[config.METRIC])\n",
    "    history = model.fit(train_x, train_y_enc, epochs=config.EPOCHS,\n",
    "                        batch_size=config.BATCH_SIZE,\n",
    "                        sample_weight=sample_weight, verbose=1)\n",
    "    return model, history, y_encoder\n",
    "\n",
    "\n",
    "def get_bilstm(config, train_x, train_y, sample_weight):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(CuDNNLSTM(config.LSTM_UNITS,\n",
    "                                      return_sequences=True),\n",
    "                            input_shape=train_x.shape[1:]))\n",
    "    model.add(Dropout(config.DROPOUT))\n",
    "    model.add(TimeDistributed(Dense(config.N_CLASSES, activation='softmax')))\n",
    "    model.compile(loss=config.LOSS, optimizer=config.OPTIMIZER,\n",
    "                  metrics=[config.METRIC], sample_weight_mode='temporal')\n",
    "    history = model.fit(train_x, train_y, epochs=config.EPOCHS,\n",
    "                        batch_size=config.BATCH_SIZE,\n",
    "                        sample_weight=sample_weight, verbose=1)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def get_cnn(config, train_x, train_y, sample_weight):\n",
    "    embedding_layer = Embedding(config.VOCAB_SIZE,\n",
    "                              768,\n",
    "                              input_length=train_x.shape[1],\n",
    "                              trainable=False)\n",
    "    y_encoder = preprocessing.OneHotEncoder()\n",
    "    train_y_enc = y_encoder.fit_transform(train_y.to_numpy().reshape(-1, 1))\n",
    "\n",
    "    sequence_input = Input(shape=(train_x.shape[1],), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Dropout(config.DROPOUT)(embedded_sequences)\n",
    "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(35)(x)  \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(config.DROPOUT)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(config.N_CLASSES, activation='softmax')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss=config.LOSS, optimizer=config.OPTIMIZER,\n",
    "                  metrics=[config.METRIC])\n",
    "\n",
    "    history = model.fit(train_x, train_y_enc, epochs=config.EPOCHS,\n",
    "                        batch_size=config.BATCH_SIZE,\n",
    "                        sample_weight=sample_weight, verbose=2)\n",
    "    \n",
    "    return model, history, y_encoder\n",
    "\n",
    "\n",
    "def get_kimcnn(config, train_x, train_y, sample_weight):\n",
    "    '''Inspired by Alexander Rakhlin's keras implementation of Yoon Kim's paper \n",
    "    \"Convolutional Neural Networks for Sentence Classification\"\n",
    "    Repository: https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras\n",
    "    Kim's paper: http://arxiv.org/pdf/1408.5882v2.pdf'''\n",
    "    embedding_layer = Embedding(config.VOCAB_SIZE,\n",
    "                              768,\n",
    "                              input_length=train_x.shape[1],\n",
    "                              trainable=False)\n",
    "    y_encoder = preprocessing.OneHotEncoder()\n",
    "    train_y_enc = y_encoder.fit_transform(train_y.to_numpy().reshape(-1, 1))\n",
    "    sequence_input = Input(shape=(train_x.shape[1],), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    z = Dropout(config.DROPOUT_PROB[0])(embedded_sequences)\n",
    "    conv_blocks = []\n",
    "    for sz in config.FILTER_SIZES:\n",
    "        conv = Convolution1D(filters=config.NUM_FILTERS,\n",
    "                            kernel_size=sz,\n",
    "                            padding=\"valid\",\n",
    "                            activation=\"relu\",\n",
    "                            strides=1)(z)\n",
    "        conv = MaxPooling1D(pool_size=2)(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    z = Concatenate()(conv_blocks) #if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "    z = Dropout(config.DROPOUT_PROB[1])(z)\n",
    "    z = Dense(config.HIDDEN_DIMS, activation=\"relu\")(z)\n",
    "    model_output = Dense(config.N_CLASSES, activation='softmax')(z)\n",
    "\n",
    "    model = Model(sequence_input, model_output)\n",
    "\n",
    "    model.compile(loss=config.LOSS, optimizer=config.OPTIMIZER,\n",
    "                  metrics=[config.METRIC])\n",
    "\n",
    "    history = model.fit(train_x, train_y_enc, epochs=config.EPOCHS,\n",
    "                        batch_size=config.BATCH_SIZE,\n",
    "                        sample_weight=sample_weight, verbose=2)\n",
    "    \n",
    "    return model, history, y_encoder\n",
    "\n",
    "\n",
    "\n",
    "###############\n",
    "# Predictions #\n",
    "###############\n",
    "\n",
    "\n",
    "def get_bio_predictions(model, x, x_raw, n_classes):\n",
    "    y_hat = model.predict(x)\n",
    "    y_hat = y_hat.reshape(-1, n_classes).argmax(axis=1).reshape(x.shape[:2])\n",
    "    labels = []\n",
    "    for row in x_raw.itertuples():\n",
    "        sent_idx = row.Index - 1\n",
    "        for tok_idx in range(row.n_toks):\n",
    "            if y_hat[sent_idx][tok_idx] == 0:\n",
    "                label = \"O\"\n",
    "            elif y_hat[sent_idx][tok_idx] == 1:\n",
    "                label = \"I\"\n",
    "            else:\n",
    "                label = \"B\"\n",
    "            labels.append(label)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def si_predictions_to_spans(label_df):\n",
    "    spans = []\n",
    "    prev_label = 'O'\n",
    "    prev_span_start = '-1'\n",
    "    prev_span_end = '-1'\n",
    "    prev_article = ''\n",
    "\n",
    "    for row in label_df.itertuples():\n",
    "        article = row.document_id\n",
    "        span_start = row.token_start\n",
    "        span_end = row.token_end\n",
    "        label = row.label_pred\n",
    "\n",
    "        span, prev_span_start = update_predicted_span(article, label,\n",
    "                                                      span_start, span_end,\n",
    "                                                      prev_article, prev_label,\n",
    "                                                      prev_span_start,\n",
    "                                                      prev_span_end)\n",
    "        if span is not None:\n",
    "            spans.append(span)\n",
    "\n",
    "        prev_article = article\n",
    "        prev_label = label\n",
    "        prev_span_end = span_end\n",
    "\n",
    "    # Make sure we get the last prediction\n",
    "    span, _ = update_predicted_span(article, label, span_start, span_end,\n",
    "                                    prev_article, prev_label, prev_span_start,\n",
    "                                    prev_span_end)\n",
    "    if span is not None:\n",
    "        spans.append(span)\n",
    "    return spans\n",
    "\n",
    "\n",
    "# Helper method for si_predictions_to_spans\n",
    "def update_predicted_span(article, label, span_start, span_end, prev_article,\n",
    "                          prev_label, prev_span_start, prev_span_end):\n",
    "    span = None\n",
    "    cur_span_start = prev_span_start\n",
    "    # Ending a span: I-O, B-O, I-B, B-B, new article\n",
    "    if prev_label != 'O' and (label != 'I' or prev_article != article):\n",
    "        span = (prev_article, prev_span_start, prev_span_end)\n",
    "\n",
    "    # Starting a new span: O-B, O-I, I-B, B-B, new article\n",
    "    if label == 'B' or (label == 'I' and prev_label == 'O') \\\n",
    "            or prev_article != article:\n",
    "        # Update the start of the current label span\n",
    "        cur_span_start = span_start\n",
    "    return span, cur_span_start\n",
    "\n",
    "\n",
    "def print_spans(spans, file_prefix, file_stem, file_suffix):\n",
    "    outfile = file_prefix + 'spans_' + file_stem + '_' + file_suffix + '.txt'\n",
    "    with open(outfile, mode='w') as f:\n",
    "        for span in spans:\n",
    "            f.write(str(span[0]) + '\\t' + str(span[1]) + '\\t' +\n",
    "                    str(span[2]) + '\\n')\n",
    "\n",
    "\n",
    "def predict_si(config, model, history, dev_df, dev_raw, dev_x, comments,\n",
    "               file_prefix, file_stem, file_suffix, features,\n",
    "               predict_spans=True):\n",
    "    y_hat = get_bio_predictions(model, dev_x, dev_raw, config.N_CLASSES)\n",
    "    result_df = pd.concat([dev_df, pd.DataFrame(y_hat, columns=['label_pred'])],\n",
    "                          axis=1, sort=False)\n",
    "\n",
    "    logfile = file_prefix + 'log_' + file_stem + '_' + file_suffix + '.txt'\n",
    "\n",
    "    with open(logfile, mode='w') as f:\n",
    "        f.write('DATA PREPROCESSING\\n\\n')\n",
    "        for comment in comments:\n",
    "            comment = comment.replace('#', '')\n",
    "            fields = comment.split(',')\n",
    "            for field in fields:\n",
    "                f.write(comment.strip() + '\\n')\n",
    "        f.write('Additional features:' + str(features) + '\\n')\n",
    "        f.write('\\n\\nCONFIG\\n\\n')\n",
    "        f.write(config.pretty_str())\n",
    "        f.write('\\n\\nMODEL HISTORY\\n\\n')\n",
    "        f.write('Loss ' + config.LOSS + '\\n')\n",
    "        f.write(str(history.history['loss']) + '\\n')\n",
    "        f.write(config.METRIC + '\\n')\n",
    "        f.write(str(history.history[config.METRIC]) + '\\n')\n",
    "        f.write('\\n\\nMODEL SUMMARY\\n\\n')\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    if predict_spans:\n",
    "        spans = si_predictions_to_spans(result_df)\n",
    "        print_spans(spans, file_prefix, file_stem, file_suffix)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def predict_tc(config, model, history, dev_df, dev_x, comments, file_prefix,\n",
    "               file_stem, file_suffix, features, y_encoder=None,\n",
    "               print_log=True):\n",
    "    if print_log:\n",
    "        logfile = file_prefix + 'log_' + file_stem + '_' + file_suffix + '.txt'\n",
    "        with open(logfile, mode='w') as f:\n",
    "            f.write('DATA PREPROCESSING\\n\\n')\n",
    "            for comment in comments:\n",
    "                comment = comment.replace('#', '')\n",
    "                fields = comment.split(',')\n",
    "                for field in fields:\n",
    "                    f.write(comment.strip() + '\\n')\n",
    "            f.write('Additional features:' + str(features) + '\\n')\n",
    "            f.write('\\n\\nCONFIG\\n\\n')\n",
    "            f.write(config.pretty_str())\n",
    "            if history:\n",
    "                f.write('\\n\\nMODEL HISTORY\\n\\n')\n",
    "                f.write('Loss ' + config.LOSS + '\\n')\n",
    "                f.write(str(history.history['loss']) + '\\n')\n",
    "                f.write(config.METRIC + '\\n')\n",
    "                f.write(str(history.history[config.METRIC]) + '\\n')\n",
    "                f.write('\\n\\nMODEL SUMMARY\\n\\n')\n",
    "                model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "    y_hat = model.predict(dev_x)\n",
    "    if y_encoder:\n",
    "        if config.PRED_ALTS:\n",
    "            # Get the runner-up predictions\n",
    "            y_hat_alt = []\n",
    "            y_score = []\n",
    "            y_score_alt = []\n",
    "            for tok_idx in range(y_hat.shape[0]):\n",
    "                preds = np.argsort(y_hat[tok_idx])\n",
    "                top_idx = preds[-1]\n",
    "                alt_idx = preds[-2]\n",
    "                alt = np.eye(N=1, M=14, k=alt_idx)\n",
    "                y_hat_alt.append(y_encoder.inverse_transform(alt)[0][0])\n",
    "                y_score.append(y_hat[tok_idx][top_idx])\n",
    "                y_score_alt.append(y_hat[tok_idx][alt_idx])\n",
    "\n",
    "        # Decode the predictions\n",
    "        y_hat = y_encoder.inverse_transform(y_hat)\n",
    "\n",
    "        if config.PRED_ALTS:\n",
    "            out_df = pd.concat([dev_df,\n",
    "                                pd.DataFrame(data={'label_alt': y_hat_alt,\n",
    "                                                   'pred_score': y_score,\n",
    "                                                   'alt_score': y_score_alt})],\n",
    "                          axis=1, sort=False)\n",
    "            print_tc(y_hat, out_df, file_prefix, 'alt_' + file_stem,\n",
    "                     file_suffix, cols=['document_id', 'label_pred',\n",
    "                                        'span_start', 'span_end', 'pred_score',\n",
    "                                        'label_alt', 'alt_score'],\n",
    "                     print_header=True)\n",
    "    return print_tc(y_hat, dev_df, file_prefix, file_stem, file_suffix)\n",
    "\n",
    "\n",
    "def print_tc(y_hat, dev_df, file_prefix, file_stem, file_suffix,\n",
    "             cols=['document_id', 'label_pred', 'span_start', 'span_end'],\n",
    "             print_header=False):\n",
    "    outfile = file_prefix + 'labels_' + file_stem + '_' + file_suffix + '.txt'\n",
    "    result_df = pd.concat([dev_df, pd.DataFrame(y_hat, columns=['label_pred'])],\n",
    "                          axis=1, sort=False)\n",
    "    result_df = result_df[cols]\n",
    "    result_df.to_csv(outfile, sep='\\t', index=False, header=print_header)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "# Putting it all together #\n",
    "###########################\n",
    "\n",
    "\n",
    "def run(config, file_stem, file_suffix, verbose=True, predict_spans=True,\n",
    "        data=None, word2embedding=None, file_prefix=''):\n",
    "    if verbose:\n",
    "        print('Running with config:')\n",
    "        print(config.pretty_str())\n",
    "    if not data:\n",
    "        if config.LOAD_DATA:\n",
    "            print('Loading data from files')\n",
    "            data = Data(path=config.DATA_PATH)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('Encoding the data')\n",
    "            data = get_data(config, word2embedding)\n",
    "            if config.SAVE_DATA:\n",
    "                data.save()\n",
    "\n",
    "    if verbose:\n",
    "        print('Additional features:', data.features)\n",
    "        print('Building the model')\n",
    "    if config.TOKEN_LVL:\n",
    "        model, history = get_bilstm(config, data.train_x, data.train_y,\n",
    "                                    data.sample_weight)\n",
    "    else:\n",
    "        history = None\n",
    "        y_encoder = None\n",
    "        if config.MODEL == 'SVM':\n",
    "            model = get_svm(data.train_x, data.train_y)\n",
    "        elif config.MODEL.startswith('FFNN'):\n",
    "            model, history, y_encoder = get_ffnn(config, data.train_x,\n",
    "                                                 data.train_y,\n",
    "                                                 data.sample_weight,\n",
    "                                                 single_layer=config.MODEL == 'FFNN-single')\n",
    "        elif config.MODEL == 'LSTM':\n",
    "            model, history = get_bilstm(config, data.train_x, data.train_y,\n",
    "                                    data.sample_weight)\n",
    "        elif config.MODEL == 'CNN':\n",
    "            model, history, y_encoder = get_cnn(config, data.train_x, data.train_y,\n",
    "                                    data.sample_weight)\n",
    "        elif config.MODEL == 'KIMCNN':\n",
    "           model, history, y_encoder = get_kimcnn(config, data.train_x, data.train_y,\n",
    "                                    data.sample_weight)\n",
    "\n",
    "    if verbose:\n",
    "        print('Predicting the test data labels/spans')\n",
    "    labels_test = None\n",
    "    if config.TOKEN_LVL:\n",
    "        labels_dev = predict_si(config, model, history, data.dev_df,\n",
    "                                data.dev_raw, data.dev_x, data.comments,\n",
    "                                file_prefix, file_stem, file_suffix,\n",
    "                                data.features, predict_spans)\n",
    "    else:\n",
    "        labels_dev = predict_tc(config, model, history, data.dev_df, data.dev_x,\n",
    "                                data.comments, file_prefix, 'dev_' + file_stem,\n",
    "                                file_suffix, data.features, y_encoder)\n",
    "        if config.TEST_URL:\n",
    "            labels_test = predict_tc(config, model, history, data.test_df,\n",
    "                                    data.test_x, data.comments, file_prefix,\n",
    "                                    'test_' + file_stem, file_suffix,\n",
    "                                     data.features, y_encoder, print_log=False)\n",
    "    if verbose:\n",
    "        print('Done!\\n\\n')\n",
    "\n",
    "    return data, labels_dev, labels_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3EnkETqKZbqT"
   },
   "source": [
    "# grid_search.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jAjKAZNfSaWO"
   },
   "outputs": [],
   "source": [
    "# from model import run, si_predictions_to_spans, print_spans\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, args=None):\n",
    "        \"\"\"Creates a default configuration.\n",
    "\n",
    "        Keyword arguments:\n",
    "        args -- a dict(str -> ?) containing values diverging from the default\n",
    "        \"\"\"\n",
    "        # Encoding the data:\n",
    "        self.TOKEN_LVL = True  # True if task 1, False if task 2.\n",
    "        if args and 'TOKEN_LVL' in args:\n",
    "            self.TOKEN_LVL = args['TOKEN_LVL']\n",
    "\n",
    "        self.ONLINE_SOURCES = True  # Input is given via URLs, not local files.\n",
    "\n",
    "        self.UNCASED = True  # If true, words are turned into lower case.\n",
    "        self.FEATURES = None  # If None, the features are determined from the\n",
    "                              # input file.\n",
    "        self.EXCLUDE_FEATURES = []\n",
    "        self.SAVE_DATA = False  # If true, the following two values can be used\n",
    "                                # for re-using the data next time.\n",
    "        # In case the training & dev data were saved and can be reused:\n",
    "        self.DATA_PATH = 'gdrive/My Drive/colab_projects/data/data/'\n",
    "        self.LOAD_DATA = False\n",
    "\n",
    "        # Building the model:\n",
    "        self.MODEL = 'LSTM'\n",
    "        self.BATCH_SIZE = 128\n",
    "        self.LSTM_UNITS = 512\n",
    "        self.DROPOUT = 0.25\n",
    "        self.OPTIMIZER = 'adam'\n",
    "        self.METRIC = 'categorical_accuracy'\n",
    "        self.LOSS = 'categorical_crossentropy'\n",
    "\n",
    "        # Making predictions:\n",
    "        self.MAJORITY_VOTING = True\n",
    "\n",
    "        # Task-specific options\n",
    "        if self.TOKEN_LVL:\n",
    "            # Task 1: Span identification\n",
    "            # For using train+dev and test, see the end of this file.\n",
    "            self.N_CLASSES = 2\n",
    "            self.MAX_SEQ_LEN = 35\n",
    "            self.EMBED_DIM = 300\n",
    "            self.EPOCHS = 10\n",
    "            self.CLASS_WEIGHTS = {'O': 1.0, 'I': 6.5, 'B': 6.5}\n",
    "            self.TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-train.tsv?token=AD7GEDMEHQSUS34AOSIHGF26Q4WYK'\n",
    "            self.DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-dev.tsv?token=AD7GEDI3J6KMIKA6XXTKT6S6Q4WYI'\n",
    "            self.TEST_URL = ''\n",
    "            #self.TEST_URL = ''\n",
    "            self.EMBEDDING_PATH = 'gdrive/My Drive/colab_projects/data/glove.42B.300d.zip'\n",
    "            self.USE_BERT = False\n",
    "            self.TRAIN_BERT = 'gdrive/My Drive/colab_projects/data/train_bert-base-uncased.tsv'\n",
    "            self.DEV_BERT = 'gdrive/My Drive/colab_projects/data/dev_bert-base-uncased.tsv'\n",
    "            self.TEST_BERT = ''\n",
    "        else:\n",
    "            # Task 2: Technique classification\n",
    "            # Options: 'SVM', 'FFNN', 'FFNN-single', 'LSTM', 'CNN', 'KIMCNN'\n",
    "            self.MODEL = 'FFNN'\n",
    "            self.HIDDEN = 128\n",
    "            self.EPOCHS = 15\n",
    "            self.CLASS_WEIGHTS = None\n",
    "            self.TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/tc-train.tsv?token=AD7GEDOONNZLYERAUKC4E5K6OFDNY'\n",
    "            self.DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/tc-dev.tsv?token=AD7GEDLMCCTZVHH5IHN7H4K6OFDN4'\n",
    "            self.TEST_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/tc-test.tsv?token=AD7GEDMR4D6KMDMTUVI534C6OFDNY'\n",
    "            self.USE_BERT = True  # Currently, we don't have an alternative to this.\n",
    "            self.EMBED_DIM = 768\n",
    "            self.N_BERT_LAYERS = 1\n",
    "            self.TRAIN_BERT = 'gdrive/My Drive/colab_projects/data/tc_train_bert-base-uncased.tsv'\n",
    "            self.DEV_BERT = 'gdrive/My Drive/colab_projects/data/tc_dev_bert-base-uncased.tsv'\n",
    "            self.TEST_BERT = 'gdrive/My Drive/colab_projects/data/tc_test_bert-base-uncased.tsv'\n",
    "            self.N_CLASSES = 14\n",
    "            self.MAX_SEQ_LEN = -1  # Value is irrelevant (fixed-size input)\n",
    "            self.VOCAB_SIZE = 30000\n",
    "            self.FILTER_SIZES = (3, 5, 7)\n",
    "            self.NUM_FILTERS = 10\n",
    "            self.DROPOUT_PROB = (0.5, 0.5)\n",
    "            self.HIDDEN_DIMS = 50\n",
    "            # Runners-up to the softmax winner:\n",
    "            self.PRED_ALTS = False\n",
    "\n",
    "        self.FLATTEN = (not self.TOKEN_LVL) or (self.MODEL != 'LSTM')\n",
    "\n",
    "        if args:\n",
    "            for key in args:\n",
    "                setattr(self, key, args[key])                \n",
    "\n",
    "    def pretty_str(self):\n",
    "        s = 'max seq len: ' + str(self.MAX_SEQ_LEN) + '\\n' + \\\n",
    "            'embedding depth: ' + str(self.EMBED_DIM) + '\\n' + \\\n",
    "            'BERT embeddings: ' + str(self.USE_BERT) + '\\n' + \\\n",
    "            'TRAIN_BERT: ' + str(self.TRAIN_BERT) + '\\n' + \\\n",
    "            'DEV_BERT: ' + str(self.DEV_BERT) + '\\n' + \\\n",
    "            'number of labels: ' + str(config.N_CLASSES) + '\\n' + \\\n",
    "            'batch size: ' + str(self.BATCH_SIZE) + '\\n' + \\\n",
    "            'epochs: ' + str(self.EPOCHS) + '\\n' + \\\n",
    "            'class weights: ' + str(self.CLASS_WEIGHTS) + '\\n' + \\\n",
    "            'LSTM units: ' + str(self.LSTM_UNITS) + '\\n' + \\\n",
    "            'dropout rate: ' + str(self.DROPOUT) + '\\n' + \\\n",
    "            'optimizer: ' + self.OPTIMIZER + '\\n' + \\\n",
    "            'metric: ' + self.METRIC + '\\n' + \\\n",
    "            'loss: ' + self.LOSS + '\\n'\n",
    "        if not self.TOKEN_LVL:\n",
    "            s += 'model: ' + str(self.MODEL) + '\\n' + \\\n",
    "                 'BERT layers: ' + str(self.N_BERT_LAYERS) + '\\n' + \\\n",
    "                 'predict alternatives: ' + str(self.PRED_ALTS) + '\\n'\n",
    "        return s\n",
    "\n",
    "\n",
    "def get_majority_vote(votes, print_near_ties=False):\n",
    "    if print_near_ties:\n",
    "        votes = [(k, v) for k, v in sorted(dict(Counter(votes)).items(),\n",
    "                                           key=lambda item: item[1],\n",
    "                                           reverse=True)]\n",
    "        if len(votes) > 1 and votes[0][1] - votes[1][1] < 2:\n",
    "            print(votes)\n",
    "        return votes[0][0]\n",
    "\n",
    "    votes = [k for k, _ in sorted(dict(Counter(votes)).items(),\n",
    "                                  key=lambda item: item[1],\n",
    "                                  reverse=True)]\n",
    "    # Task 1: For our data, preferring specific labels in tie situations\n",
    "    # doesn't make a difference.\n",
    "    # Task 2: Ties are extremely rare.\n",
    "    return votes[0]\n",
    "\n",
    "\n",
    "def print_majority_votes(config, predictions, label_cols, file_prefix,\n",
    "                         file_stem, df, print_near_ties=False):\n",
    "    labels = []\n",
    "    for row in predictions.itertuples():\n",
    "        labels.append(get_majority_vote(\n",
    "            [getattr(row, l) for l in label_cols], print_near_ties))\n",
    "    predictions['label_pred'] = labels\n",
    "    if config.TOKEN_LVL:\n",
    "        spans = si_predictions_to_spans(predictions)\n",
    "        print_spans(spans, file_prefix, file_stem, 'majority')\n",
    "    else:\n",
    "        print_tc(labels, df, file_prefix, file_stem, 'majority')\n",
    "\n",
    "\n",
    "def run_config(config, file_prefix, data=None, repetitions=5, verbose=True):\n",
    "    now = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
    "    predictions_dev = None\n",
    "    predictions_test = None\n",
    "    label_cols = []\n",
    "    for i in range(repetitions):\n",
    "        if verbose:\n",
    "            print(\"Iteration \" + str(i + 1) + \" of \" + str(repetitions))\n",
    "        data, labels_dev, labels_test = run(config, data=data, verbose=verbose,\n",
    "                                            file_prefix=file_prefix,\n",
    "                                            file_stem=now, file_suffix=str(i))\n",
    "        if config.MAJORITY_VOTING:\n",
    "            if predictions_dev is None:\n",
    "                predictions_dev = labels_dev\n",
    "                predictions_dev = predictions_dev.rename(\n",
    "                    columns={'label_pred': 'label_0'})\n",
    "                if labels_test is not None:\n",
    "                    predictions_test = labels_test\n",
    "                    predictions_test = predictions_test.rename(\n",
    "                        columns={'label_pred': 'label_0'})\n",
    "            else:\n",
    "                predictions_dev.insert(loc=len(predictions_dev.columns),\n",
    "                                       column='label_' + str(i),\n",
    "                                       value=labels_dev.label_pred)\n",
    "                if labels_test is not None:\n",
    "                    predictions_test.insert(loc=len(predictions_test.columns),\n",
    "                                            column='label_' + str(i),\n",
    "                                            value=labels_test.label_pred)\n",
    "            label_cols.append('label_' + str(i))\n",
    "    if config.MAJORITY_VOTING:\n",
    "        if verbose:\n",
    "            print('Majority voting (dev)')\n",
    "        print_majority_votes(config, predictions_dev, label_cols,\n",
    "                             file_prefix, 'dev_' + now, data.dev_df, False)\n",
    "        if labels_test is not None:\n",
    "            print('Majority voting (test)')\n",
    "            print_majority_votes(config, predictions_test, label_cols,\n",
    "                                 file_prefix, 'test_' + now, data.test_df,\n",
    "                                 False)\n",
    "\n",
    "    # Return data in case the next config only changes model features\n",
    "    return data, now\n",
    "\n",
    "\n",
    "file_prefix = '/content/gdrive/My Drive/colab_projects/semeval-predictions/'\n",
    "data = None\n",
    "repetitions = 5\n",
    "\n",
    "# Short-hands for the features:\n",
    "f_none = []\n",
    "f_all = None  # Determined from the input file header\n",
    "f_rep = ['repetitions']\n",
    "f_len = ['length']\n",
    "f_senti = ['highest_pos', 'highest_neg']\n",
    "f_arglex = ['authority', 'causation', 'contrast', 'emphasis', 'generalization',\n",
    "            'inconsistency', 'necessity', 'possibility', 'priority',\n",
    "            'structure', 'wants']\n",
    "f_question = ['question']\n",
    "f_emotion = ['fear', 'sadness', 'joy', 'anger', 'disgust']\n",
    "f_ne_2 = ['NORP', 'GPE']\n",
    "f_ne_6 = ['ORG', 'NORP', 'GPE', 'PERSON', 'CARDINAL', 'DATE']\n",
    "f_america = ['america']\n",
    "f_america_simple = ['america_simple']\n",
    "f_reductio = ['reductio']\n",
    "f_pos = ['ADJ', 'ADP', 'ADV', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART',\n",
    "         'PRON', 'PROPN', 'PUNCT', 'SYM', 'VERB', 'X']\n",
    "\n",
    "### You can change config values by passing a dictionary to the constructor.\n",
    "\n",
    "### Hyperparameter tuning (example):\n",
    "# for epochs in [5, 15, 20, 25]:\n",
    "#     config = Config({'USE_BERT': True, 'TOKEN_LVL': False, 'EPOCHS': epochs})\n",
    "#     data, _ = run_config(config, file_prefix, data)\n",
    "\n",
    "##################################\n",
    "# Subtask 1: Span identification #\n",
    "##################################\n",
    "\n",
    "## GloVe-100, no features\n",
    "# config = Config({'EMBEDDING_PATH': 'gdrive/My Drive/colab_projects/data/glove.6B.100d.txt',\n",
    "#                  'EMBED_DIM': 100,\n",
    "#                  'FEATURES': []})\n",
    "## GloVe-300, no features\n",
    "# config = Config({'FEATURES': []})\n",
    "## BERT (base-cased), no features:\n",
    "# config = Config({'USE_BERT': True, 'EMBED_DIM': 768, 'UNCASED': False,\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/train_bert-base-cased.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/dev_bert-base-cased.tsv',\n",
    "#                  'FEATURES': []})\n",
    "## BERT (base-uncased), no features:\n",
    "# config = Config({'USE_BERT': True, 'EMBED_DIM': 768, 'FEATURES': []})\n",
    "\n",
    "## GloVe-100, SentiWordNet:\n",
    "# config = Config({'EMBEDDING_PATH': 'gdrive/My Drive/colab_projects/data/glove.6B.100d.txt',\n",
    "#                  'EMBED_DIM': 100,\n",
    "#                  'FEATURES': ['positive', 'negative']})\n",
    "## GloVe-100, Arguing Lexicon:\n",
    "# config = Config({'EMBEDDING_PATH': 'gdrive/My Drive/colab_projects/data/glove.6B.100d.txt',\n",
    "#                  'EMBED_DIM': 100,\n",
    "#                  'FEATURES': ['arglex_any']})\n",
    "## GloVe-100, POS tags:\n",
    "# config = Config({'EMBEDDING_PATH': 'gdrive/My Drive/colab_projects/data/glove.6B.100d.txt',\n",
    "#                  'EMBED_DIM': 100,\n",
    "#                  'FEATURES': f_pos})\n",
    "## GloVe-100, SentiWordNet + Arguing Lexicon:\n",
    "# config = Config({'EMBEDDING_PATH': 'gdrive/My Drive/colab_projects/data/glove.6B.100d.txt',\n",
    "#                  'EMBED_DIM': 100,\n",
    "#                  'FEATURES': ['positive', 'negative', 'arglex_any']})\n",
    "## GloVe-100, SentiWordNet + Arguing Lexicon + POS:\n",
    "# config = Config({'EMBEDDING_PATH': 'gdrive/My Drive/colab_projects/data/glove.6B.100d.txt',\n",
    "#                  'EMBED_DIM': 100,\n",
    "#                  'FEATURES': ['positive', 'negative', 'arglex_any'] + f_pos})\n",
    "\n",
    "## BERT (base-uncased), SentiWordNet:\n",
    "# config = Config({'USE_BERT': True, 'EMBED_DIM': 768, \n",
    "#                  'FEATURES': ['positive', 'negative']})\n",
    "## BERT (base-uncased), Arguing Lexicon:\n",
    "# config = Config({'USE_BERT': True, 'EMBED_DIM': 768, \n",
    "#                  'FEATURES': ['arglex_any']})\n",
    "## BERT (base-uncased), POS tags:\n",
    "# config = Config({'USE_BERT': True, 'EMBED_DIM': 768, \n",
    "#                  'FEATURES': f_pos})\n",
    "## BERT (base-uncased), SentiWordNet + Arguing Lexicon:\n",
    "# config = Config({'USE_BERT': True, 'EMBED_DIM': 768, \n",
    "#                  'FEATURES': ['positive', 'negative', 'arglex_any']})\n",
    "## BERT (base-uncased), SentiWordNet + Arguing Lexicon + POS:\n",
    "## (FINAL MODEL before post-processing)\n",
    "# config = Config({'USE_BERT': True, 'EMBED_DIM': 768, \n",
    "#                  'FEATURES': ['positive', 'negative', 'arglex_any'] + f_pos})\n",
    "### For predictions on the final test set (task 1):\n",
    "# config = Config({'USE_BERT': True, 'EMBED_DIM': 768, \n",
    "#                  'FEATURES': ['positive', 'negative', 'arglex_any'] + f_pos,\n",
    "#                  'TRAIN_URL': 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-train%2Bdev.tsv?token=AD7GEDJ7GSTS3RSP5ZSXLZ26LP4BS',\n",
    "#                  'DEV_URL': 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-test.tsv?token=AD7GEDM7A3GFIAEZHHESFO26LP4BQ',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/train+dev_bert-base-uncased.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/test_bert-base-uncased.tsv'\n",
    "#                  })\n",
    "\n",
    "#######################################\n",
    "# Subtask 2: Technique classification #\n",
    "#######################################\n",
    "\n",
    "## bert-base-uncased embeddings of [CLS] & the first 10 tokens + CNN\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': [], 'MODEL': 'CNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_bert-base-uncased_10.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_bert-base-uncased_10.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_bert-base-uncased_10.tsv',\n",
    "#                  'EMBED_DIM': 768 * 11})\n",
    "## bert-base-uncased embeddings of [CLS] & the first 10 tokens + KimCNN\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': [], 'MODEL': 'KIMCNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_bert-base-uncased_10.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_bert-base-uncased_10.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_bert-base-uncased_10.tsv',\n",
    "#                  'EMBED_DIM': 768 * 11})\n",
    "## bert-base-uncased embeddings of [CLS] & the first 10 tokens + MLP\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': [], 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_bert-base-uncased_10.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_bert-base-uncased_10.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_bert-base-uncased_10.tsv',\n",
    "#                  'EMBED_DIM': 768 * 11})\n",
    "\n",
    "## bert-base-uncased embeddings of [CLS] + rep & bert-base-uncased & pre-softmax + MLP\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': [], 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': '/content/gdrive/My Drive/colab_projects/data/full_bert_train.tsv',\n",
    "#                  'DEV_BERT': '/content/gdrive/My Drive/colab_projects/data/full_bert_dev.tsv',\n",
    "#                  'EMBED_DIM': 768 + 14})\n",
    "## rep + bert-base-cased + pre-softmax + MLP\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': [], 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_bert-base-cased_pre-softmax.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_bert-base-cased_pre-softmax.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_bert-base-cased_pre-softmax.tsv',\n",
    "#                  'EMBED_DIM': 14, 'UNCASED': False})\n",
    "## bert-base-uncased + pre-softmax + MLP\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': [], 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_bert-base-uncased_no-rep.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_bert-base-uncased_no-rep.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_bert-base-uncased_no-rep.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "\n",
    "## rep + bert-base-uncased + linear classifier\n",
    "## -> see bert_sequence_classification.ipynb\n",
    "## rep + bert-base-uncased + pre-softmax + XGBoost\n",
    "## TODO\n",
    "## bert-base-uncased + pre-softmax + single-layer perceptron\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': [], 'MODEL': 'FFNN-single',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "## bert-base-uncased + pre-softmax + SVN\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': [], 'MODEL': 'SVN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "## bert-base-uncased + pre-softmax + multilayer perceptron\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': [], 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "\n",
    "## bert-base-uncased + pre-softmax + america + multilayer perceptron\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': f_america, 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "## bert-base-uncased + pre-softmax + bag-of-words + multilayer perceptron\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': f_reductio, 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "## bert-base-uncased + pre-softmax + emotion + multilayer perceptron\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': f_emotion, 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "## bert-base-uncased + pre-softmax + sequence length + multilayer perceptron\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': f_len, 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "## bert-base-uncased + pre-softmax + repetition count + multilayer perceptron\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': f_rep, 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "## bert-base-uncased + pre-softmax + two named entity classes (NE-2) + multilayer perceptron\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': f_ne_2, 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "## bert-base-uncased + pre-softmax + six named entity classes (NE-6) + multilayer perceptron\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': f_ne_6, 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "## bert-base-uncased + pre-softmax + arguing lexicon (AL) + multilayer perceptron\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': f_arglex, 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "## bert-base-uncased + pre-softmax + question mark feature (Q) + multilayer perceptron\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': f_question, 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "## bert-base-uncased + pre-softmax + NE-6 + AL + Q + multilayer perceptron\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
    "#                  'FEATURES': f_ne_6 + f_arglex + f_question, 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "## bert-base-uncased + pre-softmax + NE-2 + AL + Q + multilayer perceptron\n",
    "## (base model: FINAL MODEL before post-processing)\n",
    "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False, 'PRED_ALTS': True,\n",
    "#                  'FEATURES': f_ne_2 + f_arglex + f_question, 'MODEL': 'FFNN',\n",
    "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_20200308-221011_2.tsv',\n",
    "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_20200308-221011_2.tsv',\n",
    "#                  'TEST_BERT': 'gdrive/My Drive/colab_projects/data/tc_test_20200308-221011_2.tsv',\n",
    "#                  'EMBED_DIM': 14})\n",
    "\n",
    "## Unused configurations (preliminary experiments)\n",
    "## Large BERT embeddings ([CLS])\n",
    "# 'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_bert-large-uncased.tsv',\n",
    "# 'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_bert-large-uncased.tsv',\n",
    "# 'EMBED_DIM': 1024,\n",
    "## SentiWordNet feature, America-simple feature\n",
    "# 'FEATURES': f_senti\n",
    "# 'FEATURES': f_america_simple\n",
    "## Other feature combinations\n",
    "# 'FEATURES': f_rep + f_len + f_question + f_senti + f_arglex\n",
    "# 'FEATURES': f_rep + f_len + f_question + f_senti + f_arglex + f_emotion\n",
    "# 'FEATURES': f_rep + f_len + f_question + f_senti + f_arglex + f_emotion + f_ne_2 + f_america_simple\n",
    "# 'FEATURES': f_question + f_ne_2\n",
    "# 'FEATURES': f_senti + f_arglex\n",
    "# 'FEATURES': f_ne_2 + f_emotion + f_question\n",
    "# 'FEATURES': f_ne_2 + f_arglex + f_question + f_emotion\n",
    "## Class-weighting\n",
    "#  'CLASS_WEIGHTS': {'Loaded_Language': 1,\n",
    "#                    'Name_Calling,Labeling': 1,\n",
    "#                    'Repetition': 2,\n",
    "#                    'Doubt': 2,\n",
    "#                    'Exaggeration,Minimisation': 2,\n",
    "#                    'Appeal_to_fear-prejudice': 2,\n",
    "#                    'Flag-Waving': 1,\n",
    "#                    'Causal_Oversimplification': 1,\n",
    "#                    'Appeal_to_Authority': 1,\n",
    "#                    'Slogans': 1,\n",
    "#                    'Black-and-White_Fallacy': 1,\n",
    "#                    'Whataboutism,Straw_Men,Red_Herring': 1,\n",
    "#                    'Thought-terminating_Cliches': 1,\n",
    "#                    'Bandwagon,Reductio_ad_hitlerum': 1},\n",
    "\n",
    "\n",
    "data, now = run_config(config, file_prefix, data, repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mtkx52hD-mVp"
   },
   "outputs": [],
   "source": [
    "if not config.TOKEN_LVL:\n",
    "    phases = ['dev']\n",
    "    if config.TEST_URL:\n",
    "        phases.append('test')\n",
    "\n",
    "    runs = list(range(repetitions))\n",
    "    if config.MAJORITY_VOTING:\n",
    "        runs += ['majority']\n",
    "    for sfx in runs:\n",
    "        for phase in phases:\n",
    "            f = file_prefix + 'labels_' + phase + '_' + now + '_' + str(sfx) + '.txt'\n",
    "            df = pd.read_csv(f, sep='\\t', usecols=[1], names=['label'])\n",
    "            df = df['label'].value_counts().rename_axis('labels').reset_index(name='counts')\n",
    "            df['%'] = df['counts'] / df['counts'].sum()\n",
    "            print(f)\n",
    "            print(df)\n",
    "            print('\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "model+grid_search.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
