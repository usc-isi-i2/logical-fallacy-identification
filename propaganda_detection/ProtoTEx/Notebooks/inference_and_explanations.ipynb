{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae708288-f6dd-47de-8942-6bd52da391b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from importlib import reload  \n",
    "import numpy as np\n",
    "import torch,time\n",
    "from transformers import BartModel,BartConfig,BartForConditionalGeneration,BartForCausalLM, BartTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55c3d5d-6c51-4b5f-92fe-3f7bcaa5b644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/work/05773/anubrata/ls6/ProtoTEx/Notebooks', '/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python38.zip', '/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python3.8', '/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python3.8/lib-dynload', '', '/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python3.8/site-packages', '/work/05773/anubrata/ls6/ProtoTEx/']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "MOD_FOLDER = '/work/05773/anubrata/ls6/ProtoTEx/'\n",
    "# setting path to enable import from the parent directory\n",
    "sys.path.append(MOD_FOLDER)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b84c9fd-74ba-49cf-bb4d-0f8dae196430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import SimpleProtoTex\n",
    "from models import ProtoTEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f15a0bdb-96b8-4010-aedf-8376361616d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_prototypes = 20\n",
    "num_pos_prototypes = 19\n",
    "model = ProtoTEx(num_prototypes, \n",
    "                 num_pos_prototypes,\n",
    "                 bias=False, \n",
    "                 dropout=False, \n",
    "                 special_classfn=True, # special_classfn=False, ## apply dropouonly on bias \n",
    "                 p=1, #p=0.75,\n",
    "                 batchnormlp1=True)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9516b03-8074-4119-8ec7-48eb530a6e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Models/NegProtoTEx_protos_xavier_large_bs20_20_woRat_noReco_g2d_nobias_nodrop_cu1_PosUp_normed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae07f1b7-434c-4816-88e6-2df7a1af425f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MOD_FOLDER + model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cafad496-a1e0-4429-9453-0ecd0b00a8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProtoTEx(\n",
       "  (bart_model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       "  (classfn_model): Linear(in_features=20, out_features=2, bias=False)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (dropout): Dropout(p=1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c49b01be-09e8-4238-b1b6-52c8bbb6e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833c5ab0-7442-4b25-9552-47231200fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load all the functions for analyzing prototypes\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4dc8767-f9eb-41ca-abc1-c3b595465989",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load all the data classes\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a0eb398-bd28-4da2-b9ab-86451ef18504",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "train=make_dataset(pathlib.Path(\"../data/protechn_corpus_eval/train/\"))\n",
    "val=make_dataset(pathlib.Path(\"../data/protechn_corpus_eval/dev/\"))\n",
    "test=make_dataset(pathlib.Path(\"../data/protechn_corpus_eval/test/\"))\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n",
    "train_=make_bert_testset(train)\n",
    "val_=make_bert_testset(val)\n",
    "test_=make_bert_testset(test)\n",
    "# train_sents=[ \" \".join(i) for d in train_[0] for i in d]\n",
    "train_sents=[ list(map(lambda x: x[1] if x[0]==0 else \" \"+x[1], enumerate(i))) for d in train_[0] for i in d]\n",
    "# val_sents=[ \" \".join(i) for d in val_[0] for i in d]\n",
    "val_sents=[ list(map(lambda x: x[1] if x[0]==0 else \" \"+x[1], enumerate(i))) for d in val_[0] for i in d]\n",
    "# test_sents=[ \" \".join(i) for d in test_[0] for i in d]\n",
    "test_sents=[ list(map(lambda x: x[1] if x[0]==0 else \" \"+x[1], enumerate(i))) for d in test_[0] for i in d]\n",
    "\n",
    "\n",
    "train_ls=create_labels(train_)\n",
    "val_ls=create_labels(val_)\n",
    "test_ls=create_labels(test_)\n",
    "train_y_txt=[ i for d in train_[1] for i in d]\n",
    "val_y_txt=[ i for d in val_[1] for i in d]\n",
    "test_y_txt=[ i for d in test_[1] for i in d]\n",
    "\n",
    "\n",
    "train_idx_bylabel={x: [i for i in range(len(train_ls)) if train_ls[i]==x] for x in labels_set} \n",
    "val_idx_bylabel={x: [i for i in range(len(val_ls)) if val_ls[i]==x] for x in labels_set} \n",
    "test_idx_bylabel={x: [i for i in range(len(test_ls)) if test_ls[i]==x] for x in labels_set} \n",
    "\n",
    "train_dataset=BinaryClassDataset(train_sents,train_ls,train_y_txt,it_is_train=0,balance=True, tokenizer=tokenizer)\n",
    "val_dataset=BinaryClassDataset(val_sents,val_ls,val_y_txt,it_is_train=0, tokenizer=tokenizer)\n",
    "test_dataset=BinaryClassDataset(test_sents,test_ls,test_y_txt,it_is_train=0, tokenizer=tokenizer)\n",
    "train_dataset_eval=BinaryClassDataset(train_sents,train_ls,train_y_txt,it_is_train=0,balance=False, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "train_dl=torch.utils.data.DataLoader(train_dataset,batch_size=20,shuffle=True,\n",
    "                                 collate_fn=train_dataset.collate_fn)\n",
    "val_dl=torch.utils.data.DataLoader(val_dataset,batch_size=128,shuffle=False,\n",
    "                                 collate_fn=val_dataset.collate_fn)\n",
    "test_dl=torch.utils.data.DataLoader(test_dataset,batch_size=128,shuffle=False,\n",
    "                                 collate_fn=test_dataset.collate_fn)\n",
    "train_dl_eval=torch.utils.data.DataLoader(train_dataset_eval,batch_size=20,shuffle=False,\n",
    "                                 collate_fn=train_dataset_eval.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0272bddf-771a-4585-85f6-01e25139e6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:06<00:00,  5.13batches/s]\n"
     ]
    }
   ],
   "source": [
    "best_protos_per_testeg = get_best_k_protos_for_batch(test_sents, \n",
    "                                                   test_ls, \n",
    "                                                   test_y_txt, \n",
    "                                                   specific_label=None, \n",
    "                                                   model_new=model, \n",
    "                                                   tokenizer=tokenizer, topk= 5, do_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd11158c-69ac-4aab-b2a4-cfcc98b9eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents_joined=[\" \".join(i) for i in train_sents]\n",
    "test_sents_joined=[\" \".join(i) for i in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2e430e9-e10f-42a5-bf76-0619a702496f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/32 [00:22<?, ?batches/s]\u001b[A\n",
      "\n",
      "  3%|▎         | 1/32 [00:00<00:14,  2.18batches/s]\u001b[A\n",
      "  6%|▋         | 2/32 [00:00<00:09,  3.31batches/s]\u001b[A\n",
      "  9%|▉         | 3/32 [00:00<00:07,  3.90batches/s]\u001b[A\n",
      " 12%|█▎        | 4/32 [00:01<00:06,  4.27batches/s]\u001b[A\n",
      " 16%|█▌        | 5/32 [00:01<00:05,  4.53batches/s]\u001b[A\n",
      " 19%|█▉        | 6/32 [00:01<00:05,  4.65batches/s]\u001b[A\n",
      " 22%|██▏       | 7/32 [00:01<00:05,  4.77batches/s]\u001b[A\n",
      " 25%|██▌       | 8/32 [00:01<00:04,  4.82batches/s]\u001b[A\n",
      " 28%|██▊       | 9/32 [00:02<00:04,  4.85batches/s]\u001b[A\n",
      " 31%|███▏      | 10/32 [00:02<00:04,  4.90batches/s]\u001b[A\n",
      " 34%|███▍      | 11/32 [00:02<00:04,  4.94batches/s]\u001b[A\n",
      " 38%|███▊      | 12/32 [00:02<00:04,  4.91batches/s]\u001b[A\n",
      " 41%|████      | 13/32 [00:02<00:03,  4.95batches/s]\u001b[A\n",
      " 44%|████▍     | 14/32 [00:03<00:03,  4.93batches/s]\u001b[A\n",
      " 47%|████▋     | 15/32 [00:03<00:03,  4.95batches/s]\u001b[A\n",
      " 50%|█████     | 16/32 [00:03<00:03,  4.98batches/s]\u001b[A\n",
      " 53%|█████▎    | 17/32 [00:03<00:03,  4.99batches/s]\u001b[A\n",
      " 56%|█████▋    | 18/32 [00:03<00:02,  5.00batches/s]\u001b[A\n",
      " 59%|█████▉    | 19/32 [00:04<00:02,  4.98batches/s]\u001b[A\n",
      " 62%|██████▎   | 20/32 [00:04<00:02,  4.98batches/s]\u001b[A\n",
      " 66%|██████▌   | 21/32 [00:04<00:02,  4.99batches/s]\u001b[A\n",
      " 69%|██████▉   | 22/32 [00:04<00:02,  5.00batches/s]\u001b[A\n",
      " 72%|███████▏  | 23/32 [00:04<00:01,  5.01batches/s]\u001b[A\n",
      " 75%|███████▌  | 24/32 [00:05<00:01,  4.98batches/s]\u001b[A\n",
      " 78%|███████▊  | 25/32 [00:05<00:01,  4.97batches/s]\u001b[A\n",
      " 81%|████████▏ | 26/32 [00:05<00:01,  4.93batches/s]\u001b[A\n",
      " 84%|████████▍ | 27/32 [00:05<00:01,  4.96batches/s]\u001b[A\n",
      " 88%|████████▊ | 28/32 [00:05<00:00,  4.92batches/s]\u001b[A\n",
      " 91%|█████████ | 29/32 [00:06<00:00,  4.95batches/s]\u001b[A\n",
      " 94%|█████████▍| 30/32 [00:06<00:00,  4.95batches/s]\u001b[A\n",
      "100%|██████████| 32/32 [00:06<00:00,  4.91batches/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "distances generation\n",
    "test true labels and pred labels \n",
    "\"\"\"\n",
    "loader = tqdm(test_dl, total=len(test_dl), unit=\"batches\")\n",
    "model.eval()    \n",
    "with torch.no_grad():\n",
    "    test_true=[]\n",
    "    test_pred=[]\n",
    "    for batch in loader:\n",
    "        input_ids,attn_mask,y=batch\n",
    "        classfn_out,_=model(input_ids,attn_mask,y,use_decoder=False,use_classfn=1)\n",
    "        predict=torch.argmax(classfn_out,dim=1)\n",
    "#         correct_idxs.append(torch.nonzero((predicted==y.cuda())).view(-1)\n",
    "        test_pred.append(predict.cpu().numpy())\n",
    "        test_true.append(y.cpu().numpy())\n",
    "test_true=np.concatenate(test_true)\n",
    "test_pred=np.concatenate(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32108768-aeb1-406a-a7c5-71d60f393bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:22<00:00,  5.06batches/s]\n"
     ]
    }
   ],
   "source": [
    "bestk_train_data_per_proto=get_bestk_train_data_for_every_proto(train_dataset_eval, \n",
    "                                                   model_new=model, top_k=5)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "distances generation\n",
    "csv generation\n",
    "\"\"\"\n",
    "import csv\n",
    "\n",
    "fields = [\"S.No.\", \"Test Sentence\",\"Predicted\",\"Actual\",\"Actual Prop or NonProp\"]\n",
    "num_protos_per_test=5\n",
    "num_train_per_proto=5\n",
    "for i in range(num_protos_per_test):\n",
    "    for j in range(num_train_per_proto):\n",
    "        fields.append(f\"Prototype_{i}_wieght0\")\n",
    "        fields.append(f\"Prototype_{i}_wieght1\")\n",
    "        fields.append(f\"Prototype_{i}_Nearest_train_eg_{j}\")\n",
    "        fields.append(f\"Prototype_{i}_Nearest_train_eg_{j}_actuallabel\")\n",
    "        fields.append(f\"Prototype_{i}_Nearest_train_eg_{j}_distance\")\n",
    "        \n",
    "filename = f\"{model_path[len('Models/'):]}_nearest.csv\"\n",
    "weights=model.classfn_model.weight.detach().cpu().numpy()\n",
    "with open(filename, 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields)\n",
    "    for i in range(len(test_sents_joined)):\n",
    "#     for i in range(100):\n",
    "        row=[i,test_sents_joined[i],test_pred[i],test_ls[i],test_true[i]]\n",
    "        for j in range(num_protos_per_test):\n",
    "            proto_idx=best_protos_per_testeg[0][i][j]\n",
    "            for k in range(num_train_per_proto):\n",
    "#                 print(i,j,k)\n",
    "                row.append(weights[0][proto_idx])\n",
    "                row.append(weights[1][proto_idx])\n",
    "                row.append(train_sents_joined[bestk_train_data_per_proto[0][proto_idx][k]])\n",
    "                row.append(train_ls[bestk_train_data_per_proto[0][proto_idx][k]])\n",
    "                row.append(bestk_train_data_per_proto[1][k][proto_idx])\n",
    "\n",
    "        csvwriter.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prototex",
   "language": "python",
   "name": "prototex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
