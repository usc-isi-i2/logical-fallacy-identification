{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_and_freq_baseline(dataset: str, test:str):\n",
    "    train_df = pd.read_csv(f\"{dataset}/train.csv\")\n",
    "    test_df = pd.read_csv(f\"{dataset}/{test}\")\n",
    "\n",
    "    train_labels = train_df[\"label\"].values\n",
    "    labels_stats = dict(Counter(train_labels))\n",
    "    \n",
    "    all_accuracies_random = []\n",
    "    all_accuracies_freq = []\n",
    "    all_f1_random = []\n",
    "    all_f1_freq = []\n",
    "    all_precision_random = []\n",
    "    all_precision_freq = []\n",
    "    all_recall_random = []\n",
    "    all_recall_freq = []\n",
    "    \n",
    "    for _ in range(1000):\n",
    "\n",
    "        random_predictions = np.random.choice(list(labels_stats.keys()), size=len(test_df))\n",
    "        count_labels = list(labels_stats.values())\n",
    "        prob_labels = [x / sum(count_labels) for x in count_labels]\n",
    "        freq_predictions = np.random.choice(list(labels_stats.keys()), size=len(test_df), p=prob_labels)\n",
    "        correct_predictions = test_df[\"label\"].values\n",
    "        \n",
    "        all_accuracies_freq.append(accuracy_score(correct_predictions, freq_predictions))\n",
    "        all_accuracies_random.append(accuracy_score(correct_predictions, random_predictions))\n",
    "        \n",
    "        all_f1_freq.append(f1_score(correct_predictions, freq_predictions, average=\"weighted\"))\n",
    "        all_f1_random.append(f1_score(correct_predictions, random_predictions, average=\"weighted\"))\n",
    "        \n",
    "        all_precision_freq.append(precision_score(correct_predictions, freq_predictions, average=\"weighted\"))\n",
    "        all_precision_random.append(precision_score(correct_predictions, random_predictions, average=\"weighted\"))\n",
    "        \n",
    "        all_recall_freq.append(recall_score(correct_predictions, freq_predictions, average=\"weighted\"))\n",
    "        all_recall_random.append(recall_score(correct_predictions, random_predictions, average=\"weighted\"))\n",
    "        \n",
    "    print(f\"Accuracy random: {np.mean(all_accuracies_random)}\")\n",
    "    print(f\"Precision random: {np.mean(all_precision_random)}\")\n",
    "    print(f\"Recall random: {np.mean(all_recall_random)}\")\n",
    "    print(f\"F1 random: {np.mean(all_f1_random)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Accuracy freq: {np.mean(all_accuracies_freq)}\")\n",
    "    print(f\"Precision freq: {np.mean(all_precision_freq)}\")\n",
    "    print(f\"Recall freq: {np.mean(all_recall_freq)}\")\n",
    "    print(f\"F1 freq: {np.mean(all_f1_freq)}\")\n",
    "    print('----' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of the datapoints in the training split without the augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "faulty generalization     281\n",
       "ad hominem                185\n",
       "ad populum                144\n",
       "false causality           132\n",
       "circular reasoning        110\n",
       "appeal to emotion         109\n",
       "fallacy of relevance      102\n",
       "fallacy of logic          101\n",
       "intentional                92\n",
       "fallacy of credibility     89\n",
       "false dilemma              87\n",
       "fallacy of extension       80\n",
       "equivocation               32\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"finegrained/train.csv\")\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the ranodm baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigbench\n",
      "Accuracy random: 0.49970238095238095\n",
      "Precision random: 0.5018756395795523\n",
      "Recall random: 0.49970238095238095\n",
      "F1 random: 0.4999438146148811\n",
      "Accuracy freq: 0.5013571428571428\n",
      "Precision freq: 0.5013693745253403\n",
      "Recall freq: 0.5013571428571428\n",
      "F1 freq: 0.5010694117475375\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "coarsegrained\n",
      "Accuracy random: 0.24942105263157896\n",
      "Precision random: 0.4135434995353205\n",
      "Recall random: 0.24942105263157896\n",
      "F1 random: 0.2986321743579108\n",
      "Accuracy freq: 0.41536842105263155\n",
      "Precision freq: 0.41346144306844373\n",
      "Recall freq: 0.41536842105263155\n",
      "F1 freq: 0.41375048085621596\n",
      "----------------------------------------\n",
      "climate\n",
      "Accuracy random: 0.24927329192546582\n",
      "Precision random: 0.5080008544018725\n",
      "Recall random: 0.24927329192546582\n",
      "F1 random: 0.32351433939685575\n",
      "Accuracy freq: 0.4464223602484473\n",
      "Precision freq: 0.5088130521486752\n",
      "Recall freq: 0.4464223602484473\n",
      "F1 freq: 0.4680894153139386\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "finegrained\n",
      "Accuracy random: 0.07658006042296074\n",
      "Precision random: 0.09491004808028873\n",
      "Recall random: 0.07658006042296074\n",
      "F1 random: 0.07975250492297234\n",
      "Accuracy freq: 0.09470996978851963\n",
      "Precision freq: 0.09444566424200806\n",
      "Recall freq: 0.09470996978851963\n",
      "F1 freq: 0.09390198942321863\n",
      "----------------------------------------\n",
      "climate\n",
      "Accuracy random: 0.07732298136645963\n",
      "Precision random: 0.1248235213674665\n",
      "Recall random: 0.07732298136645963\n",
      "F1 random: 0.08504663694627561\n",
      "Accuracy freq: 0.07988198757763974\n",
      "Precision freq: 0.12099580018937803\n",
      "Recall freq: 0.07988198757763974\n",
      "F1 freq: 0.08071255743846743\n",
      "----------------------------------------\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for dataset in [\"bigbench\", \"coarsegrained\", \"finegrained\"]:\n",
    "    print(dataset)\n",
    "    get_random_and_freq_baseline(dataset, \"test.csv\")\n",
    "    \n",
    "    if dataset != \"bigbench\":\n",
    "        print('climate')\n",
    "        get_random_and_freq_baseline(dataset, \"climate_test.csv\")\n",
    "    print('---' * 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "633161fe0ba17fb17441f37e78ee62647412cabaeb5286dee9ea24a88b4a361a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
