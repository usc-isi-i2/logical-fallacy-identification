{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join('.', \"../../cbr_analyser/amr/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = joblib.load(\"../masked_sentences_with_AMR_container_objects_train.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_label = {}\n",
    "for obj in train_data:\n",
    "    train_to_label[obj[1].sentence.strip()] = obj[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions = pd.read_csv(\"basic_model/test.csv\")\n",
    "\n",
    "base_predictions.rename(columns={'predictions': 'base_predictions'}, inplace=True)\n",
    "base_predictions.drop(columns=['true_labels', 'masked_articles'], inplace=True)\n",
    "\n",
    "gcn_predictions = pd.read_csv(\"gcn/test.csv\")\n",
    "gcn_predictions.dropna(inplace=True)\n",
    "gcn_predictions['cbr_label'] = list(map(lambda x: train_to_label[x], gcn_predictions['cbr']))\n",
    "gcn_predictions = pd.concat([base_predictions, gcn_predictions], axis = 1)\n",
    "\n",
    "simcse_predictions = pd.read_csv(\"simcse/test.csv\")\n",
    "simcse_predictions['cbr_label'] = list(map(lambda x: train_to_label[x], simcse_predictions['cbr']))\n",
    "simcse_predictions = pd.concat([base_predictions, simcse_predictions], axis = 1)\n",
    "\n",
    "empathy_predictions = pd.read_csv(\"empathy/test.csv\")\n",
    "empathy_predictions['cbr_label'] = list(map(lambda x: train_to_label[x], empathy_predictions['cbr']))\n",
    "empathy_predictions = pd.concat([base_predictions, empathy_predictions], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simcse\n",
      "Number of cases that are actually the true labels 0.4101694915254237\n",
      "number of helpful and harmful cases\n",
      "24\n",
      "28\n",
      "Classes that they are helpful in\n",
      "appeal to emotion         6\n",
      "fallacy of credibility    4\n",
      "fallacy of relevance      4\n",
      "ad populum                4\n",
      "circular reasoning        2\n",
      "faulty generalization     2\n",
      "intentional               1\n",
      "false causality           1\n",
      "Name: true_labels, dtype: int64\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "classes that are harmful for\n",
      "fallacy of logic          5\n",
      "ad hominem                5\n",
      "fallacy of relevance      3\n",
      "faulty generalization     3\n",
      "circular reasoning        2\n",
      "fallacy of credibility    2\n",
      "appeal to emotion         2\n",
      "fallacy of extension      2\n",
      "ad populum                2\n",
      "intentional               1\n",
      "false causality           1\n",
      "Name: true_labels, dtype: int64\n",
      "******************************\n",
      "gcn\n",
      "Number of cases that are actually the true labels 0.3423728813559322\n",
      "number of helpful and harmful cases\n",
      "19\n",
      "63\n",
      "Classes that they are helpful in\n",
      "faulty generalization     7\n",
      "fallacy of relevance      4\n",
      "appeal to emotion         3\n",
      "fallacy of extension      1\n",
      "circular reasoning        1\n",
      "fallacy of logic          1\n",
      "ad hominem                1\n",
      "fallacy of credibility    1\n",
      "Name: true_labels, dtype: int64\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "classes that are harmful for\n",
      "ad hominem                12\n",
      "faulty generalization     12\n",
      "ad populum                 9\n",
      "false causality            5\n",
      "fallacy of credibility     5\n",
      "circular reasoning         4\n",
      "fallacy of logic           3\n",
      "fallacy of relevance       3\n",
      "false dilemma              3\n",
      "appeal to emotion          3\n",
      "fallacy of extension       3\n",
      "intentional                1\n",
      "Name: true_labels, dtype: int64\n",
      "******************************\n",
      "empathy\n",
      "Number of cases that are actually the true labels 0.2\n",
      "number of helpful and harmful cases\n",
      "14\n",
      "28\n",
      "Classes that they are helpful in\n",
      "faulty generalization     3\n",
      "intentional               2\n",
      "false causality           2\n",
      "fallacy of credibility    2\n",
      "appeal to emotion         2\n",
      "fallacy of relevance      1\n",
      "circular reasoning        1\n",
      "ad populum                1\n",
      "Name: true_labels, dtype: int64\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "classes that are harmful for\n",
      "fallacy of relevance      5\n",
      "ad hominem                4\n",
      "faulty generalization     4\n",
      "fallacy of credibility    3\n",
      "false causality           2\n",
      "circular reasoning        2\n",
      "intentional               2\n",
      "fallacy of logic          2\n",
      "fallacy of extension      2\n",
      "false dilemma             1\n",
      "appeal to emotion         1\n",
      "Name: true_labels, dtype: int64\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "for df, name in zip([simcse_predictions, gcn_predictions, empathy_predictions], ['simcse', 'gcn', 'empathy']):\n",
    "    print(name)\n",
    "    print('Number of cases that are actually the true labels', df[df['cbr_label'] == df['true_labels']].shape[0] / df.shape[0])\n",
    "\n",
    "    helpful = df[(df['base_predictions'] != df['true_labels']) & (df['predictions'] == df['true_labels'])]\n",
    "    harmful = df[(df['base_predictions'] == df['true_labels']) & (df['predictions'] != df['true_labels'])]\n",
    "    \n",
    "    \n",
    "    print('number of helpful and harmful cases')\n",
    "    print(len(helpful))\n",
    "    print(len(harmful))\n",
    "    \n",
    "    print('Classes that they are helpful in')\n",
    "    print(helpful['true_labels'].value_counts())\n",
    "    \n",
    "    print('$$$' * 10)\n",
    "    \n",
    "    print('classes that are harmful for')\n",
    "    print(harmful['true_labels'].value_counts())\n",
    "    \n",
    "    print('***' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('general')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67db27fda20c3469892b63d0da2b9ea8ebefbe65ab5f07c46b1a9548ed206d0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
