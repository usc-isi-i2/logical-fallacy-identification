{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86182d11-e222-4dc7-8af5-3ffa3a927e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from importlib import reload  \n",
    "import numpy as np\n",
    "import torch,time\n",
    "from transformers import BartModel,BartConfig,BartForConditionalGeneration,BartForCausalLM\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8b195ea-9934-4ecf-b54b-e096c439e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "  \n",
    "# setting path to enable import from the parent directory\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3647212-f6c0-4bb4-a44e-68f2f83e0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, score_at_min1=0,patience=100, verbose=False, delta=0, path='checkpoint.pt',\n",
    "                 trace_func=print,save_epochwise=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = score_at_min1\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        self.state_dict_list=[None]*patience\n",
    "        self.improved=0\n",
    "        self.stop_update=0\n",
    "        self.save_model_counter=0\n",
    "        self.save_epochwise=save_epochwise\n",
    "        self.times_improved=0\n",
    "        self.activated=False\n",
    "    def activate(self,s1,s2):\n",
    "        if not self.activated and s1>0 and s2>0: self.activated=True\n",
    "    def __call__(self, score, epoch,model):\n",
    "        if not self.activated: return None\n",
    "        self.save_model_counter = (self.save_model_counter + 1) % 4\n",
    "        if not self.stop_update:\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'\\033[91m The val score  of epoch {epoch} is {score:.4f} \\033[0m')\n",
    "            if score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                self.trace_func(f'\\033[93m EarlyStopping counter: {self.counter} out of {self.patience} \\033[0m')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "                self.improved=0\n",
    "            else:\n",
    "                self.save_checkpoint(score, model,epoch)\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "                self.improved=1\n",
    "        else:\n",
    "            self.improved=0 #not needed though\n",
    "\n",
    "    def save_checkpoint(self, score, model,epoch):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        # if self.verbose:\n",
    "        self.times_improved+=1\n",
    "        self.trace_func(f'\\033[92m Validation score improved ({self.best_score:.4f} --> {score:.4f}). \\033[0m')\n",
    "        if self.save_epochwise:\n",
    "            path=self.path+\"_\"+str(self.times_improved)+\"_\"+str(epoch)\n",
    "        else:\n",
    "            path=self.path\n",
    "        torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ceb6ba7-a1dd-4806-ad0b-9d9b0c3d8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import make_dataset\n",
    "import pathlib\n",
    "train=make_dataset(pathlib.Path(\"../data/protechn_corpus_eval/train/\"))\n",
    "val=make_dataset(pathlib.Path(\"../data/protechn_corpus_eval/dev/\"))\n",
    "test=make_dataset(pathlib.Path(\"../data/protechn_corpus_eval/test/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d9e6bb0-c5ca-4d79-9e53-33965ff44e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6a8a72f-1a8e-4f20-b8a5-103c46a576b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import make_bert_dataset,make_bert_testset\n",
    "train_=make_bert_testset(train)\n",
    "val_=make_bert_testset(val)\n",
    "test_=make_bert_testset(test)\n",
    "train_sents=[ list(map(lambda x: x[1] if x[0]==0 else \" \"+x[1], enumerate(i))) for d in train_[0] for i in d]\n",
    "val_sents=[ list(map(lambda x: x[1] if x[0]==0 else \" \"+x[1], enumerate(i))) for d in val_[0] for i in d]\n",
    "test_sents=[ list(map(lambda x: x[1] if x[0]==0 else \" \"+x[1], enumerate(i))) for d in test_[0] for i in d]\n",
    "def create_labels(dataset):\n",
    "    temp=[ set(i)-set(\"O\") for d in dataset[1] for i in d]\n",
    "    return [ next(iter(i)) if len(i)>0 else \"O\"  for i in temp]\n",
    "train_ls=create_labels(train_)\n",
    "val_ls=create_labels(val_)\n",
    "test_ls=create_labels(test_)\n",
    "train_y_txt=[ i for d in train_[1] for i in d]\n",
    "val_y_txt=[ i for d in val_[1] for i in d]\n",
    "test_y_txt=[ i for d in test_[1] for i in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cabafab5-aab5-495f-ae3e-85ed83edf97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_set={'Appeal_to_Authority',\n",
    " 'Appeal_to_fear-prejudice',\n",
    " 'Bandwagon',\n",
    " 'Black-and-White_Fallacy',\n",
    " 'Causal_Oversimplification',\n",
    " 'Doubt',\n",
    " 'Exaggeration,Minimisation',\n",
    " 'Flag-Waving',\n",
    " 'Loaded_Language',\n",
    " 'Name_Calling,Labeling',\n",
    " 'O',\n",
    " 'Obfuscation,Intentional_Vagueness,Confusion',\n",
    " 'Red_Herring',\n",
    " 'Reductio_ad_hitlerum',\n",
    " 'Repetition',\n",
    " 'Slogans',\n",
    " 'Straw_Men',\n",
    " 'Thought-terminating_Cliches',\n",
    " 'Whataboutism'}\n",
    "train_idx_bylabel={x: [i for i in range(len(train_ls)) if train_ls[i]==x] for x in labels_set} \n",
    "val_idx_bylabel={x: [i for i in range(len(val_ls)) if val_ls[i]==x] for x in labels_set} \n",
    "test_idx_bylabel={x: [i for i in range(len(test_ls)) if test_ls[i]==x] for x in labels_set} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b24cbc76-be6b-4749-a243-29d0f3a16799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x,y,y_txt,it_is_train=1,pos_or_neg=None,fix_seq_len=256,balance=False,\n",
    "                 specific_label=None,for_protos=False):\n",
    "        self.x=[]\n",
    "        self.attn_mask=[]\n",
    "        self.labels_mask=[]\n",
    "        self.y_txt=[]\n",
    "        self.y=[]\n",
    "        self.labels_ids={}\n",
    "        for i in labels_set:\n",
    "            self.labels_ids[i]=len(self.labels_ids)\n",
    "        self.y_fine_int=[]\n",
    "        it_is_train_proxy=it_is_train\n",
    "        for split_sent,y_tags,y_sent in zip(x,y_txt,y):\n",
    "            if specific_label is not None and specific_label!=y_sent: continue\n",
    "            if pos_or_neg==\"pos\" and y_sent==\"O\": continue\n",
    "            elif pos_or_neg==\"neg\" and y_sent!=\"O\": continue                \n",
    "            if y_sent==\"O\":\n",
    "                it_is_train=0\n",
    "            else:\n",
    "                it_is_train=it_is_train_proxy               \n",
    "            tmp=tokenizer(split_sent,is_split_into_words=False)[\"input_ids\"]\n",
    "            tmp_x=[]\n",
    "            tmp_attn=[]\n",
    "            tmp_y=[]\n",
    "            for i,chunk in enumerate(tmp):\n",
    "                if for_protos and y_tags[i]==\"O\":\n",
    "                    continue\n",
    "                tmp_y.extend([y_tags[i]]*len(chunk))\n",
    "                if y_tags[i]!=\"O\":\n",
    "                    mask=1\n",
    "                else:\n",
    "                    if it_is_train:\n",
    "                        mask=0\n",
    "                    else:\n",
    "                        mask=1\n",
    "                tmp_x.extend(chunk[1:-1])\n",
    "                tmp_attn.extend([mask]*(len(chunk)-2))\n",
    "            tmp_x.append(tokenizer.eos_token_id)\n",
    "            tmp_x.insert(0,tokenizer.bos_token_id)\n",
    "            tmp_attn.append(tmp_attn[-1])\n",
    "            tmp_attn.insert(0,tmp_attn[0])\n",
    "            self.x.append(tmp_x)\n",
    "            self.attn_mask.append(tmp_attn)\n",
    "            self.y_txt.append(tmp_y)\n",
    "            self.y.append(1 if y_sent!=\"O\" else 0)\n",
    "            self.y_fine_int.append(self.labels_ids[y_sent])\n",
    "        for tokid_sent in self.x:\n",
    "            tokid_sent.extend([tokenizer.pad_token_id]*(fix_seq_len-len(tokid_sent)))\n",
    "        for mask_vec in self.attn_mask:\n",
    "            mask_vec.extend([0]*(fix_seq_len-len(mask_vec)))\n",
    "        if balance:\n",
    "            num_pos=np.sum(self.y)\n",
    "            assert num_pos<len(self.y_fine_int)//2\n",
    "            \n",
    "            pos_indices=np.random.choice([i for i in range(len(self.y)) if self.y[i]==1],\n",
    "                                         size=len(self.y)-2*num_pos,replace=True)\n",
    "            self.x.extend([self.x[i] for i in pos_indices])\n",
    "            self.y.extend([1 for i in pos_indices])\n",
    "            self.y_fine_int.extend([self.y_fine_int[i] for i in pos_indices])\n",
    "            self.attn_mask.extend([self.attn_mask[i] for i in pos_indices])\n",
    "        self.fix_seq_len=fix_seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx],self.attn_mask[idx],self.y[idx]\n",
    "    def collate_fn(self,batch):        \n",
    "        return (torch.LongTensor([i[0] for i in batch]),\n",
    "                torch.Tensor([i[1] for i in batch]),\n",
    "                torch.LongTensor([i[2] for i in batch]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5012b7a8-5ac6-42a6-b5c5-1de4f3b15c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=BinaryClassDataset(train_sents,train_ls,train_y_txt,it_is_train=0,balance=True)\n",
    "val_dataset=BinaryClassDataset(val_sents,val_ls,val_y_txt,it_is_train=0)\n",
    "test_dataset=BinaryClassDataset(test_sents,test_ls,test_y_txt,it_is_train=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "517c1969-3d65-4a23-aca2-12d9cb034fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl=torch.utils.data.DataLoader(train_dataset,batch_size=20,shuffle=True,\n",
    "                                     collate_fn=train_dataset.collate_fn)\n",
    "val_dl=torch.utils.data.DataLoader(val_dataset,batch_size=128,shuffle=False,\n",
    "                                     collate_fn=val_dataset.collate_fn)\n",
    "test_dl=torch.utils.data.DataLoader(test_dataset,batch_size=128,shuffle=False,\n",
    "                                     collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8127b463-edb2-4301-8063-f8793d2b5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_logs(file,info,epoch,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1):\n",
    "    logs=[]\n",
    "    s=\" \".join((info+\" epoch\",str(epoch),\"Total loss %.4f\"%(val_loss),\"\\n\"))\n",
    "    logs.append(s)\n",
    "    print(s)\n",
    "    s=\" \".join((info+\" epoch\",str(epoch),\"Prec\",str(mac_val_prec),\"\\n\"))\n",
    "    logs.append(s)\n",
    "    print(s)\n",
    "    s=\" \".join((info+\" epoch\",str(epoch),\"Recall\",str(mac_val_rec),\"\\n\"))\n",
    "    logs.append(s)\n",
    "    print(s)\n",
    "    s=\" \".join((info+\" epoch\",str(epoch),\"F1\",str(mac_val_f1),\"\\n\"))\n",
    "    logs.append(s)\n",
    "    print(s)\n",
    "#     print(\"epoch\",epoch,\"MICRO val precision %.4f, recall %.4f, f1 %.4f,\"%(mic_val_prec,mic_val_rec,mic_val_f1))\n",
    "    print() \n",
    "    logs.append(\"\\n\")\n",
    "    f=open(file,\"a\")\n",
    "    f.writelines(logs)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58e8ba21-15d4-4bed-92ef-a40531dd1e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "def evaluate(dl,model_new=None,path=None,modelclass=None):\n",
    "    assert (model_new is not None) ^ (path is not None)\n",
    "    if path is not None:\n",
    "        model_new=modelclass().cuda()\n",
    "        model_new.load_state_dict(torch.load(path))\n",
    "    loader = tqdm(dl, total=len(dl), unit=\"batches\")\n",
    "    total_len=0\n",
    "    model_new.eval()    \n",
    "    with torch.no_grad():\n",
    "        total_loss=0\n",
    "        tts=0\n",
    "        y_pred=[]\n",
    "        y_true=[]\n",
    "        for batch in loader:\n",
    "            input_ids,attn_mask,y=batch\n",
    "            classfn_out,loss=model_new(input_ids,attn_mask,y,use_decoder=False,use_classfn=1)\n",
    "            if classfn_out.ndim==1:\n",
    "                predict=torch.zeros_like(y)\n",
    "                predict[classfn_out>0]=1\n",
    "            else:\n",
    "                predict=torch.argmax(classfn_out,dim=1)\n",
    "                \n",
    "            y_pred.append(predict.cpu().numpy())\n",
    "            y_true.append(y.cpu().numpy())\n",
    "            total_loss+=(len(input_ids)*loss[0].item())\n",
    "            total_len+=len(input_ids)\n",
    "        total_loss=total_loss/total_len\n",
    "        mac_prec,mac_recall,mac_f1_score,_=precision_recall_fscore_support(np.concatenate(y_true),np.concatenate(y_pred),labels=[0,1])\n",
    "        mic_prec,mic_recall,mic_f1_score,_=0,0,0,0\n",
    "\n",
    "    return total_loss,mac_prec,mac_recall,mac_f1_score,mic_prec,mic_recall,mic_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0ae2e4c-4e43-4a4c-9d25-f0e0f7254e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_prototypes=20\n",
    "num_pos_protos=19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a88b8d2-1700-48eb-b86b-6535ace7886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProtoTex(torch.nn.Module):\n",
    "    def __init__(self,n_classes=2):\n",
    "        super().__init__()\n",
    "        self.bart_model=BartForConditionalGeneration.from_pretrained('facebook/bart-large')   \n",
    "        self.bart_out_dim=self.bart_model.config.d_model\n",
    "        self.max_position_embeddings=256\n",
    "        self.num_protos=num_prototypes\n",
    "        self.prototypes=torch.nn.Parameter(torch.rand(self.num_protos,self.max_position_embeddings,self.bart_out_dim))\n",
    "        self.classfn_model=torch.nn.Linear(self.num_protos,2)\n",
    "        self.loss_fn=torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        \n",
    "        self.set_encoder_status(True)\n",
    "        self.set_decoder_status(False)\n",
    "        self.set_protos_status(False)\n",
    "        self.set_classfn_status(False)\n",
    "        \n",
    "        self.BNLayer=torch.nn.BatchNorm1d(self.num_protos)\n",
    "        \n",
    "    def set_encoder_status(self,status=True):\n",
    "        self.num_enc_layers=len(self.bart_model.base_model.encoder.layers)\n",
    "        for (i,x) in enumerate(self.bart_model.base_model.encoder.layers):\n",
    "            requires_grad=False\n",
    "            if i==self.num_enc_layers-1: requires_grad=status\n",
    "            for y in x.parameters():\n",
    "                y.requires_grad=requires_grad\n",
    "    def set_decoder_status(self,status=True):\n",
    "        self.num_dec_layers=len(self.bart_model.base_model.decoder.layers)\n",
    "        for (i,x) in enumerate(self.bart_model.base_model.decoder.layers):\n",
    "            requires_grad=False\n",
    "            if i==self.num_dec_layers-1: requires_grad=status\n",
    "            for y in x.parameters():\n",
    "                y.requires_grad=requires_grad\n",
    "    def set_classfn_status(self,status=True):\n",
    "        self.classfn_model.requires_grad=status\n",
    "    def set_protos_status(self,status=True):\n",
    "        self.prototypes.requires_grad=status       \n",
    "        \n",
    "\n",
    "    def forward(self,input_ids,attn_mask,y,use_decoder=1,use_classfn=0,use_rc=0,use_p1=0,use_p2=0,rc_loss_lamb=0.95,p1_lamb=0.93,p2_lamb=0.92):\n",
    "        batch_size=input_ids.size(0)\n",
    "        if use_decoder:\n",
    "            labels=input_ids.cuda()+0 \n",
    "            labels[labels==self.bart_model.config.pad_token_id]=-100\n",
    "            bart_output=self.bart_model(labels,attn_mask.cuda(),labels=labels,\n",
    "                                        output_attentions=False,output_hidden_states=False)\n",
    "            rc_loss,last_hidden_state=batch_size*bart_output.loss,bart_output.encoder_last_hidden_state\n",
    "        else:\n",
    "            rc_loss=0\n",
    "            last_hidden_state=self.bart_model.base_model.encoder(input_ids.cuda(),attn_mask.cuda(),\n",
    "                                                                 output_attentions=False,\n",
    "                                                                 output_hidden_states=False).last_hidden_state\n",
    "        input_for_classfn,l_p1,l_p2,classfn_out,classfn_loss=None,0,0,None,0\n",
    "        if use_classfn or use_p1 or use_p2:\n",
    "            input_for_classfn=torch.cdist(last_hidden_state.view(batch_size,-1),\n",
    "                                          self.prototypes.view(self.num_protos,-1))\n",
    "        if use_p1:\n",
    "            l_p1=torch.mean(torch.min(input_for_classfn,dim=0)[0])\n",
    "        if use_p2:            \n",
    "            l_p2=torch.mean(torch.min(input_for_classfn,dim=1)[0])\n",
    "        if use_classfn:\n",
    "            classfn_out=self.classfn_model(input_for_classfn).view(batch_size,2)\n",
    "            classfn_loss=self.loss_fn(classfn_out,y.cuda())\n",
    "        if not use_rc:\n",
    "            rc_loss=0\n",
    "        total_loss=classfn_loss+rc_loss_lamb*rc_loss+p1_lamb*l_p1+p2_lamb*l_p2\n",
    "        # return classfn_out,total_loss \n",
    "        return classfn_out, (total_loss, classfn_loss.detach().cpu(), rc_loss, l_p1,\n",
    "                             l_p2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df84b349-e680-458b-96fc-01eba9216f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()        \n",
    "model=SimpleProtoTex().cuda()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "modelname=\"apr_24_simpleprotobart_onlyclass_80_20_train_nomask_protos_yesmask_enc_on\"\n",
    "save_path=\"../Models/\"+modelname\n",
    "logs_path=\"../Logs/\"+modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a8610d-7440-4435-a1d4-e119d36b4b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3dbd4a27ce14b258e2dbabe0d377921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch -1 Total loss 57.4708 \n",
      "\n",
      "VAL SCORES epoch -1 Prec [0.66666667 0.        ] \n",
      "\n",
      "VAL SCORES epoch -1 Recall [1. 0.] \n",
      "\n",
      "VAL SCORES epoch -1 F1 [0.8 0. ] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da535d9227c744a2bf1f59d7249cee52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch -1 Total loss 86.2612 \n",
      "\n",
      "TRAIN SCORES epoch -1 Prec [0.5 0. ] \n",
      "\n",
      "TRAIN SCORES epoch -1 Recall [1. 0.] \n",
      "\n",
      "TRAIN SCORES epoch -1 F1 [0.66666667 0.        ] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58679eba67794ed1aaa3332fe498f608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dfe72007144470b7a5e49c5ef97430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 0 Total loss 0.6963 \n",
      "\n",
      "TRAIN SCORES epoch 0 Prec [0.50823896 0.58589815] \n",
      "\n",
      "TRAIN SCORES epoch 0 Recall [0.92751494 0.10255657] \n",
      "\n",
      "TRAIN SCORES epoch 0 F1 [0.65665742 0.17455819] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e584c01dfb49aca15bfdf228578162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 0 Total loss 0.6770 \n",
      "\n",
      "VAL SCORES epoch 0 Prec [0.67310878 0.38862559] \n",
      "\n",
      "VAL SCORES epoch 0 Recall [0.90430267 0.12166172] \n",
      "\n",
      "VAL SCORES epoch 0 F1 [0.77176322 0.18531073] \n",
      "\n",
      "\n",
      "\u001b[92m Validation score improved (-inf --> 0.4785). \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3697c8e4008482a831b390197f85831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SCORES epoch 0 Total loss 0.6672 \n",
      "\n",
      "TEST SCORES epoch 0 Prec [0.74040067 0.31619537] \n",
      "\n",
      "TEST SCORES epoch 0 Recall [0.90912197 0.11647727] \n",
      "\n",
      "TEST SCORES epoch 0 F1 [0.8161325  0.17024221] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159f3720a57d455ab80979453951708a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58981c64f3c745328f3f915d62033be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 1 Total loss 0.6959 \n",
      "\n",
      "TRAIN SCORES epoch 1 Prec [0.  0.5] \n",
      "\n",
      "TRAIN SCORES epoch 1 Recall [0. 1.] \n",
      "\n",
      "TRAIN SCORES epoch 1 F1 [0.         0.66666667] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beaddb5dc07b4a70b286d38b3275935b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 1 Total loss 0.7143 \n",
      "\n",
      "VAL SCORES epoch 1 Prec [0.         0.33333333] \n",
      "\n",
      "VAL SCORES epoch 1 Recall [0. 1.] \n",
      "\n",
      "VAL SCORES epoch 1 F1 [0.  0.5] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 1 out of 7 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9defdbd1f2a4417a80755ba2a28d33e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3e5bdd343a47d29504946439bb5cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 2 Total loss 0.7029 \n",
      "\n",
      "TRAIN SCORES epoch 2 Prec [0.5 0. ] \n",
      "\n",
      "TRAIN SCORES epoch 2 Recall [1. 0.] \n",
      "\n",
      "TRAIN SCORES epoch 2 F1 [0.66666667 0.        ] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60563aec63c64fcb923b13f366e03177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 2 Total loss 0.6560 \n",
      "\n",
      "VAL SCORES epoch 2 Prec [0.66666667 0.        ] \n",
      "\n",
      "VAL SCORES epoch 2 Recall [1. 0.] \n",
      "\n",
      "VAL SCORES epoch 2 F1 [0.8 0. ] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 2 out of 7 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe78b0ab7e24d61bf3e04ea48e350ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d410ec6f4b4e68b69b763ae14a7a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 3 Total loss 0.7005 \n",
      "\n",
      "TRAIN SCORES epoch 3 Prec [0.49936168 0.25      ] \n",
      "\n",
      "TRAIN SCORES epoch 3 Recall [0.99617984 0.00127339] \n",
      "\n",
      "TRAIN SCORES epoch 3 F1 [0.66524939 0.00253387] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a5a0d0ec3e457d80a9ecac80c50da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 3 Total loss 0.6592 \n",
      "\n",
      "VAL SCORES epoch 3 Prec [0.66666667 0.        ] \n",
      "\n",
      "VAL SCORES epoch 3 Recall [1. 0.] \n",
      "\n",
      "VAL SCORES epoch 3 F1 [0.8 0. ] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 3 out of 7 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c48c8ebcd01453f8d7c10bea2bc378b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2681963ac546d5ba2e7076b5d509ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 4 Total loss 0.6979 \n",
      "\n",
      "TRAIN SCORES epoch 4 Prec [0.48309889 0.46059716] \n",
      "\n",
      "TRAIN SCORES epoch 4 Recall [0.67616809 0.27652072] \n",
      "\n",
      "TRAIN SCORES epoch 4 F1 [0.56355621 0.34557473] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c332b4ca5502463d8b84d07e5abd8d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 4 Total loss 0.7008 \n",
      "\n",
      "VAL SCORES epoch 4 Prec [0.64687741 0.29793103] \n",
      "\n",
      "VAL SCORES epoch 4 Recall [0.62240356 0.32047478] \n",
      "\n",
      "VAL SCORES epoch 4 F1 [0.63440454 0.30879199] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 4 out of 7 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1efca1f63946c39efd40cae260cd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SCORES (not the best ones) epoch 4 Total loss 0.6941 \n",
      "\n",
      "TEST SCORES (not the best ones) epoch 4 Prec [0.72050147 0.23446105] \n",
      "\n",
      "TEST SCORES (not the best ones) epoch 4 Recall [0.66757772 0.28219697] \n",
      "\n",
      "TEST SCORES (not the best ones) epoch 4 F1 [0.69303068 0.25612376] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f319dd412e7041b7a040e981a0029a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bf5d53df174a8ca82a01a017fbeb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 5 Total loss 0.7161 \n",
      "\n",
      "TRAIN SCORES epoch 5 Prec [0.47623369 0.34971306] \n",
      "\n",
      "TRAIN SCORES epoch 5 Recall [0.8224116  0.09550397] \n",
      "\n",
      "TRAIN SCORES epoch 5 F1 [0.60318259 0.15003462] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493799bda15843b4b27fffdc9388bd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 5 Total loss 0.6713 \n",
      "\n",
      "VAL SCORES epoch 5 Prec [0.63442136 0.17210682] \n",
      "\n",
      "VAL SCORES epoch 5 Recall [0.79302671 0.08605341] \n",
      "\n",
      "VAL SCORES epoch 5 F1 [0.70491263 0.11473788] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 5 out of 7 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498bae3b76af47b58e27f71b67cc191b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4032e7cc816e4fccb5a6be1546683c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 6 Total loss 0.7517 \n",
      "\n",
      "TRAIN SCORES epoch 6 Prec [0.47734975 0.35224761] \n",
      "\n",
      "TRAIN SCORES epoch 6 Recall [0.827799   0.09364286] \n",
      "\n",
      "TRAIN SCORES epoch 6 F1 [0.60552431 0.14795326] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3706fc9bb94b4eb8e2e7b267c5c395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 6 Total loss 0.6716 \n",
      "\n",
      "VAL SCORES epoch 6 Prec [0.63636364 0.16339869] \n",
      "\n",
      "VAL SCORES epoch 6 Recall [0.81008902 0.07418398] \n",
      "\n",
      "VAL SCORES epoch 6 F1 [0.71279373 0.10204082] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 6 out of 7 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de473f09370a4316b2e640adf5e105d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e734d62bdb4a0a89a792f51d13b120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 7 Total loss 0.6900 \n",
      "\n",
      "TRAIN SCORES epoch 7 Prec [0.61437411 0.62120448] \n",
      "\n",
      "TRAIN SCORES epoch 7 Recall [0.63218729 0.60319326] \n",
      "\n",
      "TRAIN SCORES epoch 7 F1 [0.62315342 0.61206639] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec5f2cc87664bf1b000b1ec7526e4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 7 Total loss 0.7222 \n",
      "\n",
      "VAL SCORES epoch 7 Prec [0.73802817 0.41274817] \n",
      "\n",
      "VAL SCORES epoch 7 Recall [0.58308605 0.58605341] \n",
      "\n",
      "VAL SCORES epoch 7 F1 [0.6514712  0.48436542] \n",
      "\n",
      "\n",
      "\u001b[92m Validation score improved (0.4785 --> 0.5679). \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c139ee5f4c41ae9d9c9cb804e507e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SCORES epoch 7 Total loss 0.7038 \n",
      "\n",
      "TEST SCORES epoch 7 Prec [0.78436874 0.34811828] \n",
      "\n",
      "TEST SCORES epoch 7 Recall [0.66860266 0.4905303 ] \n",
      "\n",
      "TEST SCORES epoch 7 F1 [0.72187385 0.4072327 ] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff23980f18e47948e75e39f30b539f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad1b7abdb514dd295ccbfc9d89de39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 8 Total loss 0.7215 \n",
      "\n",
      "TRAIN SCORES epoch 8 Prec [0.48420803 0.42557359] \n",
      "\n",
      "TRAIN SCORES epoch 8 Recall [0.79890293 0.14898619] \n",
      "\n",
      "TRAIN SCORES epoch 8 F1 [0.60296455 0.22070667] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba833e86e0642d896bb486f5bb86392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 8 Total loss 0.6934 \n",
      "\n",
      "VAL SCORES epoch 8 Prec [0.63718958 0.20215633] \n",
      "\n",
      "VAL SCORES epoch 8 Recall [0.78041543 0.11127596] \n",
      "\n",
      "VAL SCORES epoch 8 F1 [0.70156719 0.14354067] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 1 out of 7 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad5dc6f727645cf8481a304a30f1f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8b7adb6a3d485398a9b1073a2aeca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 9 Total loss 0.7232 \n",
      "\n",
      "TRAIN SCORES epoch 9 Prec [0.49118943 0.46908127] \n",
      "\n",
      "TRAIN SCORES epoch 9 Recall [0.7645215  0.20805172] \n",
      "\n",
      "TRAIN SCORES epoch 9 F1 [0.59810721 0.28825405] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07654582a3864fcbb0ee4ba856d10e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 9 Total loss 0.7160 \n",
      "\n",
      "VAL SCORES epoch 9 Prec [0.63653724 0.22838137] \n",
      "\n",
      "VAL SCORES epoch 9 Recall [0.74183976 0.15281899] \n",
      "\n",
      "VAL SCORES epoch 9 F1 [0.68516615 0.18311111] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 2 out of 7 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c697784f1134b68a6cec3614e287c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SCORES (not the best ones) epoch 9 Total loss 0.6724 \n",
      "\n",
      "TEST SCORES (not the best ones) epoch 9 Prec [0.71732234 0.19855596] \n",
      "\n",
      "TEST SCORES (not the best ones) epoch 9 Recall [0.77246327 0.15625   ] \n",
      "\n",
      "TEST SCORES (not the best ones) epoch 9 F1 [0.74387235 0.17488076] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f4fdbfa04d49f89d94f1cb5b6b44dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fe3208cb944d6d80a454fae35ce00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 10 Total loss 0.7094 \n",
      "\n",
      "TRAIN SCORES epoch 10 Prec [0.50364378 0.50997986] \n",
      "\n",
      "TRAIN SCORES epoch 10 Recall [0.73787834 0.27279851] \n",
      "\n",
      "TRAIN SCORES epoch 10 F1 [0.59866487 0.35545629] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56ff4abaa974322a743a4202782f3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 10 Total loss 0.7344 \n",
      "\n",
      "VAL SCORES epoch 10 Prec [0.66761769 0.33548387] \n",
      "\n",
      "VAL SCORES epoch 10 Recall [0.69436202 0.30860534] \n",
      "\n",
      "VAL SCORES epoch 10 F1 [0.68072727 0.32148377] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 3 out of 7 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb12384297894724aed3333794b9fdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68484007fad471194ccfe7977f1db18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 11 Total loss 0.7021 \n",
      "\n",
      "TRAIN SCORES epoch 11 Prec [0.62357797 0.61707201] \n",
      "\n",
      "TRAIN SCORES epoch 11 Recall [0.60671956 0.63375453] \n",
      "\n",
      "TRAIN SCORES epoch 11 F1 [0.61503326 0.62530202] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80337d370e824a19b1886397e87e195d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 11 Total loss 0.7631 \n",
      "\n",
      "VAL SCORES epoch 11 Prec [0.73382046 0.39379699] \n",
      "\n",
      "VAL SCORES epoch 11 Recall [0.52151335 0.62166172] \n",
      "\n",
      "VAL SCORES epoch 11 F1 [0.60971379 0.48216341] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 4 out of 7 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa389094e4e74a5fa39ec8ae9182c11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1e104d97f247e1964e51aebdcada3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 12 Total loss 0.6960 \n",
      "\n",
      "TRAIN SCORES epoch 12 Prec [0.76124409 0.62363807] \n",
      "\n",
      "TRAIN SCORES epoch 12 Recall [0.48907826 0.84660594] \n",
      "\n",
      "TRAIN SCORES epoch 12 F1 [0.59553912 0.71821506] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a80443ed1e4116a694b0e7d98be749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 12 Total loss 0.7238 \n",
      "\n",
      "VAL SCORES epoch 12 Prec [0.70542636 0.38559814] \n",
      "\n",
      "VAL SCORES epoch 12 Recall [0.60756677 0.4925816 ] \n",
      "\n",
      "VAL SCORES epoch 12 F1 [0.65284974 0.43257329] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 5 out of 7 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1323f18861b4472d953e69868fa7fc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bd697747ff49d98bd8d754fa606e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 13 Total loss 0.6962 \n",
      "\n",
      "TRAIN SCORES epoch 13 Prec [0.97916667 0.53119457] \n",
      "\n",
      "TRAIN SCORES epoch 13 Recall [0.11969831 0.99745323] \n",
      "\n",
      "TRAIN SCORES epoch 13 F1 [0.21331937 0.69321624] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0d496de4134a27b6a389e37bbcff75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 13 Total loss 0.7801 \n",
      "\n",
      "VAL SCORES epoch 13 Prec [0.84567901 0.36749117] \n",
      "\n",
      "VAL SCORES epoch 13 Recall [0.20326409 0.92581602] \n",
      "\n",
      "VAL SCORES epoch 13 F1 [0.3277512  0.52613828] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 6 out of 7 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efacdab36fdd42df959e0e24238a2da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5c9eef6baf4160a6b685c6f728d2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SimpleProtoTEx\n",
    "\"\"\"\n",
    "from transformers.optimization import AdamW\n",
    "# optim=torch.optim.Adam(model.parameters(),lr=5e-5,weight_decay=0.01)\n",
    "optim=AdamW(model.parameters(),lr=3e-5,weight_decay=0.01,eps=1e-8)\n",
    "f=open(logs_path,\"w\")\n",
    "f.writelines([\"\"])\n",
    "f.close()\n",
    "epoch=-1\n",
    "val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(val_dl,model)\n",
    "print_logs(logs_path,\"VAL SCORES\",epoch,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)\n",
    "val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(train_dl,model)\n",
    "print_logs(logs_path,\"TRAIN SCORES\",epoch,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)\n",
    "es=EarlyStopping(-np.inf,patience=7,path=save_path,save_epochwise=False)\n",
    "n_iters=500\n",
    "for epoch in range(n_iters):\n",
    "    total_loss=0\n",
    "    model.train()\n",
    "    model.set_encoder_status(status=True)\n",
    "    model.set_decoder_status(status=False)\n",
    "    model.set_protos_status(status=True)\n",
    "    model.set_classfn_status(status=True)\n",
    "    classfn_loss,rc_loss,l_p1,l_p2,l_p3=[0]*5\n",
    "    train_loader = tqdm(train_dl, total=len(train_dl), unit=\"batches\",desc=\"training\")\n",
    "    for batch in train_loader:\n",
    "        input_ids,attn_mask,y=batch\n",
    "        classfn_out,loss=model(input_ids,attn_mask,y,use_decoder=0,use_classfn=1,\n",
    "                               use_rc=0,use_p1=1,use_p2=1,rc_loss_lamb=1.0,p1_lamb=1.0,\n",
    "                               p2_lamb=1.0)\n",
    "        optim.zero_grad()\n",
    "        loss[0].backward()\n",
    "        optim.step()\n",
    "        classfn_out=None\n",
    "        loss=None\n",
    "    total_loss=total_loss/len(train_dataset)\n",
    "    val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(train_dl,model)\n",
    "    print_logs(logs_path,\"TRAIN SCORES\",epoch,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)\n",
    "    es.activate(mac_val_f1[0],mac_val_f1[1])\n",
    "    val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(val_dl,model)\n",
    "    print_logs(logs_path,\"VAL SCORES\",epoch,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)\n",
    "    es((mac_val_f1[1]+mac_val_f1[0])/2,epoch,model)\n",
    "    if es.early_stop:\n",
    "        break\n",
    "    if es.improved:\n",
    "        \"\"\"\n",
    "        Below using \"val_\" prefix but the dl is that of test.\n",
    "        \"\"\"\n",
    "        val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(test_dl,model)\n",
    "        print_logs(logs_path,\"TEST SCORES\",epoch,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)\n",
    "    elif (epoch+1)%5==0:\n",
    "        \"\"\"\n",
    "        Below using \"val_\" prefix but the dl is that of test.\n",
    "        \"\"\"\n",
    "        val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(test_dl,model)\n",
    "        print_logs(logs_path,\"TEST SCORES (not the best ones)\",epoch,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c3091-f37d-4562-8c03-ec77524e623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoTex(torch.nn.Module):\n",
    "    def __init__(self,n_classes=2,bias=True,dropout=False,special_classfn=False,p=0.5,batchnormlp1=False):\n",
    "        super().__init__()\n",
    "        self.bart_model=BartForConditionalGeneration.from_pretrained('facebook/bart-large') \n",
    "        self.bart_out_dim=self.bart_model.config.d_model\n",
    "        self.one_by_sqrt_bartoutdim=1/torch.sqrt(torch.tensor(self.bart_out_dim).float())\n",
    "        self.max_position_embeddings=256\n",
    "        self.num_protos=num_prototypes\n",
    "        self.num_pos_protos=num_pos_protos\n",
    "        self.num_neg_protos=self.num_protos-self.num_pos_protos\n",
    "        self.pos_prototypes=torch.nn.Parameter(torch.rand(self.num_pos_protos,self.max_position_embeddings,self.bart_out_dim))\n",
    "        self.neg_prototypes=torch.nn.Parameter(torch.rand(self.num_neg_protos,self.max_position_embeddings,self.bart_out_dim))\n",
    "        self.classfn_model=torch.nn.Linear(self.num_protos,2,bias=bias)\n",
    "        self.loss_fn=torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        \n",
    "        self.do_dropout=dropout\n",
    "        self.special_classfn=special_classfn\n",
    "        \n",
    "        self.dropout=torch.nn.Dropout(p=p)\n",
    "        self.dobatchnorm=batchnormlp1\n",
    "        self.distance_grounder = torch.zeros(2, self.num_protos).cuda()\n",
    "        self.distance_grounder[0][:self.num_pos_protos] = 1e7\n",
    "        self.distance_grounder[1][self.num_pos_protos:] = 1e7\n",
    "\n",
    "    \n",
    "    def set_prototypes(self,do_random=False):\n",
    "        if do_random:\n",
    "            print(\"initializing prototypes with xavier init\")\n",
    "            torch.nn.init.xavier_normal_(self.pos_prototypes)\n",
    "            torch.nn.init.xavier_normal_(self.neg_prototypes)\n",
    "        else:\n",
    "            print(\"initializing prototypes with encoded outputs\")\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                self.pos_prototypes=torch.nn.Parameter(\n",
    "                    self.bart_model.base_model.encoder(input_ids_pos_rdm.cuda(),\n",
    "                                                       attn_mask_pos_rdm.cuda(),\n",
    "                                                       output_attentions=False,\n",
    "                                                       output_hidden_states=False).last_hidden_state)\n",
    "                self.neg_prototypes=torch.nn.Parameter(\n",
    "                    self.bart_model.base_model.encoder(input_ids_neg_rdm.cuda(),\n",
    "                                                       attn_mask_neg_rdm.cuda(),\n",
    "                                                       output_attentions=False,\n",
    "                                                       output_hidden_states=False).last_hidden_state)\n",
    "    \n",
    "    def set_shared_status(self,status=True):\n",
    "        print(\"ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\")\n",
    "        self.bart_model.model.shared.requires_grad_(status)\n",
    "\n",
    "    def set_encoder_status(self,status=True):\n",
    "        self.num_enc_layers=len(self.bart_model.base_model.encoder.layers)\n",
    "        for i in range(self.num_enc_layers):\n",
    "            self.bart_model.base_model.encoder.layers[i].requires_grad_(False)\n",
    "        self.bart_model.base_model.encoder.layers[self.num_enc_layers-1].requires_grad_(status)\n",
    "        return\n",
    "    def set_decoder_status(self,status=True):\n",
    "        self.num_dec_layers=len(self.bart_model.base_model.decoder.layers)\n",
    "        for i in range(self.num_dec_layers):\n",
    "            self.bart_model.base_model.decoder.layers[i].requires_grad_(False)\n",
    "        self.bart_model.base_model.decoder.layers[self.num_dec_layers-1].requires_grad_(status)\n",
    "        return\n",
    "    def set_classfn_status(self,status=True):\n",
    "        self.classfn_model.requires_grad_(status)\n",
    "\n",
    "    def set_protos_status(self,pos_or_neg=None,status=True):\n",
    "        if pos_or_neg==\"pos\" or pos_or_neg is None:\n",
    "            self.pos_prototypes.requires_grad=status       \n",
    "        if pos_or_neg==\"neg\" or pos_or_neg is None:\n",
    "            self.neg_prototypes.requires_grad=status       \n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attn_mask, y, use_decoder=1, use_classfn=0, use_rc=0, use_p1=0, use_p2=0,\n",
    "                use_p3=0, classfn_lamb=1.0, rc_loss_lamb=0.95, p1_lamb=0.93, p2_lamb=0.92, p3_lamb=1.0,\n",
    "                distmask_lp1=0,distmask_lp2=0,\n",
    "                pos_or_neg=None,random_mask_for_distanceMat=None):\n",
    "        \"\"\"\n",
    "            1. p3_loss is the prototype-distance-maximising loss. See the set of lines after the line \"if use_p3:\"\n",
    "            2. We also have flags distmask_lp1 and distmask_lp2 which uses \"masked\" distance matrix for calculating lp1 and lp2 loss.\n",
    "            3. the flag \"random_mask_for_distanceMat\" is an experimental part. It randomly masks (artificially inflates) \n",
    "            random places in the distance matrix so as to encourage more prototypes be \"discovered\" by the training \n",
    "            examples.\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.size(0)\n",
    "        if use_decoder:\n",
    "            labels = input_ids.cuda() + 0\n",
    "            labels[labels == self.bart_model.config.pad_token_id] = -100\n",
    "            bart_output = self.bart_model(input_ids.cuda(), attn_mask.cuda(), labels=labels,\n",
    "                                          output_attentions=False, output_hidden_states=False)\n",
    "            rc_loss, last_hidden_state = bart_output.loss, bart_output.encoder_last_hidden_state\n",
    "        else:\n",
    "            rc_loss = torch.tensor(0)\n",
    "            last_hidden_state = self.bart_model.base_model.encoder(input_ids.cuda(), attn_mask.cuda(),\n",
    "                                                                   output_attentions=False,\n",
    "                                                                   output_hidden_states=False).last_hidden_state\n",
    "        input_for_classfn, l_p1, l_p2, l_p3, l_p4, classfn_out, classfn_loss = (None, torch.tensor(0), torch.tensor(0),\n",
    "                                                                                torch.tensor(0), torch.tensor(0), None,\n",
    "                                                                                torch.tensor(0))\n",
    "        if use_classfn or use_p1 or use_p2 or use_p3:\n",
    "            all_protos = torch.cat((self.pos_prototypes, self.neg_prototypes), dim=0)\n",
    "            if use_classfn or use_p1 or use_p2:\n",
    "                if not self.dobatchnorm:\n",
    "                    input_for_classfn = self.one_by_sqrt_bartoutdim * torch.cdist(last_hidden_state.view(batch_size, -1),\n",
    "                                                                                  all_protos.view(self.num_protos, -1))\n",
    "                else:\n",
    "                    input_for_classfn = torch.cdist(last_hidden_state.view(batch_size, -1),\n",
    "                                                    all_protos.view(self.num_protos, -1))\n",
    "                    input_for_classfn= torch.nn.functional.instance_norm(\n",
    "                        input_for_classfn.view(batch_size,1,self.num_protos)).view(batch_size,\n",
    "                                                                                   self.num_protos)\n",
    "            if use_p1 or use_p2:\n",
    "                distance_mask = self.distance_grounder[y.cuda()]\n",
    "                input_for_classfn_masked = input_for_classfn+distance_mask\n",
    "                if random_mask_for_distanceMat:\n",
    "                    random_mask=torch.bernoulli(torch.ones_like(input_for_classfn_masked)*\n",
    "                                                random_mask_for_distanceMat).bool()\n",
    "                    input_for_classfn_masked[random_mask]=1e7\n",
    "        if use_p1:\n",
    "            l_p1 = torch.mean(torch.min(input_for_classfn_masked if distmask_lp1 else input_for_classfn, dim=0)[0])\n",
    "        if use_p2:\n",
    "            l_p2 = torch.mean(torch.min(input_for_classfn_masked if distmask_lp2 else input_for_classfn, dim=1)[0])\n",
    "        if use_p3:\n",
    "            l_p3 = self.one_by_sqrt_bartoutdim * torch.mean(torch.pdist(\n",
    "                self.pos_prototypes.view(self.num_pos_protos,-1)))\n",
    "        if use_classfn:\n",
    "            if self.do_dropout:\n",
    "                if self.special_classfn:\n",
    "                    classfn_out = (input_for_classfn@self.classfn_model.weight.t()+\n",
    "                                   self.dropout(self.classfn_model.bias.repeat(batch_size,1))).view(batch_size, 2)\n",
    "                else:\n",
    "                    classfn_out = self.classfn_model(self.dropout(input_for_classfn)).view(batch_size, 2)\n",
    "            else:\n",
    "                classfn_out = self.classfn_model(input_for_classfn).view(batch_size, 2)\n",
    "            classfn_loss = self.loss_fn(classfn_out, y.cuda())\n",
    "        if not use_rc:\n",
    "            rc_loss = torch.tensor(0)\n",
    "        total_loss = classfn_lamb * classfn_loss + rc_loss_lamb * rc_loss + p1_lamb * l_p1 + p2_lamb * l_p2 - p3_lamb * l_p3\n",
    "        return classfn_out, (total_loss, classfn_loss.detach().cpu(), rc_loss.detach().cpu(), l_p1.detach().cpu(),\n",
    "                             l_p2.detach().cpu(), l_p3.detach().cpu())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c9a4c-f945-4df5-8b05-7c75bf4c4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "modelname=\"NegProtoTEx_protos_xavier_large_bs20_20_woRat_noReco_g2d_nobias_nodrop_cu1_PosUp_normed\"\n",
    "model=ProtoTex(bias=False,dropout=False,special_classfn=False,p=0.75,batchnormlp1=True).cuda()\n",
    "model.set_prototypes(do_random=True)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "save_path=\"../Models/\"+modelname\n",
    "logs_path=\"../Logs/\"+modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb20877-aca8-4173-a1d9-78ea0a00bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ProtoTEx Training\n",
    "\"\"\"\n",
    "from transformers.optimization import AdamW\n",
    "optim=AdamW(model.parameters(),lr=3e-5,weight_decay=0.01,eps=1e-8)\n",
    "f=open(logs_path,\"w\")\n",
    "f.writelines([\"\"])\n",
    "f.close()\n",
    "val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(val_dl,model)\n",
    "epoch=-1\n",
    "print_logs(logs_path,\"VAL SCORES\",epoch,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)\n",
    "es=EarlyStopping(-np.inf,patience=7,path=save_path,save_epochwise=False)\n",
    "n_iters=1000\n",
    "gamma=2\n",
    "delta=1\n",
    "kappa=1\n",
    "p1_lamb=0.9\n",
    "p2_lamb=0.9\n",
    "p3_lamb=0.9\n",
    "for iter_ in range(n_iters):\n",
    "    total_loss = 0\n",
    "    \"\"\"\n",
    "    During Delta, We want decoder to become better at decoding the trained encoder\n",
    "    and Prototypes to become closer to some encoded representation. And that's why it makes \n",
    "    sense to use l_p1 loss and not l_p2 loss.\n",
    "    losses- rc_loss, l_p1 loss\n",
    "    trainable- decoder and prototypes\n",
    "    details- makes pos_prototypes closer to pos_egs and neg_protos closer to neg_egs \n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    model.set_encoder_status(status=False)\n",
    "    model.set_decoder_status(status=False)\n",
    "    model.set_protos_status(status=True)\n",
    "    model.set_classfn_status(status=False)\n",
    "    model.set_shared_status(status=True)\n",
    "\n",
    "    for epoch in range(delta):\n",
    "        train_loader = tqdm(train_dl, total=len(train_dl), unit=\"batches\", desc=\"delta training\")\n",
    "        for batch in train_loader:\n",
    "            input_ids, attn_mask, y = batch\n",
    "            classfn_out, loss = model(input_ids, attn_mask, y, use_decoder=0, use_classfn=0,\n",
    "                                      use_rc=0, use_p1=1, use_p2=0, use_p3=0,\n",
    "                                      rc_loss_lamb=1.0, p1_lamb=p1_lamb, p2_lamb=p2_lamb,\n",
    "                                      p3_lamb=p3_lamb,distmask_lp1=1,distmask_lp2=1,\n",
    "                                      random_mask_for_distanceMat=None)\n",
    "            optim.zero_grad()\n",
    "            loss[0].backward()\n",
    "            optim.step()\n",
    "    \"\"\"\n",
    "    During gamma, we only want to improve the classification performance. Therefore we will\n",
    "    improve encoder to become closer to the prototypes, at the same time also improving\n",
    "    the classification accuracy. That's why encoder and classification layer must be trainabl\n",
    "    together without segrregating pos and neg examples.\n",
    "    Only Encoder and Classfn are trainable\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    model.set_encoder_status(status=True)\n",
    "    model.set_decoder_status(status=False)\n",
    "    model.set_protos_status(status=False)\n",
    "    model.set_classfn_status(status=True)\n",
    "    model.set_shared_status(status=True)\n",
    "\n",
    "    for epoch in range(gamma):\n",
    "        train_loader = tqdm(train_dl, total=len(train_dl), unit=\"batches\", desc=\"gamma training\")\n",
    "        for batch in train_loader:\n",
    "            input_ids, attn_mask, y = batch\n",
    "            classfn_out, loss = model(input_ids, attn_mask, y, use_decoder=0, use_classfn=1,\n",
    "                                      use_rc=0, use_p1=0, use_p2=1,\n",
    "                                      rc_loss_lamb=1., p1_lamb=p1_lamb,p2_lamb=p2_lamb,\n",
    "                                      distmask_lp1 = 1, distmask_lp2 = 1)\n",
    "            optim.zero_grad()\n",
    "            loss[0].backward()\n",
    "            optim.step()\n",
    "\n",
    "    val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(train_dl,model)\n",
    "    print_logs(logs_path,\"TRAIN SCORES\",iter_,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)\n",
    "    es.activate(mac_val_f1[0],mac_val_f1[1])\n",
    "\n",
    "    val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(val_dl,model)\n",
    "    print_logs(logs_path,\"VAL SCORES\",iter_,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)        \n",
    "    es(0.5*(mac_val_f1[1]+mac_val_f1[0]),epoch,model)\n",
    "    if es.early_stop:\n",
    "        break\n",
    "    if es.improved:\n",
    "        \"\"\"\n",
    "        Below using \"val_\" prefix but the dl is that of test.\n",
    "        \"\"\"\n",
    "        val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(test_dl,model)\n",
    "        print_logs(logs_path,\"TEST SCORES\",iter_,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)\n",
    "    elif (iter_+1)%5==0:\n",
    "        \"\"\"\n",
    "        Below using \"val_\" prefix but the dl is that of test.\n",
    "        \"\"\"\n",
    "        val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(test_dl,model)\n",
    "        print_logs(logs_path,\"TEST SCORES (not the best ones)\",iter_,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2eef14-ee7f-4b10-a9b7-9f3c6f50aae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prototex",
   "language": "python",
   "name": "prototex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
