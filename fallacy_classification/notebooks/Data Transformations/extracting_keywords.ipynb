{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task Description for Experiment: \n",
    "* Given the source_text of sentences for the LOGIC dataset, \n",
    "  extract keywords. \n",
    "* Keywords can be extracted from spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keybert in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: rich>=10.4.0 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from keybert) (12.4.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from keybert) (1.0.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from keybert) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from keybert) (1.22.3)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from rich>=10.4.0->keybert) (4.1.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from rich>=10.4.0->keybert) (2.11.2)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from rich>=10.4.0->keybert) (0.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from scikit-learn>=0.22.2->keybert) (1.8.0)\n",
      "Requirement already satisfied: tqdm in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (4.49.0)\n",
      "Requirement already satisfied: torchvision in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (0.11.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (1.10.1)\n",
      "Requirement already satisfied: nltk in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Requirement already satisfied: huggingface-hub in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.95)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from sentence-transformers>=0.3.8->keybert) (4.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.3.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
      "Requirement already satisfied: sacremoses in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.0.49)\n",
      "Requirement already satisfied: filelock in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.6.0)\n",
      "Requirement already satisfied: requests in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.12.1)\n",
      "Requirement already satisfied: click in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.0.4)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (6.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.5.18)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.0.12)\n",
      "Requirement already satisfied: six in /nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_trf \n",
    "#!pip install git+https://github.com/LIAAD/yake\n",
    "!pip install keybert \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd \n",
    "import re \n",
    "import yake \n",
    "from keybert import KeyBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "yake_kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "keybert_kw_extractor = KeyBERT(model='all-mpnet-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_spacy(sentence): \n",
    "\n",
    "  doc = nlp(sentence)\n",
    "  entities = list(doc.ents) \n",
    "  return entities\n",
    "\n",
    "def extract_keywords_yake(sentence): \n",
    "  #yake_kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "  keywords = yake_kw_extractor.extract_keywords(sentence)\n",
    "  return keywords \n",
    "\n",
    "def extract_keywords_keybert(sentence): \n",
    "  #keybert_kw_extractor = KeyBERT(model='all-mpnet-base-v2')\n",
    "\n",
    "  keywords = keybert_kw_extractor.extract_keywords(sentence, keyphrase_ngram_range=(1,3), stop_words = 'english', highlight = False, top_n=3)\n",
    "  keywords_list = list(dict(keywords).keys()) \n",
    "  return keywords_list \n",
    "  \n",
    "def get_keywords_list(data, field, type): \n",
    "  sentences = data[field].to_list() \n",
    "  if type=='spacy': \n",
    "    keywords = [ extract_keywords_spacy(sentence) for sentence in sentences] \n",
    "  return keywords \n",
    "\n",
    "def get_stats_keywords(data): \n",
    "  empty_data = 0 \n",
    "  full_data = 0\n",
    "  sum_data = 0 \n",
    "  for elt in data: \n",
    "    #print(len(elt))\n",
    "    if len(elt) <1: \n",
    "      empty_data +=1 \n",
    "    else: \n",
    "      full_data+=1 \n",
    "    \n",
    "    sum_data+=len(elt) \n",
    "  return empty_data,full_data, round(empty_data/(empty_data+full_data), 2), round(full_data/(empty_data+full_data), 2), round(sum_data/len(data), 2) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Dilemma Prompt Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompts(sentence, label): \n",
    "  '''\n",
    "  Params: \n",
    "  1. sentence - sentence to parse \n",
    "  2. label - type of fallacy ( since the format changes from fallacy type to type)\n",
    "  '''\n",
    "  if label == 'false dilemma': \n",
    "    index_choice2 = sentence.find('Choice 2:')\n",
    "    choice2 = sentence[index_choice2+len('Choice 2:'):] \n",
    "    #print(choice2)\n",
    "    index_choice1 = sentence.find('Choice 1:') \n",
    "    choice1 = sentence[index_choice1+len('Choice 1:'):index_choice2] \n",
    "    #print(choice1)\n",
    "    return choice1 , choice2\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/pure_classes/false dilemma_train.csv') \n",
    "#dataset = dataset.drop('text_generated', axis=1)\n",
    "\n",
    "choice1_list, choice2_list = [], [] \n",
    "sentences = dataset['clean_prompt']\n",
    "for sentence in sentences: \n",
    "  choice1, choice2 = extract_prompts(sentence, 'false dilemma') \n",
    "  choice1_list.append(choice1) \n",
    "  choice2_list.append(choice2) \n",
    "\n",
    "dataset['choice 1'] = choice1_list \n",
    "dataset['choice 2'] = choice2_list \n",
    "\n",
    "dataset.to_csv('../data/pure_classes/false dilemma_train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                false dilemma \n",
       "1     candy can cause cavities \n",
       "2                 force people \n",
       "3                “you love me” \n",
       "4               rich kid thing \n",
       "Name: choice 1, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['choice 1'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' get stuck with cable '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['choice 2'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('academic decathlon', 0.04940384002065631),\n",
       " ('decathlon', 0.15831692877998726),\n",
       " ('academic', 0.29736558256021506)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keywords_yake(dataset['choice 2'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['choice_1_spacy'] = [ extract_keywords_spacy(choice1) for choice1 in dataset['choice 1'].to_list()]   \n",
    "dataset['choice_2_spacy'] = [ extract_keywords_spacy(choice2) for choice2 in dataset['choice 2'].to_list()]  \n",
    "dataset['choice_1_yake'] = [ extract_keywords_yake(choice1) for choice1 in dataset['choice 1'].to_list()]   \n",
    "dataset['choice_2_yake'] = [ extract_keywords_yake(choice2) for choice2 in dataset['choice 2'].to_list()]  \n",
    "dataset['choice_1_keybert'] = [ extract_keywords_keybert(choice1) for choice1 in dataset['choice 1'].to_list()]  \n",
    "dataset['choice_2_keybert'] = [ extract_keywords_keybert(choice2) for choice2 in dataset['choice 2'].to_list()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('../data/pure_classes/false dilemma_keywords.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics for spaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty, full, empty_percentage, full_percentage, avg_keywords = [], [], [], [], [] \n",
    "rows = ['choice_1_spacy', 'choice_2_spacy', 'choice_1_yake', 'choice_2_yake', 'choice_1_keybert', 'choice_2_keybert']\n",
    "\n",
    "for row in rows: \n",
    "  stats = get_stats_keywords(dataset[row].to_list()) \n",
    "  empty.append(stats[0]) \n",
    "  full.append(stats[1]) \n",
    "  empty_percentage.append(stats[2]) \n",
    "  full_percentage.append(stats[3]) \n",
    "  avg_keywords.append(stats[4]) \n",
    "statistics_keywords = {'empty_data':empty, 'full_data': full, 'empty_data_percentage': empty_percentage, 'full_data_percentage': full_percentage, 'average_num_keywords_per_sentence_extracted': avg_keywords} \n",
    "stats = pd.DataFrame.from_dict(statistics_keywords) \n",
    "stats.to_csv('../results/performance_of_keyword_extractors.csv')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('neuros')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4455a7d9a5fde2bdf2f7faf52cc6c3c081bc73476d60241f2f03234fa3a9b34e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
