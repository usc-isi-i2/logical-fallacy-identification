{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 50962169856\n",
      "Free memory: 22835298304\n",
      "Used memory: 28126871552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nvidia_smi\n",
    "nvidia_smi.nvmlInit()\n",
    "\n",
    "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "# card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "\n",
    "info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "print(\"Total memory:\", info.total)\n",
    "print(\"Free memory:\", info.free)\n",
    "print(\"Used memory:\", info.used)\n",
    "\n",
    "nvidia_smi.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get memory info (0, 0)\n",
      "Get number of devices available:  4\n",
      "Memory stats about which device is free: \n",
      "Device 0: b'Quadro RTX 8000', Memory : (44.81% free): 50962169856(total), 22835298304 (free), 28126871552 (used)\n",
      "Device 1: b'Quadro RTX 8000', Memory : (45.84% free): 50962169856(total), 23362732032 (free), 27599437824 (used)\n",
      "Device 2: b'Quadro RTX 8000', Memory : (37.59% free): 50962169856(total), 19157942272 (free), 31804227584 (used)\n",
      "Device 3: b'Quadro RTX 8000', Memory : (40.17% free): 50962169856(total), 20470759424 (free), 30491410432 (used)\n",
      "Current Device:   0\n"
     ]
    }
   ],
   "source": [
    "print(\"Get memory info\", torch.cuda.mem_get_info(device=None)) \n",
    "print(\"Get number of devices available: \", torch.cuda.device_count())\n",
    "\n",
    "print(\"Memory stats about which device is free: \")\n",
    "\n",
    "nvidia_smi.nvmlInit()\n",
    "\n",
    "deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
    "for i in range(deviceCount):\n",
    "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
    "    info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(\"Device {}: {}, Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(i, nvidia_smi.nvmlDeviceGetName(handle), 100*info.free/info.total, info.total, info.free, info.used))\n",
    "\n",
    "nvidia_smi.nvmlShutdown()\n",
    "\n",
    "print(\"Current Device:  \", torch.cuda.current_device()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bckg08.isi.edu/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mset_device(\u001b[39m0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bckg08.isi.edu/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mempty_cache()\n",
      "File \u001b[0;32m~/anaconda3/envs/neuros/lib/python3.8/site-packages/torch/cuda/memory.py:114\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 114\u001b[0m     torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_emptyCache()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.cuda.empty_cache() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicalFallacy(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataset\n",
    "        self.text = dataset.source_article\n",
    "        self.targets = dataset.mapped_label\n",
    "        self.max_len = max_len\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'sentence': text,\n",
    "            \n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "    \n",
    "        self.classifier = torch.nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop( train_loader, test_loader, label, test_data, epochs=5):\n",
    "  train_loss = []\n",
    "  test_loss = []\n",
    "  train_accuracy = []\n",
    "  test_accuracy = []\n",
    "  model = RobertaClass()\n",
    "  model.to(device)\n",
    "  loss_function = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  test_answers = [[[],[]], [[],[]]]\n",
    "  for epoch in range(epochs):\n",
    "    for phase in ['Train', 'Test']:\n",
    "      if(phase == 'Train'):\n",
    "        model.train()\n",
    "        loader = train_loader\n",
    "      else:\n",
    "        model.eval()\n",
    "        loader = test_loader  \n",
    "      epoch_loss = 0\n",
    "      epoch_acc = 0\n",
    "      len(loader)\n",
    "      for steps, data in tqdm(enumerate(loader, 0)):\n",
    "        sentence = data['sentence']\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "      \n",
    "        outputs = model.forward(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_function(outputs, targets)        \n",
    "        \n",
    "        epoch_loss += loss.detach()\n",
    "        _, max_indices = torch.max(outputs.data, dim=1)\n",
    "        bath_acc = (max_indices==targets).sum().item()/targets.size(0)\n",
    "        epoch_acc += bath_acc\n",
    "\n",
    "        if (phase == 'Train'):\n",
    "          train_loss.append(loss.detach()) \n",
    "          train_accuracy.append(bath_acc)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        else:\n",
    "          test_loss.append(loss.detach()) \n",
    "          test_accuracy.append(bath_acc)\n",
    "         \n",
    "\n",
    "      print(f\"{phase} Loss: {epoch_loss/steps}\")\n",
    "      print(f\"{phase} Accuracy: {epoch_acc/steps}\")\n",
    "  \n",
    "  torch.save(model, '../models/broad_classifiers/broad_classifier_trained_roberta_prompt_balanced.pt')\n",
    "  return (train_loss, test_loss, train_accuracy, test_accuracy, test_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver_code():\n",
    "    train_df = pd.read_csv('../data/broad_classifier/updated_edu_train.csv')\n",
    "    test_df = pd.read_csv('../data/broad_classifier/updated_edu_dev.csv')\n",
    "    \n",
    "\n",
    "    train_set = LogicalFallacy(train_df, tokenizer, MAX_LEN)\n",
    "    test_set = LogicalFallacy(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "    train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "    test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "    train_loader = DataLoader(train_set, **train_params)\n",
    "    test_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "    \n",
    "    value = train_loop( train_loader, test_loader, 'xxxx',  test_df['updated_label'], epochs=5)\n",
    "    return value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bckg08.isi.edu/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m vals \u001b[39m=\u001b[39m driver_code()\n",
      "\u001b[1;32m/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb Cell 11\u001b[0m in \u001b[0;36mdriver_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bckg08.isi.edu/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_set, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_params)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bckg08.isi.edu/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_set, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtest_params)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bckg08.isi.edu/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m value \u001b[39m=\u001b[39m train_loop( train_loader, test_loader, \u001b[39m'\u001b[39;49m\u001b[39mxxxx\u001b[39;49m\u001b[39m'\u001b[39;49m,  test_df[\u001b[39m'\u001b[39;49m\u001b[39mupdated_label\u001b[39;49m\u001b[39m'\u001b[39;49m], epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bckg08.isi.edu/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m value\n",
      "\u001b[1;32m/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(train_loader, test_loader, label, test_data, epochs)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bckg08.isi.edu/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m test_accuracy \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bckg08.isi.edu/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m RobertaClass()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bckg08.isi.edu/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bckg08.isi.edu/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m loss_function \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bckg08.isi.edu/data/vishnu/neurosymbolic-argumentation/notebooks/multi_class_classifier.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params \u001b[39m=\u001b[39m  model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mLEARNING_RATE)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuros/lib/python3.8/site-packages/torch/nn/modules/module.py:899\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    897\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 899\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuros/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    572\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/neuros/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    572\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/neuros/lib/python3.8/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    572\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/neuros/lib/python3.8/site-packages/torch/nn/modules/module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 593\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    594\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    595\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/neuros/lib/python3.8/site-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 897\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "vals = driver_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('neuros')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4455a7d9a5fde2bdf2f7faf52cc6c3c081bc73476d60241f2f03234fa3a9b34e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
