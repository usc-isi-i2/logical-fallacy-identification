{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report,  confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 50962169856\n",
      "Free memory: 444006400\n",
      "Used memory: 50518163456\n"
     ]
    }
   ],
   "source": [
    "import nvidia_smi\n",
    "nvidia_smi.nvmlInit()\n",
    "\n",
    "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "# card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "\n",
    "info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "print(\"Total memory:\", info.total)\n",
    "print(\"Free memory:\", info.free)\n",
    "print(\"Used memory:\", info.used)\n",
    "\n",
    "nvidia_smi.nvmlShutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get memory info (49782652928, 50962169856)\n",
      "Get number of devices available:  4\n",
      "Memory stats about which device is free: \n",
      "Device 0: b'Quadro RTX 8000', Memory : (2.32% free): 50962169856(total), 1180106752 (free), 49782063104 (used)\n",
      "Device 1: b'Quadro RTX 8000', Memory : (6.16% free): 50962169856(total), 3138846720 (free), 47823323136 (used)\n",
      "Device 2: b'Quadro RTX 8000', Memory : (57.40% free): 50962169856(total), 29251534848 (free), 21710635008 (used)\n",
      "Device 3: b'Quadro RTX 8000', Memory : (97.69% free): 50962169856(total), 49782652928 (free), 1179516928 (used)\n"
     ]
    }
   ],
   "source": [
    "print(\"Get memory info\", torch.cuda.mem_get_info(device=None)) \n",
    "print(\"Get number of devices available: \", torch.cuda.device_count())\n",
    "\n",
    "print(\"Memory stats about which device is free: \")\n",
    "\n",
    "nvidia_smi.nvmlInit()\n",
    "\n",
    "\n",
    "deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
    "for i in range(deviceCount):\n",
    "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
    "    info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(\"Device {}: {}, Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(i, nvidia_smi.nvmlDeviceGetName(handle), 100*info.free/info.total, info.total, info.free, info.used))\n",
    "\n",
    "nvidia_smi.nvmlShutdown()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device:   3\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Device:  \", torch.cuda.current_device()) \n",
    "torch.cuda.set_device(3)\n",
    "torch.cuda.empty_cache() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicalFallacy(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataset\n",
    "        self.text = dataset.clean_prompt\n",
    "        self.targets = dataset.label\n",
    "        self.max_len = max_len\n",
    "        #self.original_label = dataset.updated_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'sentence': text,\n",
    "            \n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "       # sfmax = torch.nn.functional.softmax(output)\n",
    "       # return sfmax\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accu(big_idx, targets):\n",
    "   \n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "def generate_classification_report(preds, targets): \n",
    "    target_names = ['fallacy of relevance', 'component fallacy', 'fallacy of ambiguity'] \n",
    "\n",
    "    print(classification_report(targets, preds, target_names=target_names, digits=4))\n",
    "    cm = confusion_matrix(targets, preds) \n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(cm)\n",
    "  \n",
    "    print(\"Per class Accuracy: \", cm.diagonal()/cm.sum(axis=1) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(loader, model_path, epochs=1):\n",
    "    val, og_val = [], [] \n",
    "    model = torch.load(model_path)\n",
    "    model.eval()\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    test_answers = [[[],[]], [[],[]]]\n",
    "\n",
    "    n_correct = 0 \n",
    "    nb_tr_steps = 0 \n",
    "    nb_tr_examples = 0 \n",
    "    for epoch in range(epochs):\n",
    "        for steps, data in tqdm(enumerate(loader, 0)):\n",
    "            sentence = data['sentence']\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "            outputs = model.forward(ids, mask, token_type_ids)\n",
    "            #print(torch.max(outputs.data, dim=1))\n",
    "            _, max_indices = torch.max(outputs.data, dim=1)\n",
    "            \n",
    "            val.extend(max_indices.tolist())   \n",
    "            \n",
    "           \n",
    "            og_val.extend(targets.tolist())\n",
    "            \n",
    "            n_correct+= calcuate_accu(max_indices, targets) \n",
    "\n",
    "            nb_tr_steps +=1 \n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "           \n",
    "    accuracy = (n_correct*100)/nb_tr_examples \n",
    "\n",
    "        \n",
    "    return accuracy, val, og_val\n",
    "                                                                \n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver_code(test_file, model_path, model_name):\n",
    "   \n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    test_set = LogicalFallacy(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "    \n",
    "    test_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "    #print(test_loader)\n",
    "    value, preds, targets = predict(test_loader, model_path)\n",
    "    print(\"Model Name: \", model_name)\n",
    "    print(\"Accuracy of the model: \", value) \n",
    "    print(\"Classification Report: \")\n",
    "    generate_classification_report(preds, targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "137it [00:03, 39.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name:  roBERTa -  - (only sentence)\n",
      "Accuracy of the model:  70.32967032967034\n",
      "Classification Report: \n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "fallacy of relevance     0.7773    0.6453    0.7052       265\n",
      "   component fallacy     0.7162    0.7910    0.7518       268\n",
      "fallacy of ambiguity     0.0333    0.0769    0.0465        13\n",
      "\n",
      "            accuracy                         0.7033       546\n",
      "           macro avg     0.5089    0.5044    0.5011       546\n",
      "        weighted avg     0.7296    0.7033    0.7124       546\n",
      "\n",
      "Confusion Matrix: \n",
      "[[171  75  19]\n",
      " [ 46 212  10]\n",
      " [  3   9   1]]\n",
      "Per class Accuracy:  [0.64528302 0.79104478 0.07692308]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "driver_code('../data/broad_classifier/updated_edu_test.csv','../models/broad_classifiers/broad_classifier_trained_roberta.pt',  \"roBERTa -  - (only sentence)\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "137it [00:03, 39.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name:  roBERTa -  - (only sentences + prompts)\n",
      "Accuracy of the model:  77.28937728937728\n",
      "Classification Report: \n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "fallacy of relevance     0.7854    0.7736    0.7795       265\n",
      "   component fallacy     0.7660    0.8060    0.7855       268\n",
      "fallacy of ambiguity     0.3333    0.0769    0.1250        13\n",
      "\n",
      "            accuracy                         0.7729       546\n",
      "           macro avg     0.6282    0.5522    0.5633       546\n",
      "        weighted avg     0.7651    0.7729    0.7668       546\n",
      "\n",
      "Confusion Matrix: \n",
      "[[205  58   2]\n",
      " [ 52 216   0]\n",
      " [  4   8   1]]\n",
      "Per class Accuracy:  [0.77358491 0.80597015 0.07692308]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "driver_code('../data/broad_classifier/updated_edu_test_with_neg.csv','../models/broad_classifiers/broad_classifier_trained_roberta_sentence_prompts_with_neg.pt',  \"roBERTa -  - (only sentences + prompts)\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "137it [00:03, 41.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name:  roBERTa -  - (only prompts)\n",
      "Accuracy of the model:  65.56776556776556\n",
      "Classification Report: \n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "fallacy of relevance     0.7019    0.5509    0.6173       265\n",
      "   component fallacy     0.6272    0.7910    0.6997       268\n",
      "fallacy of ambiguity     0.0000    0.0000    0.0000        13\n",
      "\n",
      "            accuracy                         0.6557       546\n",
      "           macro avg     0.4430    0.4473    0.4390       546\n",
      "        weighted avg     0.6485    0.6557    0.6431       546\n",
      "\n",
      "Confusion Matrix: \n",
      "[[146 119   0]\n",
      " [ 56 212   0]\n",
      " [  6   7   0]]\n",
      "Per class Accuracy:  [0.5509434  0.79104478 0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "driver_code('../data/broad_classifier/updated_edu_test_with_neg.csv','../models/broad_classifiers/broad_classifier_trained_roberta_prompts_with_neg.pt',  \"roBERTa -  - (only prompts)\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300, 259, 13]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/broad_classifier/updated_edu_dev_with_neg.csv')\n",
    "distribution = dataset['label'].value_counts().to_list() \n",
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.52, 0.45, 0.02]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_distribution = [ round(elt/sum(distribution),2) for elt in distribution] \n",
    "percentage_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4904"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.56**2+0.42**2+0.02**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/nas/home/vprasann/anaconda3/envs/neuros/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "199it [00:04, 40.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name:  roBERTa -  - (only sentence+prompts) balanced\n",
      "Accuracy of the model:  55.23329129886507\n",
      "Classification Report: \n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "fallacy of relevance     0.6596    0.7019    0.6801       265\n",
      "   component fallacy     0.4725    0.8657    0.6113       268\n",
      "fallacy of ambiguity     1.0000    0.0769    0.1429       260\n",
      "\n",
      "            accuracy                         0.5523       793\n",
      "           macro avg     0.7107    0.5482    0.4781       793\n",
      "        weighted avg     0.7080    0.5523    0.4807       793\n",
      "\n",
      "Confusion Matrix: \n",
      "[[186  79   0]\n",
      " [ 36 232   0]\n",
      " [ 60 180  20]]\n",
      "Per class Accuracy:  [0.70188679 0.86567164 0.07692308]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "driver_code('../data/broad_classifier/updated_edu_test_balanced.csv','../models/broad_classifiers/broad_classifier_trained_roberta_sentence_prompt_balanced.pt',  \"roBERTa -  - (only sentence+prompts) balanced\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('neuros')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4455a7d9a5fde2bdf2f7faf52cc6c3c081bc73476d60241f2f03234fa3a9b34e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
