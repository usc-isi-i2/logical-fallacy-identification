Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
wandb: Currently logged in as: zhpinkman. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.3
wandb: Run data is saved locally in /cluster/raid/home/zhivar.sourati/logical-fallacy-identification/AMR_parsers_and_graphs/wandb/run-20220930_231323-1uo9gpz9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-frost-32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zhpinkman/Logical%20Fallacy%20Main%20Classifier
wandb: üöÄ View run at https://wandb.ai/zhpinkman/Logical%20Fallacy%20Main%20Classifier/runs/1uo9gpz9

  0%|          | 0/2 [00:00<?, ?ba/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 19.81ba/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 19.73ba/s]

  0%|          | 0/1 [00:00<?, ?ba/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 117.56ba/s]

  0%|          | 0/1 [00:00<?, ?ba/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 116.50ba/s]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/cluster/raid/home/zhivar.sourati/logical-fallacy-identification/AMR_parsers_and_graphs/main_classifier.py", line 645, in <module>
    do_train_process(args)
  File "/cluster/raid/home/zhivar.sourati/logical-fallacy-identification/AMR_parsers_and_graphs/main_classifier.py", line 512, in do_train_process
    training_args = TrainingArguments(
  File "<string>", line 104, in __init__
  File "/cluster/raid/home/zhivar.sourati/anaconda3/envs/general/lib/python3.10/site-packages/transformers/training_args.py", line 1056, in __post_init__
    raise ValueError(
ValueError: --load_best_model_at_end requires the save and eval strategy to match, but found
- Evaluation strategy: steps
- Save strategy: no
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)
wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)
wandb: | 0.001 MB of 0.011 MB uploaded (0.000 MB deduped)
wandb: / 0.001 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: - 0.006 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: \ 0.008 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: | 0.008 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: / 0.008 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: - 0.008 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: \ 0.008 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: | 0.008 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: / 0.008 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: - 0.008 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: \ 0.008 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: | 0.008 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: / 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: - 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: \ 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: | 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: / 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: - 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb: \ 0.013 MB of 0.013 MB uploaded (0.000 MB deduped)
wandb:                                                                                
wandb: Synced zany-frost-32: https://wandb.ai/zhpinkman/Logical%20Fallacy%20Main%20Classifier/runs/1uo9gpz9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220930_231323-1uo9gpz9/logs
