09/22/2022 06:30:49 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
09/22/2022 06:30:49 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ./models/sp_model/config.json
09/22/2022 06:30:49 - INFO - pytorch_transformers.modeling_utils -   Model config {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "finetuning_task": "eg",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 1,
  "torchscript": false,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

09/22/2022 06:30:49 - INFO - pytorch_transformers.tokenization_utils -   Model name './models/sp_model' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli). Assuming './models/sp_model' is a path or url to a directory containing tokenizer files.
09/22/2022 06:30:49 - INFO - pytorch_transformers.tokenization_utils -   loading file ./models/sp_model/vocab.json
09/22/2022 06:30:49 - INFO - pytorch_transformers.tokenization_utils -   loading file ./models/sp_model/merges.txt
09/22/2022 06:30:49 - INFO - pytorch_transformers.tokenization_utils -   loading file ./models/sp_model/added_tokens.json
09/22/2022 06:30:49 - INFO - pytorch_transformers.tokenization_utils -   loading file ./models/sp_model/special_tokens_map.json
09/22/2022 06:30:49 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./models/sp_model/pytorch_model.bin
09/22/2022 06:31:24 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in RobertaForEX: ['classifier_stance.dense.weight', 'classifier_stance.dense.bias', 'classifier_stance.out_proj.weight', 'classifier_stance.out_proj.bias']
09/22/2022 06:31:24 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, cache_dir='./tmp', config_name='', data_cache_dir='./tmp', data_dir='./data', device=device(type='cuda'), do_eval=False, do_eval_edge=False, do_lower_case=True, do_prediction=True, do_train=False, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=1e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_nodes=20, max_seq_length=300, max_steps=-1, model_name_or_path='./models/sp_model', model_type='roberta_eg', n_gpu=1, no_cuda=False, num_train_epochs=10.0, output_dir='./models/sp_model', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, run_on_test=False, save_steps=50, seed=42, server_ip='', server_port='', task_name='eg', tokenizer_name='', warmup_pct=None, warmup_steps=0, weight_decay=0.1)
09/22/2022 06:31:24 - INFO - __main__ -   Prediction on the test set (note: Training will not be executed.) 
09/22/2022 06:31:24 - INFO - __main__ -   Creating features from dataset file at ./data
Traceback (most recent call last):
  File "structured_model/run_joint_model.py", line 744, in <module>
    main()
  File "structured_model/run_joint_model.py", line 669, in main
    result = evaluate(args, model, tokenizer, processor,
  File "structured_model/run_joint_model.py", line 264, in evaluate
    eval_dataset, examples = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True,
  File "structured_model/run_joint_model.py", line 465, in load_and_cache_examples
    examples = processor.get_test_examples(args.data_dir)
  File "/cluster/raid/home/zhivar.sourati/logical-fallacy-identification/AMR_parsers_and_graphs/ExplaGraphs/structured_model/utils_joint_model.py", line 71, in get_test_examples
    return self._create_examples(self._read_tsv(os.path.join(data_dir, "test.tsv")))
  File "/cluster/raid/home/zhivar.sourati/logical-fallacy-identification/AMR_parsers_and_graphs/ExplaGraphs/structured_model/utils_joint_model.py", line 223, in _create_examples
    stance_label = record[2]
IndexError: list index out of range
mv: cannot stat './models/sp_model/prediction_nodes_test.lst': No such file or directory
09/22/2022 06:31:28 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
09/22/2022 06:31:28 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ./models/sp_model/config.json
09/22/2022 06:31:28 - INFO - pytorch_transformers.modeling_utils -   Model config {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "finetuning_task": "eg",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 1,
  "torchscript": false,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

09/22/2022 06:31:28 - INFO - pytorch_transformers.tokenization_utils -   Model name './models/sp_model' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli). Assuming './models/sp_model' is a path or url to a directory containing tokenizer files.
09/22/2022 06:31:28 - INFO - pytorch_transformers.tokenization_utils -   loading file ./models/sp_model/vocab.json
09/22/2022 06:31:28 - INFO - pytorch_transformers.tokenization_utils -   loading file ./models/sp_model/merges.txt
09/22/2022 06:31:28 - INFO - pytorch_transformers.tokenization_utils -   loading file ./models/sp_model/added_tokens.json
09/22/2022 06:31:28 - INFO - pytorch_transformers.tokenization_utils -   loading file ./models/sp_model/special_tokens_map.json
09/22/2022 06:31:28 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./models/sp_model/pytorch_model.bin
09/22/2022 06:31:41 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in RobertaForEX: ['classifier_stance.dense.weight', 'classifier_stance.dense.bias', 'classifier_stance.out_proj.weight', 'classifier_stance.out_proj.bias']
09/22/2022 06:31:41 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, cache_dir='./tmp', config_name='', data_cache_dir='./tmp', data_dir='./data', device=device(type='cuda'), do_eval=True, do_eval_edge=True, do_lower_case=True, do_prediction=False, do_train=False, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=1e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_nodes=20, max_seq_length=300, max_steps=-1, model_name_or_path='./models/sp_model', model_type='roberta_eg', n_gpu=1, no_cuda=False, num_train_epochs=10.0, output_dir='./models/sp_model', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, run_on_test=False, save_steps=50, seed=42, server_ip='', server_port='', task_name='eg', tokenizer_name='', warmup_pct=None, warmup_steps=0, weight_decay=0.1)
09/22/2022 06:31:41 - INFO - __main__ -   Evaluate the following checkpoints: ['./models/sp_model']
09/22/2022 06:31:41 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ./models/sp_model/config.json
09/22/2022 06:31:41 - INFO - pytorch_transformers.modeling_utils -   Model config {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "finetuning_task": "eg",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pad_token_id": 1,
  "torchscript": false,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

09/22/2022 06:31:41 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./models/sp_model/pytorch_model.bin
09/22/2022 06:31:51 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in RobertaForEX: ['classifier_stance.dense.weight', 'classifier_stance.dense.bias', 'classifier_stance.out_proj.weight', 'classifier_stance.out_proj.bias']
09/22/2022 06:31:51 - INFO - __main__ -   Creating features from dataset file at ./data
Traceback (most recent call last):
  File "./structured_model/run_joint_model.py", line 744, in <module>
    main()
  File "./structured_model/run_joint_model.py", line 721, in main
    result = evaluate(args, model, tokenizer, processor,
  File "./structured_model/run_joint_model.py", line 264, in evaluate
    eval_dataset, examples = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True,
  File "./structured_model/run_joint_model.py", line 462, in load_and_cache_examples
    examples = processor.get_dev_examples(
  File "/cluster/raid/home/zhivar.sourati/logical-fallacy-identification/AMR_parsers_and_graphs/ExplaGraphs/structured_model/utils_joint_model.py", line 65, in get_dev_examples
    open(os.path.join(data_dir, "internal_nodes_dev.txt"),
FileNotFoundError: [Errno 2] No such file or directory: './data/internal_nodes_dev.txt'
mv: cannot stat './models/sp_model/prediction_edges_dev.lst': No such file or directory
