/cluster/raid/home/zhivar.sourati/logical-fallacy-identification/AMR_parsers_and_graphs
Tue Sep 27 02:31:40 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  A100-PCIE-40GB      Off  | 00000000:27:00.0 Off |                    0 |
| N/A   39C    P0    35W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
0
cuda
using GPU
augmenting train dataset
                        precision    recall  f1-score   support

            ad hominem       0.99      0.99      0.99       225
            ad populum       0.96      0.99      0.98       158
     appeal to emotion       1.00      0.96      0.98       130
    circular reasoning       0.99      0.99      0.99       134
          equivocation       0.90      0.92      0.91        39
fallacy of credibility       0.98      0.99      0.99       107
  fallacy of extension       1.00      0.97      0.99       106
      fallacy of logic       0.95      0.99      0.97       121
  fallacy of relevance       1.00      0.99      1.00       114
       false causality       1.00      0.95      0.98       174
         false dilemma       0.99      1.00      1.00       110
 faulty generalization       0.98      1.00      0.99       319
           intentional       0.99      0.97      0.98       112

              accuracy                           0.98      1849
             macro avg       0.98      0.98      0.98      1849
          weighted avg       0.98      0.98      0.98      1849

augmenting dev dataset
Error finding the predicted label for sentence: " There MSK0 was in the MSK1 of MSK2 , and yet a perfect stranger ; without home and without friends , in the MSK1 of thousands of MSK0 own brethren â€” children of a common Father , and yet MSK0 dared not to unfold to any one of MSK2 MSK0 sad condition . " Check each literary term found in this quote : 
                        precision    recall  f1-score   support

            ad hominem       0.49      0.47      0.48        36
            ad populum       0.61      0.52      0.56        44
     appeal to emotion       0.25      0.15      0.19        13
    circular reasoning       0.48      0.56      0.51        18
          equivocation       0.33      0.60      0.43         5
fallacy of credibility       0.06      0.12      0.08         8
  fallacy of extension       0.60      0.21      0.32        14
      fallacy of logic       0.43      0.35      0.39        17
  fallacy of relevance       0.25      0.17      0.20        24
       false causality       0.50      0.42      0.45        24
         false dilemma       0.68      0.68      0.68        19
 faulty generalization       0.43      0.61      0.50        61
           intentional       0.42      0.31      0.36        16

              accuracy                           0.45       299
             macro avg       0.42      0.40      0.40       299
          weighted avg       0.46      0.45      0.44       299

augmenting test dataset
Error finding the predicted label for sentence: MSK0 call MSK1 yet MSK0 can not lend MSK2 money for MSK2 school project , murdering MSK2 chance for a higher grade . MSK0 are just like MSK1 . 
                        precision    recall  f1-score   support

            ad hominem       0.47      0.51      0.49        41
            ad populum       0.64      0.47      0.54        30
     appeal to emotion       0.54      0.30      0.39        23
    circular reasoning       0.19      0.32      0.24        19
          equivocation       0.00      0.00      0.00         5
fallacy of credibility       0.27      0.35      0.31        17
  fallacy of extension       0.80      0.38      0.52        21
      fallacy of logic       0.18      0.21      0.19        14
  fallacy of relevance       0.33      0.30      0.32        23
       false causality       0.40      0.11      0.17        18
         false dilemma       0.41      0.92      0.56        12
 faulty generalization       0.52      0.59      0.55        61
           intentional       0.50      0.47      0.48        15

              accuracy                           0.43       299
             macro avg       0.40      0.38      0.37       299
          weighted avg       0.46      0.43      0.42       299

Model loaded!
Start the training ...
{'loss': 2.6181, 'learning_rate': 4.461206896551724e-05, 'epoch': 0.43}
{'eval_loss': 2.497129440307617, 'eval_accuracy': 0.12040133779264214, 'eval_f1': 0.025877302450955927, 'eval_precision': 0.014496482142257915, 'eval_recall': 0.12040133779264214, 'eval_runtime': 1.1125, 'eval_samples_per_second': 268.772, 'eval_steps_per_second': 17.079, 'epoch': 0.43}
{'loss': 2.6097, 'learning_rate': 3.922413793103448e-05, 'epoch': 0.86}
{'eval_loss': 2.486952066421509, 'eval_accuracy': 0.12040133779264214, 'eval_f1': 0.025877302450955927, 'eval_precision': 0.014496482142257915, 'eval_recall': 0.12040133779264214, 'eval_runtime': 1.1123, 'eval_samples_per_second': 268.804, 'eval_steps_per_second': 17.081, 'epoch': 0.86}
{'loss': 2.5577, 'learning_rate': 3.383620689655172e-05, 'epoch': 1.29}
{'eval_loss': 2.4319586753845215, 'eval_accuracy': 0.2040133779264214, 'eval_f1': 0.06913786696395391, 'eval_precision': 0.04162145837294885, 'eval_recall': 0.2040133779264214, 'eval_runtime': 1.1174, 'eval_samples_per_second': 267.576, 'eval_steps_per_second': 17.003, 'epoch': 1.29}
{'loss': 2.5767, 'learning_rate': 2.844827586206897e-05, 'epoch': 1.72}
{'eval_loss': 2.4423668384552, 'eval_accuracy': 0.2040133779264214, 'eval_f1': 0.06913786696395391, 'eval_precision': 0.04162145837294885, 'eval_recall': 0.2040133779264214, 'eval_runtime': 1.1138, 'eval_samples_per_second': 268.454, 'eval_steps_per_second': 17.059, 'epoch': 1.72}
{'loss': 2.5925, 'learning_rate': 2.306034482758621e-05, 'epoch': 2.16}
{'eval_loss': 2.471160888671875, 'eval_accuracy': 0.2040133779264214, 'eval_f1': 0.06913786696395391, 'eval_precision': 0.04162145837294885, 'eval_recall': 0.2040133779264214, 'eval_runtime': 1.1187, 'eval_samples_per_second': 267.273, 'eval_steps_per_second': 16.984, 'epoch': 2.16}
{'loss': 2.5771, 'learning_rate': 1.767241379310345e-05, 'epoch': 2.59}
{'eval_loss': 2.4364404678344727, 'eval_accuracy': 0.2040133779264214, 'eval_f1': 0.06913786696395391, 'eval_precision': 0.04162145837294885, 'eval_recall': 0.2040133779264214, 'eval_runtime': 1.1187, 'eval_samples_per_second': 267.286, 'eval_steps_per_second': 16.985, 'epoch': 2.59}
{'loss': 2.5897, 'learning_rate': 1.228448275862069e-05, 'epoch': 3.02}
{'eval_loss': 2.4235341548919678, 'eval_accuracy': 0.2040133779264214, 'eval_f1': 0.06913786696395391, 'eval_precision': 0.04162145837294885, 'eval_recall': 0.2040133779264214, 'eval_runtime': 1.1269, 'eval_samples_per_second': 265.33, 'eval_steps_per_second': 16.86, 'epoch': 3.02}
{'loss': 2.5574, 'learning_rate': 6.896551724137932e-06, 'epoch': 3.45}
{'eval_loss': 2.4141299724578857, 'eval_accuracy': 0.2040133779264214, 'eval_f1': 0.06913786696395391, 'eval_precision': 0.04162145837294885, 'eval_recall': 0.2040133779264214, 'eval_runtime': 1.1211, 'eval_samples_per_second': 266.712, 'eval_steps_per_second': 16.948, 'epoch': 3.45}
{'loss': 2.5584, 'learning_rate': 1.5086206896551726e-06, 'epoch': 3.88}
{'eval_loss': 2.424743413925171, 'eval_accuracy': 0.2040133779264214, 'eval_f1': 0.06913786696395391, 'eval_precision': 0.04162145837294885, 'eval_recall': 0.2040133779264214, 'eval_runtime': 1.1292, 'eval_samples_per_second': 264.791, 'eval_steps_per_second': 16.826, 'epoch': 3.88}
{'train_runtime': 88.7761, 'train_samples_per_second': 83.311, 'train_steps_per_second': 5.227, 'train_loss': 2.581474542617798, 'epoch': 4.0}
PredictionOutput(predictions=array([[ 0.26313236,  0.16102052, -0.05764421, ..., -0.2926424 ,
         0.8544537 , -0.29479176],
       [ 0.26337582,  0.1609402 , -0.05763396, ..., -0.29263157,
         0.85455084, -0.2947279 ],
       [ 0.2633396 ,  0.16089252, -0.05764997, ..., -0.29256654,
         0.8546543 , -0.29465836],
       ...,
       [ 0.2630903 ,  0.16095151, -0.05706668, ..., -0.29243004,
         0.85459304, -0.29498458],
       [ 0.26321438,  0.16091046, -0.05757603, ..., -0.29287168,
         0.8542789 , -0.2948606 ],
       [ 0.26328903,  0.16091698, -0.05765465, ..., -0.29236937,
         0.8546116 , -0.29465657]], dtype=float32), label_ids=array([ 6, 11,  7,  9, 11,  6,  6, 11,  5,  3,  9, 11,  0,  1, 11, 12,  1,
        7,  0, 12,  3,  1,  8,  3,  6,  8,  0,  0, 11,  0, 11,  3,  3,  5,
        2,  8,  9,  1, 11,  1,  5, 12,  0, 12,  1,  2, 12,  0,  5,  7,  3,
       11,  2,  4,  8,  3, 12, 11, 11, 10,  7, 11,  0, 10, 12,  0,  1,  8,
       12,  5, 10,  5,  1,  8,  0, 12,  1,  7, 10, 12,  7,  7,  2,  0,  2,
       10, 11,  0,  6,  0, 10,  9,  5,  5,  0,  9,  1,  2,  9,  0,  2,  0,
        7, 11,  9,  6,  6,  9,  3, 12,  7, 11, 11, 11, 10,  2,  8,  9,  0,
        7,  3, 11, 11, 12,  8,  0,  4, 11,  5,  8, 11,  1,  5, 11,  7, 11,
        0, 11,  0, 11,  6,  0, 11, 11,  8, 11,  1,  8,  6,  5,  0, 11,  9,
        0,  1,  5,  0,  1,  8,  8,  0,  2, 11, 11,  0,  6,  5,  6, 11,  2,
        2,  7,  9, 11,  1, 11,  1,  8,  9,  8,  6,  3,  8, 10,  1,  2,  0,
       11, 11,  3,  8,  6,  2,  4,  1, 11, 11, 11, 11,  0, 11,  2,  1,  8,
        1,  9,  0,  1, 11,  1,  5,  1,  6,  5,  0,  8,  0,  1, 10, 11,  9,
        0, 11, 12,  2,  4,  0, 11,  2, 12,  2, 11,  7,  3,  2,  2,  8,  3,
        0,  2, 11,  8,  8,  9,  3, 11,  6, 11,  0,  3,  2,  8, 10, 12, 11,
        5, 10,  6,  9, 11,  6,  3,  1,  6,  3,  7, 11,  6,  2, 11,  4, 10,
        9,  2,  0, 11,  5,  1, 11,  6, 11,  9, 11,  0,  0,  0, 11, 11,  0,
        1, 11,  1,  0,  3,  1,  6,  1, 11,  3]), metrics={'test_loss': 2.4363045692443848, 'test_accuracy': 0.2040133779264214, 'test_f1': 0.06913786696395391, 'test_precision': 0.04162145837294885, 'test_recall': 0.2040133779264214, 'test_runtime': 1.1898, 'test_samples_per_second': 251.295, 'test_steps_per_second': 15.969})
cuda
using GPU
augmenting train dataset
                        precision    recall  f1-score   support

            ad hominem       0.99      0.99      0.99       225
            ad populum       0.96      0.99      0.98       158
     appeal to emotion       1.00      0.96      0.98       130
    circular reasoning       0.99      0.99      0.99       134
          equivocation       0.90      0.92      0.91        39
fallacy of credibility       0.98      0.99      0.99       107
  fallacy of extension       1.00      0.97      0.99       106
      fallacy of logic       0.95      0.99      0.97       121
  fallacy of relevance       1.00      0.99      1.00       114
       false causality       1.00      0.95      0.98       174
         false dilemma       0.99      1.00      1.00       110
 faulty generalization       0.98      1.00      0.99       319
           intentional       0.99      0.97      0.98       112

              accuracy                           0.98      1849
             macro avg       0.98      0.98      0.98      1849
          weighted avg       0.98      0.98      0.98      1849

augmenting dev dataset
Error finding the predicted label for sentence: " There MSK0 was in the MSK1 of MSK2 , and yet a perfect stranger ; without home and without friends , in the MSK1 of thousands of MSK0 own brethren â€” children of a common Father , and yet MSK0 dared not to unfold to any one of MSK2 MSK0 sad condition . " Check each literary term found in this quote : 
                        precision    recall  f1-score   support

            ad hominem       0.49      0.47      0.48        36
            ad populum       0.61      0.52      0.56        44
     appeal to emotion       0.25      0.15      0.19        13
    circular reasoning       0.48      0.56      0.51        18
          equivocation       0.33      0.60      0.43         5
fallacy of credibility       0.06      0.12      0.08         8
  fallacy of extension       0.60      0.21      0.32        14
      fallacy of logic       0.43      0.35      0.39        17
  fallacy of relevance       0.25      0.17      0.20        24
       false causality       0.50      0.42      0.45        24
         false dilemma       0.68      0.68      0.68        19
 faulty generalization       0.43      0.61      0.50        61
           intentional       0.42      0.31      0.36        16

              accuracy                           0.45       299
             macro avg       0.42      0.40      0.40       299
          weighted avg       0.46      0.45      0.44       299

augmenting test dataset
Error finding the predicted label for sentence: MSK0 call MSK1 yet MSK0 can not lend MSK2 money for MSK2 school project , murdering MSK2 chance for a higher grade . MSK0 are just like MSK1 . 
                        precision    recall  f1-score   support

            ad hominem       0.47      0.51      0.49        41
            ad populum       0.64      0.47      0.54        30
     appeal to emotion       0.54      0.30      0.39        23
    circular reasoning       0.19      0.32      0.24        19
          equivocation       0.00      0.00      0.00         5
fallacy of credibility       0.27      0.35      0.31        17
  fallacy of extension       0.80      0.38      0.52        21
      fallacy of logic       0.18      0.21      0.19        14
  fallacy of relevance       0.33      0.30      0.32        23
       false causality       0.40      0.11      0.17        18
         false dilemma       0.41      0.92      0.56        12
 faulty generalization       0.52      0.59      0.55        61
           intentional       0.50      0.47      0.48        15

              accuracy                           0.43       299
             macro avg       0.40      0.38      0.37       299
          weighted avg       0.46      0.43      0.42       299

Model loaded!
Start the training ...
