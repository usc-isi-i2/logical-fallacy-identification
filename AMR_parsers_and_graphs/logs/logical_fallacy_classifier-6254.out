/cluster/raid/home/zhivar.sourati/logical-fallacy-identification/AMR_parsers_and_graphs
Tue Sep 27 02:54:06 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  A100-PCIE-40GB      Off  | 00000000:28:00.0 Off |                    0 |
| N/A   29C    P0    36W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
0
cuda
using GPU
augmenting train dataset
                        precision    recall  f1-score   support

            ad hominem       0.99      0.99      0.99       225
            ad populum       0.96      0.99      0.98       158
     appeal to emotion       1.00      0.96      0.98       130
    circular reasoning       0.99      0.99      0.99       134
          equivocation       0.90      0.92      0.91        39
fallacy of credibility       0.98      0.99      0.99       107
  fallacy of extension       1.00      0.97      0.99       106
      fallacy of logic       0.95      0.99      0.97       121
  fallacy of relevance       1.00      0.99      1.00       114
       false causality       1.00      0.95      0.98       174
         false dilemma       0.99      1.00      1.00       110
 faulty generalization       0.98      1.00      0.99       319
           intentional       0.99      0.97      0.98       112

              accuracy                           0.98      1849
             macro avg       0.98      0.98      0.98      1849
          weighted avg       0.98      0.98      0.98      1849

Number of data points augmented with high confidence: 0.6652244456462953
augmenting dev dataset
Error finding the predicted label for sentence: " Just like students are given a couple of MSK0 of MSK3 before taking exams , MSK1 should also be given few days or MSK0 to MSK3 MSK1 before an operation or MSK2 , after all MSK2 is not as easy task " Is an example of .... 
Error finding the predicted label for sentence: You do n’t have to do this . MSK0 grandmother is in the hospital . MSK0 need MSK0 salary to support MSK1 medication . MSK1 ’s dying . 
Error finding the predicted label for sentence: I know MSK0 . MSK0 are all MSK1 . Therefore , Kentuckians are MSK1 . 
Error finding the predicted label for sentence: MSK1 : What are MSK2 ? Do you even know ? MSK3 of State : MSK4 're ... what MSK4 MSK5 to make MSK6 ! MSK1 : But why do MSK4 MSK5 MSK4 to make MSK6 ? MSK3 of Defense : [ raises hand after a pause ] Because MSK2 . 
Error finding the predicted label for sentence: This is a fallacy of irrelevance that is based solely on someone 's or MSK0 history , origin , or source rather than MSK0 current meaning 
Error finding the predicted label for sentence: MSK0 : MSK0 should not be MSK1 that ... it has been scientifically proven that MSK1 MSK3 are no good for MSK0 health . Hugh : MSK0 MSK1 MSK3 all the time so that ca n’t be true . 
Error finding the predicted label for sentence: MSK0 : MSK1 MSK0 MSK2 that MSK3 is morally MSK4 . MSK1 MSK5 : MSK1 Of course MSK5 would say that , MSK5 ’re a MSK6 . MSK1 MSK0 : MSK1 What about the arguments MSK0 gave to support MSK0 position ? MSK1 MSK5 : MSK1 Those do n’t count . Like MSK0 said , MSK5 ’re a MSK6 , so MSK5 have to say that MSK3 is MSK4 . Further , MSK5 are just a lackey to the Pope , so MSK0 ca n’t MSK2 what MSK5 say . MSK1 
Error finding the predicted label for sentence: " There MSK0 was in the MSK1 of MSK2 , and yet a perfect stranger ; without home and without friends , in the MSK1 of thousands of MSK0 own brethren — children of a common Father , and yet MSK0 dared not to unfold to any one of MSK2 MSK0 sad condition . " Check each literary term found in this quote : 
                        precision    recall  f1-score   support

            ad hominem       0.49      0.47      0.48        36
            ad populum       0.61      0.52      0.56        44
     appeal to emotion       0.25      0.15      0.19        13
    circular reasoning       0.48      0.56      0.51        18
          equivocation       0.33      0.60      0.43         5
fallacy of credibility       0.06      0.12      0.08         8
  fallacy of extension       0.60      0.21      0.32        14
      fallacy of logic       0.43      0.35      0.39        17
  fallacy of relevance       0.25      0.17      0.20        24
       false causality       0.50      0.42      0.45        24
         false dilemma       0.68      0.68      0.68        19
 faulty generalization       0.43      0.61      0.50        61
           intentional       0.42      0.31      0.36        16

              accuracy                           0.45       299
             macro avg       0.42      0.40      0.40       299
          weighted avg       0.46      0.45      0.44       299

Number of data points augmented with high confidence: 0.31666666666666665
augmenting test dataset
Error finding the predicted label for sentence: People who drive big cars probably hate the environment . 
Error finding the predicted label for sentence: MSK0 ca n't jump . No , really , MSK0 ca n't ! 
Error finding the predicted label for sentence: MSK0 call MSK1 yet MSK0 can not lend MSK2 money for MSK2 school project , murdering MSK2 chance for a higher grade . MSK0 are just like MSK1 . 
                        precision    recall  f1-score   support

            ad hominem       0.47      0.51      0.49        41
            ad populum       0.64      0.47      0.54        30
     appeal to emotion       0.54      0.30      0.39        23
    circular reasoning       0.19      0.32      0.24        19
          equivocation       0.00      0.00      0.00         5
fallacy of credibility       0.27      0.35      0.31        17
  fallacy of extension       0.80      0.38      0.52        21
      fallacy of logic       0.18      0.21      0.19        14
  fallacy of relevance       0.33      0.30      0.32        23
       false causality       0.40      0.11      0.17        18
         false dilemma       0.41      0.92      0.56        12
 faulty generalization       0.52      0.59      0.55        61
           intentional       0.50      0.47      0.48        15

              accuracy                           0.43       299
             macro avg       0.40      0.38      0.37       299
          weighted avg       0.46      0.43      0.42       299

Number of data points augmented with high confidence: 0.28
Model loaded!
Start the training ...
{'loss': 2.546, 'learning_rate': 4.461206896551724e-05, 'epoch': 0.43}
{'eval_loss': 2.432830333709717, 'eval_accuracy': 0.2054794520547945, 'eval_f1': 0.07004981320049813, 'eval_precision': 0.0422218052167386, 'eval_recall': 0.2054794520547945, 'eval_runtime': 0.9089, 'eval_samples_per_second': 321.254, 'eval_steps_per_second': 20.903, 'epoch': 0.43}
{'loss': 2.5022, 'learning_rate': 3.922413793103448e-05, 'epoch': 0.86}
{'eval_loss': 2.45491623878479, 'eval_accuracy': 0.2054794520547945, 'eval_f1': 0.07004981320049813, 'eval_precision': 0.0422218052167386, 'eval_recall': 0.2054794520547945, 'eval_runtime': 0.9057, 'eval_samples_per_second': 322.416, 'eval_steps_per_second': 20.979, 'epoch': 0.86}
{'loss': 2.4725, 'learning_rate': 3.383620689655172e-05, 'epoch': 1.29}
{'eval_loss': 2.4125611782073975, 'eval_accuracy': 0.2054794520547945, 'eval_f1': 0.07004981320049813, 'eval_precision': 0.0422218052167386, 'eval_recall': 0.2054794520547945, 'eval_runtime': 0.9114, 'eval_samples_per_second': 320.397, 'eval_steps_per_second': 20.848, 'epoch': 1.29}
{'loss': 2.5162, 'learning_rate': 2.844827586206897e-05, 'epoch': 1.72}
{'eval_loss': 2.43021559715271, 'eval_accuracy': 0.2054794520547945, 'eval_f1': 0.07004981320049813, 'eval_precision': 0.0422218052167386, 'eval_recall': 0.2054794520547945, 'eval_runtime': 0.914, 'eval_samples_per_second': 319.491, 'eval_steps_per_second': 20.789, 'epoch': 1.72}
{'loss': 2.5056, 'learning_rate': 2.306034482758621e-05, 'epoch': 2.16}
{'eval_loss': 2.444608449935913, 'eval_accuracy': 0.2054794520547945, 'eval_f1': 0.07004981320049813, 'eval_precision': 0.0422218052167386, 'eval_recall': 0.2054794520547945, 'eval_runtime': 0.9185, 'eval_samples_per_second': 317.913, 'eval_steps_per_second': 20.686, 'epoch': 2.16}
{'loss': 2.4813, 'learning_rate': 1.767241379310345e-05, 'epoch': 2.59}
{'eval_loss': 2.4155306816101074, 'eval_accuracy': 0.2054794520547945, 'eval_f1': 0.07004981320049813, 'eval_precision': 0.0422218052167386, 'eval_recall': 0.2054794520547945, 'eval_runtime': 0.9214, 'eval_samples_per_second': 316.895, 'eval_steps_per_second': 20.62, 'epoch': 2.59}
{'loss': 2.4906, 'learning_rate': 1.228448275862069e-05, 'epoch': 3.02}
{'eval_loss': 2.422720432281494, 'eval_accuracy': 0.2054794520547945, 'eval_f1': 0.07004981320049813, 'eval_precision': 0.0422218052167386, 'eval_recall': 0.2054794520547945, 'eval_runtime': 0.9245, 'eval_samples_per_second': 315.843, 'eval_steps_per_second': 20.551, 'epoch': 3.02}
{'loss': 2.4835, 'learning_rate': 6.896551724137932e-06, 'epoch': 3.45}
{'eval_loss': 2.4168028831481934, 'eval_accuracy': 0.2054794520547945, 'eval_f1': 0.07004981320049813, 'eval_precision': 0.0422218052167386, 'eval_recall': 0.2054794520547945, 'eval_runtime': 0.9282, 'eval_samples_per_second': 314.597, 'eval_steps_per_second': 20.47, 'epoch': 3.45}
{'loss': 2.4892, 'learning_rate': 1.5086206896551726e-06, 'epoch': 3.88}
{'eval_loss': 2.4215149879455566, 'eval_accuracy': 0.2054794520547945, 'eval_f1': 0.07004981320049813, 'eval_precision': 0.0422218052167386, 'eval_recall': 0.2054794520547945, 'eval_runtime': 0.9289, 'eval_samples_per_second': 314.353, 'eval_steps_per_second': 20.454, 'epoch': 3.88}
{'train_runtime': 84.3897, 'train_samples_per_second': 87.641, 'train_steps_per_second': 5.498, 'train_loss': 2.4975755625757676, 'epoch': 4.0}
PredictionOutput(predictions=array([[ 0.49266818,  0.1755629 , -0.14295153, ..., -0.20038736,
         0.805335  , -0.1766363 ],
       [ 0.4926914 ,  0.17546867, -0.14286976, ..., -0.20040013,
         0.8052892 , -0.17661501],
       [ 0.4926566 ,  0.17553414, -0.14285877, ..., -0.2003736 ,
         0.8053412 , -0.1766157 ],
       ...,
       [ 0.4926033 ,  0.17547926, -0.14293917, ..., -0.20036463,
         0.805295  , -0.17665382],
       [ 0.4926101 ,  0.17552778, -0.14299123, ..., -0.20033123,
         0.80522627, -0.17664744],
       [ 0.49263912,  0.17545666, -0.14292888, ..., -0.200327  ,
         0.8052676 , -0.17668234]], dtype=float32), label_ids=array([ 7,  9, 11,  6,  6, 11,  5,  3,  9, 11,  0,  1, 11, 12,  1,  7,  0,
       12,  3,  1,  8,  3,  6,  8,  0,  0, 11,  0, 11,  3,  3,  5,  2,  8,
        9,  1, 11,  1,  5, 12,  0, 12,  1,  2, 12,  0,  5,  7,  3, 11,  2,
        4,  8,  3, 12, 11, 11, 10,  7, 11,  0, 10, 12,  0,  1,  8, 12,  5,
       10,  5,  1,  8,  0, 12,  1,  7, 10, 12,  7,  7,  2,  0,  2, 10, 11,
        0,  6,  0, 10,  9,  5,  5,  0,  9,  1,  2,  9,  0,  2,  0,  7, 11,
        9,  6,  6,  9,  3, 12,  7, 11, 11, 11, 10,  2,  8,  9,  0,  7,  3,
       11, 11, 12,  8,  0,  4, 11,  5,  8, 11,  1,  5, 11,  7, 11,  0, 11,
        0, 11,  6,  0, 11, 11,  8, 11,  1,  8,  6,  5,  0, 11,  9,  0,  1,
        5,  0,  1,  8,  8,  0,  2, 11, 11,  0,  6,  5,  6, 11,  2,  2,  7,
        9, 11,  1, 11,  1,  8,  9,  8,  6,  3,  8, 10,  1,  2,  0, 11, 11,
        3,  8,  6,  2,  4,  1, 11, 11, 11, 11,  0, 11,  2,  1,  8,  1,  9,
        0,  1, 11,  1,  5,  1,  6,  5,  0,  8,  0,  1, 10, 11,  9,  0, 11,
       12,  2,  4,  0, 11,  2, 12,  2, 11,  7,  3,  2,  2,  8,  3,  0,  2,
       11,  8,  8,  9,  3, 11,  6, 11,  0,  3,  2,  8, 10, 12, 11,  5, 10,
        6,  9, 11,  6,  3,  1,  6,  3,  7, 11,  6,  2, 11,  4, 10,  9,  2,
        0, 11,  5,  1, 11,  6, 11,  9, 11,  0,  0,  0, 11, 11,  0,  1, 11,
        1,  0,  3,  1,  6,  1, 11,  3]), metrics={'test_loss': 2.436230421066284, 'test_accuracy': 0.20202020202020202, 'test_f1': 0.06790595025889144, 'test_precision': 0.040812162024283234, 'test_recall': 0.20202020202020202, 'test_runtime': 0.9906, 'test_samples_per_second': 299.811, 'test_steps_per_second': 19.18})
cuda
using GPU
augmenting train dataset
                        precision    recall  f1-score   support

            ad hominem       0.99      0.99      0.99       225
            ad populum       0.96      0.99      0.98       158
     appeal to emotion       1.00      0.96      0.98       130
    circular reasoning       0.99      0.99      0.99       134
          equivocation       0.90      0.92      0.91        39
fallacy of credibility       0.98      0.99      0.99       107
  fallacy of extension       1.00      0.97      0.99       106
      fallacy of logic       0.95      0.99      0.97       121
  fallacy of relevance       1.00      0.99      1.00       114
       false causality       1.00      0.95      0.98       174
         false dilemma       0.99      1.00      1.00       110
 faulty generalization       0.98      1.00      0.99       319
           intentional       0.99      0.97      0.98       112

              accuracy                           0.98      1849
             macro avg       0.98      0.98      0.98      1849
          weighted avg       0.98      0.98      0.98      1849

Number of data points augmented with high confidence: 0.6652244456462953
augmenting dev dataset
Error finding the predicted label for sentence: " Just like students are given a couple of MSK0 of MSK3 before taking exams , MSK1 should also be given few days or MSK0 to MSK3 MSK1 before an operation or MSK2 , after all MSK2 is not as easy task " Is an example of .... 
Error finding the predicted label for sentence: You do n’t have to do this . MSK0 grandmother is in the hospital . MSK0 need MSK0 salary to support MSK1 medication . MSK1 ’s dying . 
Error finding the predicted label for sentence: I know MSK0 . MSK0 are all MSK1 . Therefore , Kentuckians are MSK1 . 
Error finding the predicted label for sentence: MSK1 : What are MSK2 ? Do you even know ? MSK3 of State : MSK4 're ... what MSK4 MSK5 to make MSK6 ! MSK1 : But why do MSK4 MSK5 MSK4 to make MSK6 ? MSK3 of Defense : [ raises hand after a pause ] Because MSK2 . 
Error finding the predicted label for sentence: This is a fallacy of irrelevance that is based solely on someone 's or MSK0 history , origin , or source rather than MSK0 current meaning 
Error finding the predicted label for sentence: MSK0 : MSK0 should not be MSK1 that ... it has been scientifically proven that MSK1 MSK3 are no good for MSK0 health . Hugh : MSK0 MSK1 MSK3 all the time so that ca n’t be true . 
Error finding the predicted label for sentence: MSK0 : MSK1 MSK0 MSK2 that MSK3 is morally MSK4 . MSK1 MSK5 : MSK1 Of course MSK5 would say that , MSK5 ’re a MSK6 . MSK1 MSK0 : MSK1 What about the arguments MSK0 gave to support MSK0 position ? MSK1 MSK5 : MSK1 Those do n’t count . Like MSK0 said , MSK5 ’re a MSK6 , so MSK5 have to say that MSK3 is MSK4 . Further , MSK5 are just a lackey to the Pope , so MSK0 ca n’t MSK2 what MSK5 say . MSK1 
Error finding the predicted label for sentence: " There MSK0 was in the MSK1 of MSK2 , and yet a perfect stranger ; without home and without friends , in the MSK1 of thousands of MSK0 own brethren — children of a common Father , and yet MSK0 dared not to unfold to any one of MSK2 MSK0 sad condition . " Check each literary term found in this quote : 
                        precision    recall  f1-score   support

            ad hominem       0.49      0.47      0.48        36
            ad populum       0.61      0.52      0.56        44
     appeal to emotion       0.25      0.15      0.19        13
    circular reasoning       0.48      0.56      0.51        18
          equivocation       0.33      0.60      0.43         5
fallacy of credibility       0.06      0.12      0.08         8
  fallacy of extension       0.60      0.21      0.32        14
      fallacy of logic       0.43      0.35      0.39        17
  fallacy of relevance       0.25      0.17      0.20        24
       false causality       0.50      0.42      0.45        24
         false dilemma       0.68      0.68      0.68        19
 faulty generalization       0.43      0.61      0.50        61
           intentional       0.42      0.31      0.36        16

              accuracy                           0.45       299
             macro avg       0.42      0.40      0.40       299
          weighted avg       0.46      0.45      0.44       299

Number of data points augmented with high confidence: 0.31666666666666665
augmenting test dataset
Error finding the predicted label for sentence: People who drive big cars probably hate the environment . 
Error finding the predicted label for sentence: MSK0 ca n't jump . No , really , MSK0 ca n't ! 
Error finding the predicted label for sentence: MSK0 call MSK1 yet MSK0 can not lend MSK2 money for MSK2 school project , murdering MSK2 chance for a higher grade . MSK0 are just like MSK1 . 
                        precision    recall  f1-score   support

            ad hominem       0.47      0.51      0.49        41
            ad populum       0.64      0.47      0.54        30
     appeal to emotion       0.54      0.30      0.39        23
    circular reasoning       0.19      0.32      0.24        19
          equivocation       0.00      0.00      0.00         5
fallacy of credibility       0.27      0.35      0.31        17
  fallacy of extension       0.80      0.38      0.52        21
      fallacy of logic       0.18      0.21      0.19        14
  fallacy of relevance       0.33      0.30      0.32        23
       false causality       0.40      0.11      0.17        18
         false dilemma       0.41      0.92      0.56        12
 faulty generalization       0.52      0.59      0.55        61
           intentional       0.50      0.47      0.48        15

              accuracy                           0.43       299
             macro avg       0.40      0.38      0.37       299
          weighted avg       0.46      0.43      0.42       299

Number of data points augmented with high confidence: 0.28
Model loaded!
Start the training ...
{'loss': 2.5179, 'learning_rate': 4.461206896551724e-05, 'epoch': 0.43}
{'eval_loss': 2.4274933338165283, 'eval_accuracy': 0.2054794520547945, 'eval_f1': 0.07004981320049813, 'eval_precision': 0.0422218052167386, 'eval_recall': 0.2054794520547945, 'eval_runtime': 1.8342, 'eval_samples_per_second': 159.198, 'eval_steps_per_second': 10.359, 'epoch': 0.43}
{'loss': 2.4997, 'learning_rate': 3.922413793103448e-05, 'epoch': 0.86}
{'eval_loss': 2.4385459423065186, 'eval_accuracy': 0.2054794520547945, 'eval_f1': 0.07004981320049813, 'eval_precision': 0.0422218052167386, 'eval_recall': 0.2054794520547945, 'eval_runtime': 1.8404, 'eval_samples_per_second': 158.661, 'eval_steps_per_second': 10.324, 'epoch': 0.86}
{'loss': 2.4669, 'learning_rate': 3.383620689655172e-05, 'epoch': 1.29}
{'eval_loss': 2.412076234817505, 'eval_accuracy': 0.2054794520547945, 'eval_f1': 0.07004981320049813, 'eval_precision': 0.0422218052167386, 'eval_recall': 0.2054794520547945, 'eval_runtime': 1.8426, 'eval_samples_per_second': 158.475, 'eval_steps_per_second': 10.312, 'epoch': 1.29}
{'loss': 2.5011, 'learning_rate': 2.844827586206897e-05, 'epoch': 1.72}
