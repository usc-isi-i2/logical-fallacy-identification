/cluster/raid/home/zhivar.sourati/logical-fallacy-identification/AMR_parsers_and_graphs
Mon Sep 26 20:27:17 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  A100-PCIE-40GB      Off  | 00000000:27:00.0 Off |                    0 |
| N/A   27C    P0    33W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
0
cuda
using GPU
0.14009396410431604
saving the model
Epoch: 001, Train Acc: 0.1823, Dev Acc: 0.1973
0.2028582760231733
saving the model
Epoch: 002, Train Acc: 0.2499, Dev Acc: 0.2843
0.2533107258073099
saving the model
Epoch: 003, Train Acc: 0.2904, Dev Acc: 0.2843
0.28241927007828393
saving the model
Epoch: 004, Train Acc: 0.3050, Dev Acc: 0.3244
0.2882037000208908
saving the model
Epoch: 005, Train Acc: 0.3369, Dev Acc: 0.3311
0.31170746068008415
saving the model
Epoch: 006, Train Acc: 0.3732, Dev Acc: 0.3478
0.32377878131713755
saving the model
Epoch: 007, Train Acc: 0.4348, Dev Acc: 0.3445
0.33587382764207735
saving the model
Epoch: 008, Train Acc: 0.4473, Dev Acc: 0.3478
0.3582198679609648
saving the model
Epoch: 009, Train Acc: 0.5057, Dev Acc: 0.3712
0.3688887662076456
saving the model
Epoch: 010, Train Acc: 0.5673, Dev Acc: 0.3846
0.38053490142881685
saving the model
Epoch: 011, Train Acc: 0.6074, Dev Acc: 0.3946
Epoch: 012, Train Acc: 0.6376, Dev Acc: 0.3913
0.3853096503659469
saving the model
Epoch: 013, Train Acc: 0.6398, Dev Acc: 0.4080
Epoch: 014, Train Acc: 0.6728, Dev Acc: 0.3712
0.394787500262794
saving the model
Epoch: 015, Train Acc: 0.7258, Dev Acc: 0.4080
Epoch: 016, Train Acc: 0.7074, Dev Acc: 0.4114
0.4062039282157762
saving the model
Epoch: 017, Train Acc: 0.7804, Dev Acc: 0.4013
0.4215758302053013
saving the model
Epoch: 018, Train Acc: 0.7945, Dev Acc: 0.4348
0.43264980950193305
saving the model
Epoch: 019, Train Acc: 0.8021, Dev Acc: 0.4415
Epoch: 020, Train Acc: 0.8405, Dev Acc: 0.3980
Epoch: 021, Train Acc: 0.8626, Dev Acc: 0.4314
0.43730109532332456
saving the model
Epoch: 022, Train Acc: 0.8534, Dev Acc: 0.4314
0.4450060540825957
saving the model
Epoch: 023, Train Acc: 0.8615, Dev Acc: 0.4548
Epoch: 024, Train Acc: 0.8902, Dev Acc: 0.4381
Epoch: 025, Train Acc: 0.9178, Dev Acc: 0.4314
Epoch: 026, Train Acc: 0.8902, Dev Acc: 0.4348
0.4473318301913619
saving the model
Epoch: 027, Train Acc: 0.9373, Dev Acc: 0.4482
Epoch: 028, Train Acc: 0.9519, Dev Acc: 0.4415
Epoch: 029, Train Acc: 0.9373, Dev Acc: 0.4314
Epoch: 030, Train Acc: 0.9454, Dev Acc: 0.4448
Epoch: 031, Train Acc: 0.9448, Dev Acc: 0.4314
Epoch: 032, Train Acc: 0.9567, Dev Acc: 0.4448
Epoch: 033, Train Acc: 0.9638, Dev Acc: 0.4348
Epoch: 034, Train Acc: 0.9600, Dev Acc: 0.4314
Epoch: 035, Train Acc: 0.9757, Dev Acc: 0.4348
Epoch: 036, Train Acc: 0.9730, Dev Acc: 0.4448
Epoch: 037, Train Acc: 0.9811, Dev Acc: 0.4415
Epoch: 038, Train Acc: 0.9767, Dev Acc: 0.4548
Epoch: 039, Train Acc: 0.9827, Dev Acc: 0.4415
Train split results
                        precision    recall  f1-score   support

            ad hominem       1.00      0.97      0.98       225
            ad populum       1.00      0.96      0.98       158
     appeal to emotion       1.00      0.98      0.99       130
    circular reasoning       0.98      0.99      0.99       134
          equivocation       1.00      0.92      0.96        39
fallacy of credibility       0.99      0.99      0.99       107
  fallacy of extension       1.00      0.99      1.00       106
      fallacy of logic       0.95      0.99      0.97       121
  fallacy of relevance       1.00      0.96      0.98       114
       false causality       0.98      0.99      0.99       174
         false dilemma       0.90      1.00      0.95       110
 faulty generalization       0.99      0.99      0.99       319
           intentional       0.98      0.98      0.98       112

              accuracy                           0.98      1849
             macro avg       0.98      0.98      0.98      1849
          weighted avg       0.98      0.98      0.98      1849

Dev split results
                        precision    recall  f1-score   support

            ad hominem       0.52      0.42      0.46        36
            ad populum       0.68      0.43      0.53        44
     appeal to emotion       0.42      0.38      0.40        13
    circular reasoning       0.38      0.56      0.45        18
          equivocation       0.43      0.60      0.50         5
fallacy of credibility       0.09      0.25      0.13         8
  fallacy of extension       0.50      0.21      0.30        14
      fallacy of logic       0.30      0.35      0.32        17
  fallacy of relevance       0.50      0.25      0.33        24
       false causality       0.44      0.33      0.38        24
         false dilemma       0.52      0.74      0.61        19
 faulty generalization       0.46      0.61      0.52        61
           intentional       0.36      0.25      0.30        16

              accuracy                           0.44       299
             macro avg       0.43      0.41      0.40       299
          weighted avg       0.47      0.44      0.44       299

Test split results
                        precision    recall  f1-score   support

            ad hominem       0.63      0.54      0.58        41
            ad populum       0.75      0.50      0.60        30
     appeal to emotion       0.35      0.30      0.33        23
    circular reasoning       0.21      0.47      0.29        19
          equivocation       0.50      0.20      0.29         5
fallacy of credibility       0.16      0.18      0.17        17
  fallacy of extension       0.43      0.29      0.34        21
      fallacy of logic       0.27      0.29      0.28        14
  fallacy of relevance       0.46      0.26      0.33        23
       false causality       0.53      0.44      0.48        18
         false dilemma       0.41      0.75      0.53        12
 faulty generalization       0.46      0.48      0.47        61
           intentional       0.33      0.40      0.36        15

              accuracy                           0.42       299
             macro avg       0.42      0.39      0.39       299
          weighted avg       0.46      0.42      0.42       299

