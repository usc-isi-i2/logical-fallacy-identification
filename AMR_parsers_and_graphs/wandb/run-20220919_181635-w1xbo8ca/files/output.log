
Start the training ...
/cluster/raid/home/zhivar.sourati/anaconda3/envs/general/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1849
  Num Epochs = 8
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 928
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/928 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.


  4%|▍         | 37/928 [00:05<01:57,  7.61it/s]
{'loss': 2.5385, 'learning_rate': 9.461206896551725e-06, 'epoch': 0.43}
  5%|▌         | 50/928 [00:07<02:06,  6.97it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
  _warn_prf(average, modifier, msg_start, len(result))


 10%|▉         | 90/928 [00:13<02:04,  6.72it/s]
 11%|█         | 100/928 [00:14<03:01,  4.57it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 74%|███████▎  | 14/19 [00:00<00:00, 31.97it/s]
  _warn_prf(average, modifier, msg_start, len(result))


 15%|█▌        | 140/928 [00:21<01:39,  7.94it/s]
 16%|█▌        | 150/928 [00:23<02:46,  4.68it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 74%|███████▎  | 14/19 [00:00<00:00, 31.89it/s]
  _warn_prf(average, modifier, msg_start, len(result))


 21%|██        | 193/928 [00:29<01:38,  7.43it/s]
{'loss': 2.1274, 'learning_rate': 7.844827586206897e-06, 'epoch': 1.72}
 22%|██▏       | 200/928 [00:30<02:18,  5.27it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
                                               /cluster/raid/home/zhivar.sourati/anaconda3/envs/general/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))



 27%|██▋       | 246/928 [00:37<01:31,  7.46it/s]
{'loss': 1.8207, 'learning_rate': 7.306034482758622e-06, 'epoch': 2.16}
 27%|██▋       | 250/928 [00:38<01:51,  6.09it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
                                               /cluster/raid/home/zhivar.sourati/anaconda3/envs/general/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))



 32%|███▏      | 297/928 [00:45<01:23,  7.56it/s]
{'loss': 1.4843, 'learning_rate': 6.767241379310346e-06, 'epoch': 2.59}
 32%|███▏      | 300/928 [00:46<01:50,  5.69it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
                                               /cluster/raid/home/zhivar.sourati/anaconda3/envs/general/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))


 38%|███▊      | 350/928 [00:53<01:16,  7.57it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
  0%|          | 0/19 [00:00<?, ?it/s]
{'loss': 1.3855, 'learning_rate': 6.228448275862069e-06, 'epoch': 3.02}
  _warn_prf(average, modifier, msg_start, len(result))


 42%|████▏     | 390/928 [00:59<01:11,  7.56it/s]
 43%|████▎     | 400/928 [01:01<01:28,  5.97it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 53%|█████▎    | 10/19 [00:00<00:00, 32.35it/s]
  _warn_prf(average, modifier, msg_start, len(result))


 48%|████▊     | 442/928 [01:07<01:06,  7.26it/s]
{'loss': 1.018, 'learning_rate': 5.150862068965518e-06, 'epoch': 3.88}
 48%|████▊     | 450/928 [01:08<01:13,  6.46it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
                                               /cluster/raid/home/zhivar.sourati/anaconda3/envs/general/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))



 54%|█████▎    | 497/928 [01:15<00:48,  8.81it/s]
{'loss': 0.8699, 'learning_rate': 4.612068965517242e-06, 'epoch': 4.31}
 54%|█████▍    | 500/928 [01:16<01:20,  5.33it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
                                               /cluster/raid/home/zhivar.sourati/anaconda3/envs/general/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 54%|█████▍    | 500/928 [01:16<01:20,  5.33it/Saving model checkpoint to ./xlm_roberta_logical_fallacy_classification/checkpoint-500
Configuration saved in ./xlm_roberta_logical_fallacy_classification/checkpoint-500/config.json
Model weights saved in ./xlm_roberta_logical_fallacy_classification/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ./xlm_roberta_logical_fallacy_classification/checkpoint-500/tokenizer_config.json
Special tokens file saved in ./xlm_roberta_logical_fallacy_classification/checkpoint-500/special_tokens_map.json



 59%|█████▊    | 545/928 [02:03<00:53,  7.20it/s]
 59%|█████▉    | 550/928 [02:04<01:17,  4.87it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 95%|█████████▍| 18/19 [00:00<00:00, 32.25it/s]



 64%|██████▍   | 597/928 [02:11<00:49,  6.67it/s]
{'loss': 0.6365, 'learning_rate': 3.5344827586206898e-06, 'epoch': 5.17}
 65%|██████▍   | 600/928 [02:12<01:00,  5.38it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16


 69%|██████▊   | 636/928 [02:17<00:38,  7.51it/s]
 70%|███████   | 650/928 [02:19<00:38,  7.16it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
  0%|          | 0/19 [00:00<?, ?it/s]



 74%|███████▍  | 690/928 [02:25<00:30,  7.80it/s]
 75%|███████▌  | 700/928 [02:27<00:46,  4.94it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 26%|██▋       | 5/19 [00:00<00:00, 48.41it/s]



 80%|███████▉  | 741/928 [02:33<00:23,  7.82it/s]
{'loss': 0.411, 'learning_rate': 1.9181034482758622e-06, 'epoch': 6.47}
 81%|████████  | 750/928 [02:35<00:30,  5.90it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16



 85%|████████▌ | 791/928 [02:41<00:19,  7.16it/s]
{'loss': 0.4036, 'learning_rate': 1.3793103448275862e-06, 'epoch': 6.9}
 86%|████████▌ | 800/928 [02:43<00:25,  5.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16



 91%|█████████ | 844/928 [02:49<00:10,  7.96it/s]
{'loss': 0.3705, 'learning_rate': 8.405172413793105e-07, 'epoch': 7.33}
 92%|█████████▏| 850/928 [02:50<00:11,  7.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16



 97%|█████████▋| 898/928 [02:57<00:04,  7.29it/s]
{'loss': 0.3103, 'learning_rate': 3.0172413793103453e-07, 'epoch': 7.76}
 97%|█████████▋| 900/928 [02:58<00:05,  4.86it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16

 99%|█████████▉| 919/928 [03:01<00:01,  6.92it/s]
100%|█████████▉| 927/928 [03:03<00:00,  7.70it/s]
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████| 928/928 [03:03<00:00,  5.07it/s]