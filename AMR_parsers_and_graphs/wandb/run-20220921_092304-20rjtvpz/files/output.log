
Start the training ...
/cluster/raid/home/zhivar.sourati/anaconda3/envs/general/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1849
  Num Epochs = 10
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 1160
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1160 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.






  4%|▎         | 41/1160 [00:13<05:22,  3.47it/s]

  4%|▍         | 49/1160 [00:15<04:54,  3.77it/s]
  4%|▍         | 50/1160 [00:15<05:17,  3.50it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
                                               /cluster/raid/home/zhivar.sourati/anaconda3/envs/general/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))







  8%|▊         | 94/1160 [00:31<05:56,  2.99it/s]

  9%|▊         | 100/1160 [00:33<06:34,  2.69it/s]
  9%|▊         | 100/1160 [00:33<06:34,  2.69it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
                                               /cluster/raid/home/zhivar.sourati/anaconda3/envs/general/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))







 12%|█▏        | 144/1160 [00:49<05:35,  3.03it/s]
 13%|█▎        | 150/1160 [00:51<05:37,  2.99it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
  0%|          | 0/19 [00:00<?, ?it/s]
  _warn_prf(average, modifier, msg_start, len(result))







 17%|█▋        | 194/1160 [01:07<05:41,  2.83it/s]

 17%|█▋        | 200/1160 [01:09<06:00,  2.66it/s]
 17%|█▋        | 200/1160 [01:09<06:00,  2.66it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
                                               /cluster/raid/home/zhivar.sourati/anaconda3/envs/general/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))







 21%|██        | 246/1160 [01:25<04:50,  3.15it/s]
 22%|██▏       | 250/1160 [01:27<05:09,  2.94it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 26%|██▋       | 5/19 [00:00<00:01, 13.37it/s]
  _warn_prf(average, modifier, msg_start, len(result))







 25%|██▌       | 295/1160 [01:43<04:23,  3.29it/s]

 26%|██▌       | 300/1160 [01:45<05:30,  2.60it/s]
 26%|██▌       | 300/1160 [01:45<05:30,  2.60it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16







 30%|██▉       | 346/1160 [02:01<04:07,  3.29it/s]
 30%|███       | 350/1160 [02:02<03:46,  3.58it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 47%|████▋     | 9/19 [00:00<00:00, 11.39it/s]








 34%|███▍      | 395/1160 [02:19<04:15,  3.00it/s]
 34%|███▍      | 400/1160 [02:21<04:34,  2.77it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 26%|██▋       | 5/19 [00:00<00:01, 13.31it/s]








 39%|███▊      | 448/1160 [02:37<04:11,  2.83it/s]
{'loss': 0.0512, 'learning_rate': 6.1206896551724135e-06, 'epoch': 3.88}
 39%|███▉      | 450/1160 [02:38<04:04,  2.90it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16







 42%|████▎     | 493/1160 [02:53<03:05,  3.59it/s]

 43%|████▎     | 499/1160 [02:55<03:16,  3.36it/s]
 43%|████▎     | 500/1160 [02:55<03:57,  2.78it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 43%|████▎     | 500/1160 [02:57<03:57,  2.78itSaving model checkpoint to ./xlm_roberta_logical_fallacy_classification/checkpoint-500
Configuration saved in ./xlm_roberta_logical_fallacy_classification/checkpoint-500/config.json
Model weights saved in ./xlm_roberta_logical_fallacy_classification/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ./xlm_roberta_logical_fallacy_classification/checkpoint-500/tokenizer_config.json
Special tokens file saved in ./xlm_roberta_logical_fallacy_classification/checkpoint-500/special_tokens_map.json







 47%|████▋     | 546/1160 [03:53<03:39,  2.80it/s]
 47%|████▋     | 550/1160 [03:55<03:45,  2.70it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 37%|███▋      | 7/19 [00:00<00:01, 11.77it/s]








 51%|█████▏    | 597/1160 [04:11<03:24,  2.75it/s]
 52%|█████▏    | 600/1160 [04:12<03:18,  2.82it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 68%|██████▊   | 13/19 [00:01<00:00, 11.42it/s]








 56%|█████▌    | 648/1160 [04:29<02:45,  3.09it/s]
{'loss': 0.0087, 'learning_rate': 4.396551724137931e-06, 'epoch': 5.6}
 56%|█████▌    | 650/1160 [04:30<02:45,  3.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16







 60%|█████▉    | 694/1160 [04:45<02:19,  3.34it/s]
 60%|██████    | 700/1160 [04:47<02:52,  2.67it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
  0%|          | 0/19 [00:00<?, ?it/s]








 64%|██████▍   | 744/1160 [05:03<02:24,  2.89it/s]
 65%|██████▍   | 750/1160 [05:05<02:06,  3.25it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
  0%|          | 0/19 [00:00<?, ?it/s]








 68%|██████▊   | 794/1160 [05:21<02:07,  2.87it/s]
 69%|██████▉   | 800/1160 [05:23<02:11,  2.74it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
  0%|          | 0/19 [00:00<?, ?it/s]








 73%|███████▎  | 845/1160 [05:40<01:33,  3.38it/s]
 73%|███████▎  | 850/1160 [05:41<01:33,  3.30it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 26%|██▋       | 5/19 [00:00<00:01, 13.26it/s]








 77%|███████▋  | 895/1160 [05:58<01:25,  3.10it/s]

 78%|███████▊  | 900/1160 [06:00<01:47,  2.43it/s]
 78%|███████▊  | 900/1160 [06:00<01:47,  2.43it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16







 81%|████████▏ | 945/1160 [06:15<01:07,  3.20it/s]
 82%|████████▏ | 950/1160 [06:17<01:06,  3.17it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 26%|██▋       | 5/19 [00:00<00:01, 13.34it/s]








 86%|████████▌ | 997/1160 [06:34<00:51,  3.17it/s]
 86%|████████▌ | 1000/1160 [06:35<01:08,  2.35it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 37%|███▋      | 7/19 [00:00<00:01, 11.66it/s]

Configuration saved in ./xlm_roberta_logical_fallacy_classification/checkpoint-1000/config.json
Model weights saved in ./xlm_roberta_logical_fallacy_classification/checkpoint-1000/pytorch_model.bin
tokenizer config file saved in ./xlm_roberta_logical_fallacy_classification/checkpoint-1000/tokenizer_config.json
Special tokens file saved in ./xlm_roberta_logical_fallacy_classification/checkpoint-1000/special_tokens_map.json






 90%|█████████ | 1047/1160 [07:32<00:36,  3.07it/s]
 91%|█████████ | 1050/1160 [07:33<00:39,  2.78it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 47%|████▋     | 9/19 [00:00<00:00, 11.47it/s]








 95%|█████████▍| 1097/1160 [07:50<00:20,  3.05it/s]
 95%|█████████▍| 1100/1160 [07:51<00:22,  2.64it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 47%|████▋     | 9/19 [00:00<00:00, 11.35it/s]








 99%|█████████▉| 1147/1160 [08:08<00:03,  3.53it/s]
 99%|█████████▉| 1150/1160 [08:09<00:03,  3.12it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 16
 58%|█████▊    | 11/19 [00:00<00:00, 11.00it/s]

{'eval_loss': 0.29353296756744385, 'eval_accuracy': 0.9366666666666666, 'eval_f1': 0.9345498013292318, 'eval_precision': 0.9344560724873224, 'eval_recall': 0.9366666666666666, 'eval_runtime': 1.7484, 'eval_samples_per_second': 171.584, 'eval_steps_per_second': 10.867, 'epoch': 9.91}
100%|██████████| 1160/1160 [08:13<00:00,  3.51it/s]
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████| 1160/1160 [08:13<00:00,  2.35it/s]
***** Running Prediction *****
  Num examples = 300
  Batch size = 16
 16%|█▌        | 3/19 [00:00<00:00, 21.45it/s]
PredictionOutput(predictions=array([[ 7.9469194 ,  0.04563348, -1.4100819 , ..., -1.2132406 ,
        -0.9827322 ,  0.7476781 ],
       [-1.0354048 , -0.72686183, -0.3514983 , ..., -0.86547536,
        10.273174  , -1.1047903 ],
       [-1.1651148 , -0.73926044,  0.8540329 , ..., -1.2489113 ,
         9.672424  , -1.3429585 ],
       ...,
       [-0.5747918 ,  9.442837  , -0.60462004, ..., -0.6595606 ,
        -0.7776937 , -0.58915377],
       [-1.3711874 ,  1.3129599 , -1.2464739 , ..., -2.1859865 ,
         4.5437336 ,  2.265118  ],
       [-0.61223555, -1.027997  , -1.2183158 , ..., -0.761198  ,
        -1.3107015 , -0.46337855]], dtype=float32), label_ids=array([ 6, 11,  7,  9, 11,  6,  6, 11,  5,  3,  9, 11,  0,  1, 11, 12,  1,
        7,  0, 12,  3,  1,  8,  3,  6,  8,  0,  0, 11,  0, 11,  3,  3,  5,
        2,  8,  9,  1, 11,  1,  5, 12,  0, 12,  1,  2, 12,  0,  5,  7,  3,
       11,  2,  4,  8,  3, 12, 11, 11, 10,  7, 11,  0, 10, 12,  0,  1,  8,
       12,  5, 10,  5,  1,  8,  0, 12,  1,  7, 10, 12,  7,  7,  2,  0,  2,
       10, 11,  0,  6,  0, 10,  9,  5,  5,  0,  9,  1,  2,  8,  9,  0,  2,
        0,  7, 11,  9,  6,  6,  9,  3, 12,  7, 11, 11, 11, 10,  2,  8,  9,
        0,  7,  3, 11, 11, 12,  8,  0,  4, 11,  5,  8, 11,  1,  5, 11,  7,
       11,  0, 11,  0, 11,  6,  0, 11, 11,  8, 11,  1,  8,  6,  5,  0, 11,
        9,  0,  1,  5,  0,  1,  8,  8,  0,  2, 11, 11,  0,  6,  5,  6, 11,
        2,  2,  7,  9, 11,  1, 11,  1,  8,  9,  8,  6,  3,  8, 10,  1,  2,
        0, 11, 11,  3,  8,  6,  2,  4,  1, 11, 11, 11, 11,  0, 11,  2,  1,
        8,  1,  9,  0,  1, 11,  1,  5,  1,  6,  5,  0,  8,  0,  1, 10, 11,
        9,  0, 11, 12,  2,  4,  0, 11,  2, 12,  2, 11,  7,  3,  2,  2,  8,
        3,  0,  2, 11,  8,  8,  9,  3, 11,  6, 11,  0,  3,  2,  8, 10, 12,
       11,  5, 10,  6,  9, 11,  6,  3,  1,  6,  3,  7, 11,  6,  2, 11,  4,
       10,  9,  2,  0, 11,  5,  1, 11,  6, 11,  9, 11,  0,  0,  0, 11, 11,
100%|██████████| 19/19 [00:01<00:00,  9.98it/s]
100%|██████████| 19/19 [00:01<00:00,  9.98it/s]