Number of training graphs: 1479
Number of test graphs: 370
expected scalar type Long but found Float
Python 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]
Type 'copyright', 'credits' or 'license' for more information
IPython 8.5.0 -- An enhanced Interactive Python. Type '?' for help.
RuntimeError('expected scalar type Long but found Float')
tensor([[ 0.0595,  0.1750,  0.7051,  ..., -0.4300,  0.0065,  0.2432],
        [-0.0403,  0.4325, -0.5342,  ...,  0.1455,  0.8429, -0.1291],
        [-0.4506,  0.2608, -0.1705,  ..., -0.1747,  0.5145,  0.9890],
        ...,
        [ 0.0194,  0.5541, -0.0411,  ...,  0.7289,  0.7298,  0.3016],
        [-0.3346, -0.5063, -0.0798,  ...,  0.3726,  0.0639, -0.1583],
        [ 0.0710,  0.1908,  0.1552,  ...,  0.5803,  0.6447, -0.7999]])
tensor([[  0,   0,   1,  ..., 346, 347, 348],
        [  1,   2,   0,  ..., 348, 345, 346]])
tensor([[ 24],
        [ 45],
        [ 24],
        [ 39],
        [ 26],
        [ 45],
        [ 39],
        [ 26],
        [ 95],
        [ 39],
        [ 22],
        [ 39],
        [ 26],
        [ 22],
        [ 39],
        [ 26],
        [103],
        [ 22],
        [ 22],
        [ 79],
        [ 39],
        [ 39],
        [ 39],
        [ 39],
        [ 95],
        [ 79],
        [ 11],
        [ 11],
        [ 39],
        [ 39],
        [ 95],
        [ 39],
        [ 22],
        [ 39],
        [ 23],
        [ 71],
        [ 22],
        [ 79],
        [ 22],
        [ 39],
        [ 22],
        [ 39],
        [ 23],
        [ 71],
        [ 79],
        [ 11],
        [ 11],
        [ 39],
        [ 79],
        [ 79],
        [ 11],
        [ 11],
        [ 39],
        [ 11],
        [ 83],
        [ 22],
        [ 26],
        [ 39],
        [ 22],
        [103],
        [ 11],
        [ 79],
        [ 83],
        [ 79],
        [ 79],
        [ 11],
        [ 11],
        [ 79],
        [ 11],
        [ 11],
        [ 39],
        [ 79],
        [ 53],
        [ 22],
        [ 26],
        [ 39],
        [ 22],
        [ 95],
        [ 95],
        [ 79],
        [ 11],
        [ 53],
        [ 26],
        [ 11],
        [ 26],
        [ 95],
        [ 24],
        [ 45],
        [ 27],
        [ 24],
        [ 22],
        [ 39],
        [ 55],
        [ 85],
        [ 95],
        [ 45],
        [ 39],
        [103],
        [ 85],
        [ 27],
        [ 39],
        [ 19],
        [ 53],
        [ 22],
        [ 79],
        [ 22],
        [ 39],
        [ 39],
        [ 22],
        [ 55],
        [ 85],
        [ 39],
        [ 95],
        [ 95],
        [ 79],
        [ 11],
        [ 11],
        [ 95],
        [ 39],
        [ 55],
        [ 55],
        [ 89],
        [103],
        [ 85],
        [ 39],
        [ 55],
        [ 55],
        [ 89],
        [ 39],
        [ 79],
        [ 79],
        [ 11],
        [ 11],
        [ 39],
        [ 79],
        [ 19],
        [ 53],
        [ 39],
        [ 79],
        [ 11],
        [ 11],
        [ 39],
        [ 39],
        [ 39],
        [ 39],
        [ 39],
        [ 79],
        [ 79],
        [ 11],
        [ 11],
        [ 24],
        [ 45],
        [ 24],
        [ 11],
        [ 83],
        [  6],
        [ 45],
        [ 22],
        [ 11],
        [ 22],
        [ 39],
        [ 83],
        [ 22],
        [ 39],
        [  6],
        [ 39],
        [ 92],
        [ 22],
        [ 79],
        [ 39],
        [ 39],
        [ 79],
        [ 79],
        [ 11],
        [ 11],
        [ 79],
        [ 11],
        [ 11],
        [ 22],
        [ 79],
        [ 93],
        [ 79],
        [ 11],
        [ 11],
        [ 39],
        [ 93],
        [ 79],
        [ 92],
        [ 79],
        [ 11],
        [ 83],
        [ 11],
        [ 83],
        [ 22],
        [ 22],
        [ 39],
        [ 22],
        [ 22],
        [ 39],
        [ 77],
        [ 39],
        [ 22],
        [ 39],
        [ 95],
        [ 22],
        [ 79],
        [ 39],
        [ 79],
        [ 55],
        [ 77],
        [ 19],
        [ 79],
        [ 11],
        [ 11],
        [ 79],
        [ 11],
        [ 55],
        [ 11],
        [ 19],
        [ 22],
        [ 79],
        [ 39],
        [ 95],
        [ 79],
        [ 11],
        [ 11],
        [ 11],
        [ 83],
        [ 11],
        [ 39],
        [ 83],
        [ 39],
        [ 39],
        [ 22],
        [ 39],
        [ 22],
        [ 79],
        [ 39],
        [ 22],
        [ 39],
        [ 79],
        [ 53],
        [ 79],
        [ 11],
        [ 11],
        [ 79],
        [ 11],
        [ 39],
        [ 53],
        [ 26],
        [ 11],
        [ 26],
        [ 39],
        [ 22],
        [ 39],
        [ 39],
        [ 79],
        [ 53],
        [ 79],
        [ 11],
        [ 53],
        [ 39],
        [ 26],
        [ 11],
        [ 39],
        [ 26],
        [ 39],
        [ 39],
        [ 39],
        [ 39],
        [ 22],
        [ 39],
        [ 22],
        [ 16],
        [ 39],
        [ 16],
        [ 22],
        [ 39],
        [ 22],
        [ 39],
        [ 22],
        [ 39],
        [103],
        [103],
        [ 95],
        [ 22],
        [ 79],
        [ 22],
        [ 22],
        [ 39],
        [103],
        [103],
        [ 26],
        [103],
        [ 26],
        [ 95],
        [ 79],
        [ 11],
        [ 11],
        [103],
        [ 26],
        [103],
        [103],
        [ 22],
        [ 26],
        [ 39],
        [ 22],
        [ 39],
        [ 39],
        [ 71],
        [ 39],
        [ 55],
        [ 55],
        [ 71],
        [ 55],
        [ 55],
        [ 39],
        [ 26],
        [ 39],
        [ 39],
        [103],
        [ 26],
        [ 22],
        [ 39],
        [ 39],
        [ 79],
        [103],
        [ 22],
        [ 79],
        [ 11],
        [ 11],
        [ 22],
        [ 22],
        [ 39],
        [ 24],
        [ 45],
        [ 24],
        [ 22],
        [ 39],
        [ 23],
        [ 71],
        [ 45],
        [ 39],
        [ 26],
        [  9],
        [ 50],
        [ 22],
        [103],
        [ 22],
        [ 39],
        [ 79],
        [ 23],
        [ 22],
        [ 39],
        [ 71],
        [ 71],
        [103],
        [ 26],
        [ 26],
        [ 11],
        [ 83],
        [  6],
        [ 11],
        [ 79],
        [ 83],
        [ 79],
        [  6],
        [ 79],
        [ 83],
        [ 79],
        [ 11],
        [ 11],
        [ 79],
        [ 11],
        [ 11],
        [ 79],
        [ 11],
        [ 11],
        [ 79],
        [ 11],
        [ 11],
        [ 39],
        [ 39],
        [ 99],
        [ 95],
        [ 39],
        [ 22],
        [ 39],
        [ 99],
        [ 95],
        [ 83],
        [ 22],
        [ 11],
        [ 39],
        [ 22],
        [ 26],
        [ 11],
        [ 26],
        [ 79],
        [ 79],
        [ 11],
        [ 11],
        [ 22],
        [ 79],
        [ 79],
        [ 11],
        [ 83],
        [ 11],
        [ 83],
        [ 71],
        [ 79],
        [ 79],
        [ 11],
        [ 11],
        [ 39],
        [ 79],
        [ 26],
        [  9],
        [ 50],
        [ 79],
        [ 79],
        [ 11],
        [ 11],
        [ 79],
        [ 11],
        [ 11],
        [ 11],
        [ 83],
        [ 11],
        [ 22],
        [ 39],
        [ 83],
        [ 22],
        [ 39],
        [ 26],
        [ 23],
        [ 22],
        [ 22],
        [ 79],
        [ 22],
        [ 39],
        [ 90],
        [ 79],
        [ 11],
        [ 11],
        [ 90],
        [ 11],
        [ 83],
        [ 11],
        [ 55],
        [ 39],
        [ 83],
        [ 55],
        [ 26],
        [ 55],
        [ 55],
        [ 39],
        [ 38],
        [ 26],
        [ 79],
        [ 23],
        [ 55],
        [ 22],
        [ 38],
        [ 39],
        [103],
        [ 39],
        [ 39],
        [ 26],
        [  9],
        [ 58],
        [103],
        [ 39],
        [ 22],
        [ 26],
        [  9],
        [ 58],
        [ 22],
        [ 39],
        [ 39],
        [ 26],
        [ 39],
        [ 23],
        [ 23],
        [ 23],
        [ 23],
        [ 55],
        [103],
        [ 55],
        [ 55],
        [103],
        [ 26],
        [ 55],
        [ 26],
        [ 55],
        [ 55],
        [ 79],
        [ 11],
        [ 11],
        [ 55],
        [ 24],
        [ 45],
        [ 24],
        [ 39],
        [ 26],
        [ 95],
        [ 45],
        [ 39],
        [ 26],
        [103],
        [ 95],
        [ 39],
        [ 26],
        [ 79],
        [ 26],
        [ 26],
        [ 79],
        [ 95],
        [ 79],
        [ 11],
        [ 11],
        [ 79],
        [ 11],
        [ 11],
        [ 39],
        [ 79],
        [ 39],
        [103],
        [ 22],
        [ 95],
        [ 79],
        [ 11],
        [ 11],
        [ 26],
        [ 39],
        [ 22],
        [ 95],
        [ 95],
        [ 24],
        [ 45],
        [ 24],
        [ 39],
        [ 85],
        [ 45],
        [ 22],
        [ 39],
        [ 95],
        [ 95],
        [ 39],
        [ 22],
        [ 26],
        [ 85],
        [ 22],
        [ 39],
        [ 95],
        [ 22],
        [ 79],
        [ 26],
        [ 16],
        [ 79],
        [ 11],
        [ 11],
        [ 16],
        [ 11],
        [ 11],
        [ 22],
        [ 39],
        [ 79],
        [ 95],
        [ 79],
        [ 11],
        [ 11],
        [ 22],
        [ 79],
        [ 39],
        [ 22],
        [ 39],
        [ 95],
        [ 95],
        [ 79],
        [ 11],
        [ 11],
        [ 22],
        [ 22],
        [ 71],
        [ 39],
        [ 22],
        [ 71],
        [ 51],
        [ 51],
        [ 39],
        [ 39],
        [ 22],
        [ 39],
        [ 22],
        [ 39],
        [ 79],
        [ 79],
        [ 11],
        [ 11],
        [ 24],
        [ 45],
        [ 27],
        [ 24],
        [ 22],
        [ 39],
        [  4],
        [ 45],
        [ 39],
        [  4],
        [ 27],
        [ 22],
        [ 22],
        [ 22],
        [ 39],
        [ 22],
        [ 39],
        [ 99],
        [ 95],
        [  4],
        [ 39],
        [ 79],
        [ 99],
        [ 95],
        [ 79],
        [ 11],
        [ 11],
        [ 39],
        [ 22],
        [ 39],
        [ 55],
        [ 95],
        [  4],
        [ 22],
        [ 39],
        [103],
        [ 55],
        [ 95],
        [103],
        [ 22],
        [ 24],
        [ 45],
        [ 24],
        [ 39],
        [ 26],
        [ 45],
        [ 39],
        [ 26],
        [ 39],
        [ 39],
        [ 26],
        [103],
        [ 26],
        [ 79],
        [ 39],
        [ 39],
        [ 50],
        [ 26],
        [ 93],
        [103],
        [ 22],
        [ 26],
        [ 39],
        [ 71],
        [ 50],
        [ 71],
        [ 93],
        [ 79],
        [ 79],
        [ 11],
        [ 11],
        [ 22],
        [ 55],
        [ 26],
        [ 55],
        [ 79],
        [ 11],
        [ 11],
        [ 39],
        [ 22],
        [ 39],
        [ 95],
        [ 26],
        [ 79],
        [ 22],
        [ 39],
        [ 39],
        [ 95],
        [ 39],
        [ 22],
        [ 39],
        [ 22],
        [ 79],
        [ 22],
        [ 39],
        [103],
        [ 79],
        [ 11],
        [ 11],
        [ 22],
        [103],
        [103],
        [ 95],
        [103],
        [ 95],
        [ 79],
        [ 11],
        [ 11],
        [ 11],
        [ 83],
        [ 11],
        [ 39],
        [ 83],
        [ 39],
        [ 84],
        [ 39],
        [ 16],
        [ 16],
        [ 39],
        [ 16],
        [ 39],
        [ 84],
        [ 39],
        [  9],
        [ 16],
        [  9]])
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
File ~/logical-fallacy-identification/AMR_parsers_and_graphs/gcn.py:53, in NormalNet.forward(self, data, batch)
     52 x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr
---> 53 x = self.conv1(x, edge_index, edge_attr)
     55 x = F.relu(x)
File ~/anaconda3/envs/general/lib/python3.10/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs)
   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1109         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1110     return forward_call(*input, **kwargs)
   1111 # Do not call functions when jit is used
File ~/anaconda3/envs/general/lib/python3.10/site-packages/torch_geometric/nn/conv/transformer_conv.py:176, in TransformerConv.forward(self, x, edge_index, edge_attr, return_attention_weights)
    175 # propagate_type: (query: Tensor, key:Tensor, value: Tensor, edge_attr: OptTensor) # noqa
--> 176 out = self.propagate(edge_index, query=query, key=key, value=value,
    177                      edge_attr=edge_attr, size=None)
    179 alpha = self._alpha
File ~/anaconda3/envs/general/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:374, in MessagePassing.propagate(self, edge_index, size, **kwargs)
    373         msg_kwargs = res[0] if isinstance(res, tuple) else res
--> 374 out = self.message(**msg_kwargs)
    375 for hook in self._message_forward_hooks.values():
File ~/anaconda3/envs/general/lib/python3.10/site-packages/torch_geometric/nn/conv/transformer_conv.py:211, in TransformerConv.message(self, query_i, key_j, value_j, edge_attr, index, ptr, size_i)
    210 assert edge_attr is not None
--> 211 edge_attr = self.lin_edge(edge_attr).view(-1, self.heads,
    212                                           self.out_channels)
    213 key_j += edge_attr
File ~/anaconda3/envs/general/lib/python3.10/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs)
   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1109         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1110     return forward_call(*input, **kwargs)
   1111 # Do not call functions when jit is used
File ~/anaconda3/envs/general/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py:118, in Linear.forward(self, x)
    114 r"""
    115 Args:
    116     x (Tensor): The features.
    117 """
--> 118 return F.linear(x, self.weight, self.bias)
RuntimeError: expected scalar type Long but found Float
During handling of the above exception, another exception occurred:
RuntimeError                              Traceback (most recent call last)
Cell In [5], line 1
----> 1 self.conv1(x, edge_index, edge_attr)
File ~/anaconda3/envs/general/lib/python3.10/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs)
   1106 # If we don't have any hooks, we want to skip the rest of the logic in
   1107 # this function, and just call forward.
   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1109         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1110     return forward_call(*input, **kwargs)
   1111 # Do not call functions when jit is used
   1112 full_backward_hooks, non_full_backward_hooks = [], []
File ~/anaconda3/envs/general/lib/python3.10/site-packages/torch_geometric/nn/conv/transformer_conv.py:176, in TransformerConv.forward(self, x, edge_index, edge_attr, return_attention_weights)
    173 value = self.lin_value(x[0]).view(-1, H, C)
    175 # propagate_type: (query: Tensor, key:Tensor, value: Tensor, edge_attr: OptTensor) # noqa
--> 176 out = self.propagate(edge_index, query=query, key=key, value=value,
    177                      edge_attr=edge_attr, size=None)
    179 alpha = self._alpha
    180 self._alpha = None
File ~/anaconda3/envs/general/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:374, in MessagePassing.propagate(self, edge_index, size, **kwargs)
    372     if res is not None:
    373         msg_kwargs = res[0] if isinstance(res, tuple) else res
--> 374 out = self.message(**msg_kwargs)
    375 for hook in self._message_forward_hooks.values():
    376     res = hook(self, (msg_kwargs, ), out)
File ~/anaconda3/envs/general/lib/python3.10/site-packages/torch_geometric/nn/conv/transformer_conv.py:211, in TransformerConv.message(self, query_i, key_j, value_j, edge_attr, index, ptr, size_i)
    209 if self.lin_edge is not None:
    210     assert edge_attr is not None
--> 211     edge_attr = self.lin_edge(edge_attr).view(-1, self.heads,
    212                                               self.out_channels)
    213     key_j += edge_attr
    215 alpha = (query_i * key_j).sum(dim=-1) / math.sqrt(self.out_channels)
File ~/anaconda3/envs/general/lib/python3.10/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs)
   1106 # If we don't have any hooks, we want to skip the rest of the logic in
   1107 # this function, and just call forward.
   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1109         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1110     return forward_call(*input, **kwargs)
   1111 # Do not call functions when jit is used
   1112 full_backward_hooks, non_full_backward_hooks = [], []
File ~/anaconda3/envs/general/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py:118, in Linear.forward(self, x)
    113 def forward(self, x: Tensor) -> Tensor:
    114     r"""
    115     Args:
    116         x (Tensor): The features.
    117     """
--> 118     return F.linear(x, self.weight, self.bias)
